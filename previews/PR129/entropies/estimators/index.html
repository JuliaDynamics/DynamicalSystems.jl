<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probabilities Estimators · DynamicalSystems.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/logo.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DynamicalSystems.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DynamicalSystems.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><a class="tocitem" href="../../contents/">Contents</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Dynamical systems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ds/general/">Dynamical System Definition</a></li><li><a class="tocitem" href="../../ds/predefined/">Predefined Dynamical Systems</a></li><li><a class="tocitem" href="../../embedding/dataset/">Numerical Data</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">Entropies</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../api/">Entropies &amp; Probabilities</a></li><li class="is-active"><a class="tocitem" href>Probabilities Estimators</a><ul class="internal"><li><a class="tocitem" href="#Visitation-frequency-(binning)"><span>Visitation frequency (binning)</span></a></li><li><a class="tocitem" href="#CountOccurrences-(counting)"><span>CountOccurrences (counting)</span></a></li><li><a class="tocitem" href="#Kernel-density"><span>Kernel density</span></a></li><li><a class="tocitem" href="#Nearest-neighbor-estimators"><span>Nearest neighbor estimators</span></a></li><li><a class="tocitem" href="#Permutation-(symbolic)"><span>Permutation (symbolic)</span></a></li><li><a class="tocitem" href="#Time-scale-(wavelet)"><span>Time-scale (wavelet)</span></a></li><li><a class="tocitem" href="#Utility-methods"><span>Utility methods</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">DelayEmbeddings</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../embedding/reconstruction/">Delay Coordinates Embedding</a></li><li><a class="tocitem" href="../../embedding/traditional/">Traditional Optimal Embedding</a></li><li><a class="tocitem" href="../../embedding/unified/">Unified Optimal Embedding</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">ChaosTools</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../chaos/orbitdiagram/">Orbit Diagrams &amp; PSOS</a></li><li><a class="tocitem" href="../../chaos/lyapunovs/">Lyapunov Exponents</a></li><li><a class="tocitem" href="../../chaos/chaos_detection/">Detecting &amp; Categorizing Chaos</a></li><li><a class="tocitem" href="../../chaos/fractaldim/">Fractal Dimension</a></li><li><a class="tocitem" href="../../chaos/nlts/">Nonlinear Timeseries Analysis</a></li><li><a class="tocitem" href="../../chaos/periodicity/">Periodicity &amp; Ergodicity</a></li><li><a class="tocitem" href="../../chaos/choosing/">Choosing a solver</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">RecurrenceAnalysis</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../rqa/rplots/">Recurrence Plots</a></li><li><a class="tocitem" href="../../rqa/quantification/">Recurrence Quantification Analysis</a></li><li><a class="tocitem" href="../../rqa/windowed/">Windowed RQA</a></li><li><a class="tocitem" href="../../rqa/networks/">Recurrence Network Analysis</a></li></ul></li><li><a class="tocitem" href="../../advanced/">Advanced Documentation</a></li><li><a class="tocitem" href="../../contributors_guide/">Contributor Guide</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Entropies</a></li><li class="is-active"><a href>Probabilities Estimators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probabilities Estimators</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/DynamicalSystems.jl/blob/master/docs/src/entropies/estimators.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Probabilities-Estimators"><a class="docs-heading-anchor" href="#Probabilities-Estimators">Probabilities Estimators</a><a id="Probabilities-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilities-Estimators" title="Permalink"></a></h1><p>In this page we list the various estimators (and further functions) that can be used to obtain probabilities representing a given dataset, or entropies directly. See <a href="../api/#Entropies-and-Probabilities">Entropies &amp; Probabilities</a> for more.</p><h2 id="Visitation-frequency-(binning)"><a class="docs-heading-anchor" href="#Visitation-frequency-(binning)">Visitation frequency (binning)</a><a id="Visitation-frequency-(binning)-1"></a><a class="docs-heading-anchor-permalink" href="#Visitation-frequency-(binning)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.VisitationFrequency" href="#Entropies.VisitationFrequency"><code>Entropies.VisitationFrequency</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">VisitationFrequency(r::RectangularBinning) &lt;: BinningProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the binning scheme <code>r</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia"># Construct boxes by dividing each coordinate axis into 5 equal-length chunks.
b = RectangularBinning(5)

# A probabilities estimator that, when applied a dataset, computes visitation frequencies
# over the boxes of the binning, constructed as describedon the previous line.
est = VisitationFrequency(b)</code></pre><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p></div></section></article><h3 id="Specifying-binning/boxes"><a class="docs-heading-anchor" href="#Specifying-binning/boxes">Specifying binning/boxes</a><a id="Specifying-binning/boxes-1"></a><a class="docs-heading-anchor-permalink" href="#Specifying-binning/boxes" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.RectangularBinning" href="#Entropies.RectangularBinning"><code>Entropies.RectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RectangularBinning(ϵ) &lt;: RectangularBinningScheme</code></pre><p>Instructions for creating a rectangular box partition using the binning scheme <code>ϵ</code>.  Binning instructions are deduced from the type of <code>ϵ</code>.</p><p>Rectangular binnings may be automatically adjusted to the data in which the <code>RectangularBinning</code>  is applied, as follows:</p><ol><li><p><code>ϵ::Int</code> divides each coordinate axis into <code>ϵ</code> equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.</p></li><li><p><code>ϵ::Float64</code> divides each coordinate axis into intervals of fixed size <code>ϵ</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li><li><p><code>ϵ::Vector{Int}</code> divides the i-th coordinate axis into <code>ϵ[i]</code> equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.</p></li><li><p><code>ϵ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size <code>ϵ[i]</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li></ol><p>Rectangular binnings may also be specified on arbitrary min-max ranges. </p><ol><li><code>ϵ::Tuple{Vector{Tuple{Float64,Float64}},Int64}</code> creates intervals   along each coordinate axis from ranges indicated by a vector of <code>(min, max)</code> tuples, then divides   each coordinate axis into an integer number of equal-length intervals. <em>Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored)</em>.</li></ol><p><strong>Example 1: Grid deduced automatically from data (partition guaranteed to cover data points)</strong></p><p><strong>Flexible box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into <code>10</code> equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.</p><pre><code class="language-julia">using Entropies
RectangularBinning(10)</code></pre><p>Now, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into <code>10</code> equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into <code>5</code> equal-length intervals. This gives (hyper-)rectangular boxes.</p><pre><code class="language-julia">using Entropies
RectangularBinning([10, 5])</code></pre><p><strong>Fixed box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size <code>0.5</code> until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.</p><pre><code class="language-julia">using Entropies
RectangularBinning(0.5)</code></pre><p>Again, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size <code>0.3</code>, and the range along the second axis into equal-length intervals of size <code>0.1</code> (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. </p><pre><code class="language-julia">using Entropies
RectangularBinning([0.3, 0.1])</code></pre><p><strong>Example 2: Custom grids (partition not guaranteed to cover data points):</strong></p><p>Assume the data consists of 3-dimensional points <code>(x, y, z)</code>, and that we want a grid  that is fixed over the intervals <code>[x₁, x₂]</code> for the first dimension, over <code>[y₁, y₂]</code> for the second dimension, and over <code>[z₁, z₂]</code> for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. <em>Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded)</em>. </p><p>The following binning specification produces the desired (hyper-)rectangular boxes. </p><pre><code class="language-julia">using Entropies, DelayEmbeddings

D = Dataset(rand(100, 3));

x₁, x₂ = 0.5, 1 # not completely covering the data, which are on [0, 1]
y₁, y₂ = -2, 1.5 # covering the data, which are on [0, 1]
z₁, z₂ = 0, 0.5 # not completely covering the data, which are on [0, 1]

ϵ = [(x₁, x₂), (y₁, y₂), (z₁, z₂)], 4 # [interval 1, interval 2, ...], n_subdivisions

RectangularBinning(ϵ)</code></pre></div></section></article><h2 id="CountOccurrences-(counting)"><a class="docs-heading-anchor" href="#CountOccurrences-(counting)">CountOccurrences (counting)</a><a id="CountOccurrences-(counting)-1"></a><a class="docs-heading-anchor-permalink" href="#CountOccurrences-(counting)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.CountOccurrences" href="#Entropies.CountOccurrences"><code>Entropies.CountOccurrences</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CountOccurrences  &lt;: CountingBasedProbabilityEstimator</code></pre><p>A probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. From these counts, construct histograms. Sum-normalize histograms to obtain probability distributions.</p></div></section></article><h2 id="Kernel-density"><a class="docs-heading-anchor" href="#Kernel-density">Kernel density</a><a id="Kernel-density-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.NaiveKernel" href="#Entropies.NaiveKernel"><code>Entropies.NaiveKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NaiveKernel(ϵ::Real, method::KernelEstimationMethod = TreeDistance()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by  counting how many other points occupy the space spanned by  a hypersphere of radius <code>ϵ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( \mathbf{x}, \epsilon) \approx \dfrac{1}{N} \sum_{s \neq i } K\left( \dfrac{||\mathbf{x}_i - \mathbf{x}_s ||}{\epsilon} \right),\]</p><p>where <span>$K(z) = 1$</span> if <span>$z &lt; 1$</span> and zero otherwise. Probabilities are then normalized.</p><p><strong>Methods</strong></p><ul><li>Tree-based evaluation of distances using <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>. Faster, but more   memory allocation.</li><li>Direct evaluation of distances using <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>. Slower, but less    memory allocation. Also works for complex numbers.</li></ul><p><strong>Estimation</strong></p><p>Probabilities or entropies can be estimated from <code>Dataset</code>s.</p><ul><li><code>probabilities(x::AbstractDataset, est::NaiveKernel)</code>. Associates a probability <code>p</code> to    each point in <code>x</code>.</li><li><code>genentropy(x::AbstractDataset, est::NaiveKernel)</code>.  Associate probability <code>p</code> to each    point in <code>x</code>, then compute the generalized entropy from those probabilities.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using Entropy, DelayEmbeddings
pts = Dataset([rand(5) for i = 1:10000]);
ϵ = 0.2
est_direct = NaiveKernel(ϵ, DirectDistance())
est_tree = NaiveKernel(ϵ, TreeDistance())

p_direct = probabilities(pts, est_direct)
p_tree = probabilities(pts, est_tree)

# Check that both methods give the same probabilities
all(p_direct .== p_tree)</code></pre><p>See also: <a href="#Entropies.DirectDistance"><code>DirectDistance</code></a>, <a href="#Entropies.TreeDistance"><code>TreeDistance</code></a>.</p></div></section></article><h3 id="Distance-evaluation-methods"><a class="docs-heading-anchor" href="#Distance-evaluation-methods">Distance evaluation methods</a><a id="Distance-evaluation-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Distance-evaluation-methods" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.TreeDistance" href="#Entropies.TreeDistance"><code>Entropies.TreeDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TreeDistance(metric::M = Euclidean()) &lt;: KernelEstimationMethod</code></pre><p>Pairwise distances are evaluated using a tree-based approach with the provided <code>metric</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.DirectDistance" href="#Entropies.DirectDistance"><code>Entropies.DirectDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DirectDistance(metric::M = Euclidean()) &lt;: KernelEstimationMethod</code></pre><p>Pairwise distances are evaluated directly using the provided <code>metric</code>.</p></div></section></article><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point <code>p</code>, measured by how many points are within radius <code>1.5</code> of <code>p</code>. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.</p><pre><code class="language-julia">using Distributions, PyPlot, DelayEmbeddings, Entropies
𝒩 = MvNormal([1, -4], 2)
N = 500
D = Dataset(sort([rand(𝒩) for i = 1:N]))
x, y = columns(D)
p = probabilities(D, NaiveKernel(1.5))
figure()
surf(x, y, p.p)
xlabel(&quot;x&quot;); ylabel(&quot;y&quot;)</code></pre><p><img src="../kernel_surface.png" alt/></p><h2 id="Nearest-neighbor-estimators"><a class="docs-heading-anchor" href="#Nearest-neighbor-estimators">Nearest neighbor estimators</a><a id="Nearest-neighbor-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbor-estimators" title="Permalink"></a></h2><h3 id="Kraskov"><a class="docs-heading-anchor" href="#Kraskov">Kraskov</a><a id="Kraskov-1"></a><a class="docs-heading-anchor-permalink" href="#Kraskov" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>k-th nearest neighbour(kNN) based</strong></p><pre><code class="language-none">Kraskov(k::Int = 1, w::Int = 1) &lt;: NearestNeighborEntropyEstimator</code></pre><p>Entropy estimator based on <code>k</code>-th nearest neighbor searches<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>. <code>w</code> is the number of nearest neighbors to exclude when searching for neighbours  (defaults to <code>0</code>, meaning that only the point itself is excluded).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation. Probabilities cannot be obtained directly.</p></div></div></div></section></article><h3 id="Kozachenko-Leonenko"><a class="docs-heading-anchor" href="#Kozachenko-Leonenko">Kozachenko-Leonenko</a><a id="Kozachenko-Leonenko-1"></a><a class="docs-heading-anchor-permalink" href="#Kozachenko-Leonenko" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KozachenkoLeonenko(; w::Int = 0) &lt;: NearestNeighborEntropyEstimator</code></pre><p>Entropy estimator based on nearest neighbors. This implementation is based on Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation. Probabilities cannot be obtained directly.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L16">source</a></section></article><h3 id="Example-2"><a class="docs-heading-anchor" href="#Example-2">Example</a><a class="docs-heading-anchor-permalink" href="#Example-2" title="Permalink"></a></h3><p>This example reproduces Figure in Charzyńska &amp; Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>. Both estimators nicely converge to the true entropy with increasing time series length. For a uniform 1D distribution <span>$U(0, 1)$</span>, the true entropy is <code>0</code> (red line).</p><pre><code class="language-julia">using DynamicalSystems, PyPlot
import Distributions: Uniform, Normal

Ns = [100:100:500; 1000:1000:10000]
Ekl = Vector{Vector{Float64}}(undef, 0)
Ekr = Vector{Vector{Float64}}(undef, 0)

est_nn = KozachenkoLeonenko(w = 0)
# with k = 1, Kraskov is virtually identical to KozachenkoLeonenko, so pick a higher
# number of neighbors
est_knn = Kraskov(w = 0, k = 3)

nreps = 50
for N in Ns
    kl = Float64[]
    kr = Float64[]
    for i = 1:nreps
        pts = Dataset([rand(Uniform(0, 1), 1) for i = 1:N]);
        push!(kl, genentropy(pts, est_nn))
         # with k = 1 almost identical
        push!(kr, genentropy(pts, est_knn))
    end
    push!(Ekl, kl)
    push!(Ekr, kr)
end

# Plot
using PyPlot, StatsBase
f = figure(figsize = (5,6))
ax = subplot(211)
px = PyPlot.plot(Ns, mean.(Ekl); color = &quot;C1&quot;, label = &quot;KozachenkoLeonenko&quot;);
PyPlot.plot(Ns, mean.(Ekl) .+ StatsBase.std.(Ekl); color = &quot;C1&quot;, label = &quot;&quot;);
PyPlot.plot(Ns, mean.(Ekl) .- StatsBase.std.(Ekl); color = &quot;C1&quot;, label = &quot;&quot;);

xlabel(&quot;Time step&quot;); ylabel(&quot;Entropy (nats)&quot;); legend()
ay = subplot(212)
py = PyPlot.plot(Ns, mean.(Ekr); color = &quot;C2&quot;, label = &quot;Kraskov&quot;);
PyPlot.plot(Ns, mean.(Ekr) .+ StatsBase.std.(Ekr); color = &quot;C2&quot;, label = &quot;&quot;);
PyPlot.plot(Ns, mean.(Ekr) .- StatsBase.std.(Ekr); color = &quot;C2&quot;, label = &quot;&quot;);

xlabel(&quot;Time step&quot;); ylabel(&quot;Entropy (nats)&quot;); legend()
tight_layout()</code></pre><p><img src="../nn_entropy_example.png" alt/></p><h2 id="Permutation-(symbolic)"><a class="docs-heading-anchor" href="#Permutation-(symbolic)">Permutation (symbolic)</a><a id="Permutation-(symbolic)-1"></a><a class="docs-heading-anchor-permalink" href="#Permutation-(symbolic)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicPermutation" href="#Entropies.SymbolicPermutation"><code>Entropies.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator
SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator
SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator</code></pre><p>Symbolic, permutation-based probabilities/entropy estimators.</p><p>Uses embedding dimension <span>$m = 3$</span> with embedding lag <span>$\tau = 1$</span> by default. The minimum dimension <span>$m$</span> is 2 (there are no sorting permutations of single-element state vectors).</p><p><strong>Repeated values during symbolization</strong></p><p>In the original implementation of permutation entropy <sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup>, equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution <sup class="footnote-reference"><a id="citeref-Zunino2017" href="#footnote-Zunino2017">[Zunino2017]</a></sup>. Here, we resolve this issue by letting the user provide a custom &quot;less-than&quot; function. The keyword <code>lt</code> accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (<code>lt = Entropies.isless_rand</code>). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.</p><p>To get the behaviour described in Bandt and Pompe (2002), use <code>lt = Base.isless</code>).</p><p><strong>Properties of original signal preserved</strong></p><ul><li><strong><code>SymbolicPermutation</code></strong>: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt &amp; Pompe et al. (2002)<sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup> and   Berger et al. (2019) <sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>.</li><li><strong><code>SymbolicWeightedPermutation</code></strong>: Like <code>SymbolicPermutation</code>, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)<sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>.</li><li><strong><code>SymbolicAmplitudeAwarePermutation</code></strong>: Like <code>SymbolicPermutation</code>, but also encodes   amplitude information by considering a weighted combination of <em>absolute amplitudes</em>   of state vectors, and <em>relative differences between elements</em> of state vectors. See   description below for explanation of the weighting parameter <code>A</code>. This implementation   is based on Azami &amp; Escudero (2016) <sup class="footnote-reference"><a id="citeref-Azami2016" href="#footnote-Azami2016">[Azami2016]</a></sup>.</li></ul><p><strong>Probability estimation</strong></p><p><strong>Univariate time series</strong></p><p>To estimate probabilities or entropies from univariate time series, use the following methods:</p><ul><li><code>probabilities(x::AbstractVector, est::SymbolicProbabilityEstimator)</code>. Constructs state vectors   from <code>x</code> using embedding lag <code>τ</code> and embedding dimension <code>m</code>, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.</li><li><code>genentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2)</code> computes   probabilities by calling <code>probabilities(x::AbstractVector, est)</code>,   then computer the order-<code>α</code> generalized entropy to the given base.</li></ul><p><strong>Speeding up repeated computations</strong></p><p>A pre-allocated integer symbol array <code>s</code> can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.</p><p><em>Note: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that <code>length(x) == length(s)</code> if <code>x</code> is a Dataset, or <code>length(s) == length(x) - (m-1)τ</code> if <code>x</code> is a univariate signal that is to be embedded first</em>.</p><p>Use the following signatures (only works for <code>SymbolicPermutation</code>).</p><pre><code class="language-julia">probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities
probabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities</code></pre><p><strong>Multivariate datasets</strong></p><p>Although not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the <span>$L$</span> existing state vectors <span>$\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x_L}\}$</span>.</p><ul><li><code>probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>. Compute ordinal patterns of the   state vectors of <code>x</code> directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.</li><li><code>genentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>. Computes probabilities from   symbol frequencies using <code>probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>,   then computes the order-<code>α</code> generalized (permutation) entropy to the given base.</li></ul><p><em>Caveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for <code>Dataset</code>s are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.</em></p><p><strong>Description</strong></p><p>All symbolic estimators use the same underlying approach to estimating probabilities.</p><p><strong>Embedding, ordinal patterns and symbolization</strong></p><p>Consider the <span>$n$</span>-element univariate time series <span>$\{x(t) = x_1, x_2, \ldots, x_n\}$</span>. Let <span>$\mathbf{x_i}^{m, \tau} = \{x_j, x_{j+\tau}, \ldots, x_{j+(m-1)\tau}\}$</span> for <span>$j = 1, 2, \ldots n - (m-1)\tau$</span> be the <span>$i$</span>-th state vector in a delay reconstruction with embedding dimension <span>$m$</span> and reconstruction lag <span>$\tau$</span>. There are then <span>$N = n - (m-1)\tau$</span> state vectors.</p><p>For an <span>$m$</span>-dimensional vector, there are <span>$m!$</span> possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a <em>motif</em>. Let <span>$\pi_i^{m, \tau}$</span> denote the motif associated with the <span>$m$</span>-dimensional state vector <span>$\mathbf{x_i}^{m, \tau}$</span>, and let <span>$R$</span> be the number of distinct motifs that can be constructed from the <span>$N$</span> state vectors. Then there are at most <span>$R$</span> motifs; <span>$R = N$</span> precisely when all motifs are unique, and <span>$R = 1$</span> when all motifs are the same.</p><p>Each unique motif <span>$\pi_i^{m, \tau}$</span> can be mapped to a unique integer symbol <span>$0 \leq s_i \leq M!-1$</span>. Let <span>$S(\pi) : \mathbb{R}^m \to \mathbb{N}_0$</span> be the function that maps the motif <span>$\pi$</span> to its symbol <span>$s$</span>, and let <span>$\Pi$</span> denote the set of symbols <span>$\Pi = \{ s_i \}_{i\in \{ 1, \ldots, R\}}$</span>.</p><p><strong>Probability computation</strong></p><p><strong><code>SymbolicPermutation</code></strong></p><p>The probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from <sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>),</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left(\mathbf{x}_k^{m, \tau} \right)} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{N},\]</p><p>where the function <span>$\mathbf{1}_A(u)$</span> is the indicator function of a set <span>$A$</span>. That     is, <span>$\mathbf{1}_A(u) = 1$</span> if <span>$u \in A$</span>, and <span>$\mathbf{1}_A(u) = 0$</span> otherwise.</p><p><strong><code>SymbolicAmplitudeAwarePermutation</code></strong></p><p>Amplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,a_k} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N a_k}.\]</p><p>The weights encoding amplitude information about state vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> are</p><p class="math-container">\[a_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1} \sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</p><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p><p><strong><code>SymbolicWeightedPermutation</code></strong></p><p>Weighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right)
\, w_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi}
\left( \mathbf{x}_k^{m, \tau} \right) \,w_k} = \dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N w_k}.\]</p><p>The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (<span>$w_j = \beta \,\,\, \forall \,\,\, j \leq N$</span> and <span>$\beta &gt; 0)$</span>. Weights are dictated by the variance of the state vectors.</p><p>Let the aritmetic mean of state vector <span>$\mathbf{x}_i$</span> be denoted by</p><p class="math-container">\[\mathbf{\hat{x}}_j^{m, \tau} = \frac{1}{m} \sum_{k=1}^m x_{j + (k+1)\tau}.\]</p><p>Weights are then computed as</p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j+(k+1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>But given the formula they give for the arithmetic mean, this is <strong>not</strong> the variance of <span>$\mathbf{x}_i$</span>, because the indices are mixed: <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms &quot;vector&quot; and &quot;neighboring vector&quot; (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for <span>$\mathbf{x}_i$</span></em>.</p><p><strong>Entropy computation</strong></p><p>The generalized order-<code>α</code> Renyi entropy<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> can be computed over the probability distribution of symbols as <span>$H(m, \tau, \alpha) = \dfrac{\alpha}{1-\alpha} \log \left( \sum_{j=1}^R p_j^\alpha \right)$</span>. Permutation entropy, as described in Bandt and Pompe (2002), is just the limiting case as <span>$α \to1$</span>, that is <span>$H(m, \tau) = - \sum_j^R p(\pi_j^{m, \tau}) \ln p(\pi_j^{m, \tau})$</span>.</p><p><em>Note: Do not confuse the order of the generalized entropy (<code>α</code>) with the order <code>m</code> of the permutation entropy (which controls the symbol size). Permutation entropy is usually estimated with <code>α = 1</code>, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.</em></p></div></section></article><h3 id="Example-3"><a class="docs-heading-anchor" href="#Example-3">Example</a><a class="docs-heading-anchor-permalink" href="#Example-3" title="Permalink"></a></h3><p>This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using <a href="entropies/@ref"><code>SymbolicWeightedPermutation</code></a> and <a href="entropies/@ref"><code>SymbolicAmplitudeAwarePermutation</code></a> are added here for comparison.</p><pre><code class="language-julia">using DynamicalSystems, PyPlot

ds = Systems.logistic()
rs = 3.4:0.001:4
N_lyap, N_ent = 100000, 10000
m, τ = 6, 1 # Symbol size/dimension and embedding lag

# Generate one time series for each value of the logistic parameter r
lyaps = Float64[]
hs_perm = Float64[]
hs_wtperm = Float64[]
hs_ampperm = Float64[]

base = Base.MathConstants.e
for r in rs
    ds.p[1] = r
    push!(lyaps, lyapunov(ds, N_lyap))

    x = trajectory(ds, N_ent) # time series
    hperm = Entropies.genentropy(x, SymbolicPermutation(m = m, τ = τ), base = base)
    hwtperm = Entropies.genentropy(x, SymbolicWeightedPermutation(m = m, τ = τ), base = base)
    hampperm = Entropies.genentropy(x, SymbolicAmplitudeAwarePermutation(m = m, τ = τ), base = base)

    push!(hs_perm, hperm); push!(hs_wtperm, hwtperm); push!(hs_ampperm, hampperm)
end

f = figure(figsize = (6, 8))
a1 = subplot(411)
plot(rs, lyaps); ylim(-2, log(2)); ylabel(&quot;\$\\lambda\$&quot;)
a1.axes.get_xaxis().set_ticklabels([])
xlim(rs[1], rs[end]);

a2 = subplot(412)
plot(rs, hs_perm; color = &quot;C2&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;&quot;); ylabel(&quot;\$h_6 (SP)\$&quot;)

a3 = subplot(413)
plot(rs, hs_wtperm; color = &quot;C3&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;&quot;); ylabel(&quot;\$h_6 (SWP)\$&quot;)

a4 = subplot(414)
plot(rs, hs_ampperm; color = &quot;C4&quot;); xlim(rs[1], rs[end]);
xlabel(&quot;\$r\$&quot;); ylabel(&quot;\$h_6 (SAAP)\$&quot;)
tight_layout()</code></pre><p><img src="../permentropy.png" alt/></p><h2 id="Time-scale-(wavelet)"><a class="docs-heading-anchor" href="#Time-scale-(wavelet)">Time-scale (wavelet)</a><a id="Time-scale-(wavelet)-1"></a><a class="docs-heading-anchor-permalink" href="#Time-scale-(wavelet)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.TimeScaleMODWT" href="#Entropies.TimeScaleMODWT"><code>Entropies.TimeScaleMODWT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TimeScaleMODWT &lt;: WaveletProbabilitiesEstimator
TimeScaleMODWT(wl::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{12}())</code></pre><p>Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. This implementation is based on Rosso et al. (2001)<sup class="footnote-reference"><a id="citeref-Rosso2001" href="#footnote-Rosso2001">[Rosso2001]</a></sup>. Optionally specify a wavelet to be used.</p><p>The probability <code>p[i]</code> is the relative/total energy for the i-th wavelet scale.</p><p><strong>Example</strong></p><p>Manually picking a wavelet is done as follows.</p><pre><code class="language-julia">using Entropies, Wavelets
N = 200
a = 10
t = LinRange(0, 2*a*π, N)
x = sin.(t .+  cos.(t/0.1)) .- 0.1;

# Pick a wavelet (if no wavelet provided, defaults to Wavelets.WL.Daubechies{12}())
wl = Wavelets.WT.Daubechies{12}()

# Compute the probabilities (relative energies) at the different wavelet scales
probabilities(x, TimeScaleMODWT(wl))</code></pre><p>If no wavelet provided, the default is <code>Wavelets.WL.Daubechies{12}())</code>.</p></div></section></article><h3 id="Example-4"><a class="docs-heading-anchor" href="#Example-4">Example</a><a class="docs-heading-anchor-permalink" href="#Example-4" title="Permalink"></a></h3><p>The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).</p><pre><code class="language-julia">using DynamicalSystems, PyPlot
N, a = 1000, 10
t = LinRange(0, 2*a*π, N)

x = sin.(t);
y = sin.(t .+  cos.(t/0.5));
z = sin.(rand(1:15, N) ./ rand(1:10, N))

est = TimeScaleMODWT()
h_x = genentropy(x, est)
h_y = genentropy(y, est)
h_z = genentropy(z, est)

f = figure(figsize = (10,6))
ax = subplot(311)
px = plot(t, x; color = &quot;C1&quot;, label = &quot;h=$(h=round(h_x, sigdigits = 5))&quot;);
ylabel(&quot;x&quot;); legend()
ay = subplot(312)
py = plot(t, y; color = &quot;C2&quot;, label = &quot;h=$(h=round(h_y, sigdigits = 5))&quot;);
ylabel(&quot;y&quot;); legend()
az = subplot(313)
pz = plot(t, z; color = &quot;C3&quot;, label = &quot;h=$(h=round(h_z, sigdigits = 5))&quot;);
ylabel(&quot;z&quot;); xlabel(&quot;Time&quot;); legend()
tight_layout()</code></pre><p><img src="../waveletentropy.png" alt/></p><h2 id="Utility-methods"><a class="docs-heading-anchor" href="#Utility-methods">Utility methods</a><a id="Utility-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-methods" title="Permalink"></a></h2><p>Some convenience functions for symbolization are provided.</p><article class="docstring"><header><a class="docstring-binding" id="Entropies.encode_as_bin" href="#Entropies.encode_as_bin"><code>Entropies.encode_as_bin</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">encode_as_bin(point, reference_point, edgelengths) → Vector{Int}</code></pre><p>Encode a point into its integer bin labels relative to some <code>reference_point</code> (always counting from lowest to highest magnitudes), given a set of box  <code>edgelengths</code> (one for each axis). The first bin on the positive side of  the reference point is indexed with 0, and the first bin on the negative  side of the reference point is indexed with -1.</p><p>See also: <a href="#Entropies.joint_visits"><code>joint_visits</code></a>, <a href="#Entropies.marginal_visits"><code>marginal_visits</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Entropies

refpoint = [0, 0, 0]
steps = [0.2, 0.2, 0.3]
encode_as_bin(rand(3), refpoint, steps)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.joint_visits" href="#Entropies.joint_visits"><code>Entropies.joint_visits</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">joint_visits(points, binning_scheme::RectangularBinning) → Vector{Vector{Int}}</code></pre><p>Determine which bins are visited by <code>points</code> given the rectangular binning scheme <code>ϵ</code>. Bins are referenced relative to the axis minima, and are  encoded as integers, such that each box in the binning is assigned a unique integer array (one element for each dimension). </p><p>For example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.</p><p>See also: <a href="#Entropies.marginal_visits"><code>marginal_visits</code></a>, <a href="#Entropies.encode_as_bin"><code>encode_as_bin</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia">using DelayEmbeddings, Entropies

pts = Dataset([rand(5) for i = 1:100]);
joint_visits(pts, RectangularBinning(0.2))</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.marginal_visits" href="#Entropies.marginal_visits"><code>Entropies.marginal_visits</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">marginal_visits(points, binning_scheme::RectangularBinning, dims) → Vector{Vector{Int}}</code></pre><p>Determine which bins are visited by <code>points</code> given the rectangular binning scheme <code>ϵ</code>, but only along the desired dimensions <code>dims</code>. Bins are referenced  relative to the axis minima, and are encoded as integers, such that each box  in the binning is assigned a unique integer array (one element for each  dimension in <code>dims</code>). </p><p>For example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.</p><p>See also: <a href="#Entropies.joint_visits"><code>joint_visits</code></a>, <a href="#Entropies.encode_as_bin"><code>encode_as_bin</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia">using DelayEmbeddings, Entropies
pts = Dataset([rand(5) for i = 1:100]);

# Marginal visits along dimension 3 and 5
marginal_visits(pts, RectangularBinning(0.3), [3, 5])

# Marginal visits along dimension 2 through 5
marginal_visits(pts, RectangularBinning(0.3), 2:5)</code></pre></div></section><section><div><pre><code class="language-none">marginal_visits(joint_visits, dims) → Vector{Vector{Int}}</code></pre><p>If joint visits have been precomputed using <a href="#Entropies.joint_visits"><code>joint_visits</code></a>, marginal  visits can be returned directly without providing the binning again  using the <code>marginal_visits(joint_visits, dims)</code> signature.</p><p>See also: <a href="#Entropies.joint_visits"><code>joint_visits</code></a>, <a href="#Entropies.encode_as_bin"><code>encode_as_bin</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-none">using DelayEmbeddings, Entropies
pts = Dataset([rand(5) for i = 1:100]);

# First compute joint visits, then marginal visits along dimensions 1 and 4
jv = joint_visits(pts, RectangularBinning(0.2))
marginal_visits(jv, [1, 4])

# Marginals along dimension 2
marginal_visits(jv, 2)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.symbolize" href="#Entropies.symbolize"><code>Entropies.symbolize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">symbolize(x::AbstractVector{T}, est::SymbolicPermutation) where {T} → Vector{Int}
symbolize!(s, x::AbstractVector{T}, est::SymbolicPermutation) where {T} → Vector{Int}</code></pre><p>If <code>x</code> is a univariate time series, first <code>x</code> create a delay reconstruction of <code>x</code> using embedding lag <code>est.τ</code> and embedding dimension <code>est.m</code>, then symbolizing the resulting state vectors with <a href="#Entropies.encode_motif"><code>encode_motif</code></a>.</p><p>Optionally, the in-place <code>symbolize!</code> can be used to put symbols in a pre-allocated integer vector <code>s</code>, where <code>length(s) == length(x)-(est.m-1)*est.τ</code>.</p><pre><code class="language-none">symbolize(x::AbstractDataset{m, T}, est::SymbolicPermutation) where {m, T} → Vector{Int}
symbolize!(s, x::AbstractDataset{m, T}, est::SymbolicPermutation) where {m, T} → Vector{Int}</code></pre><p>If <code>x</code> is an <code>m</code>-dimensional dataset, then motif lengths are determined by the dimension of the input data, and <code>x</code> is symbolized by converting each <code>m</code>-dimensional state vector as a unique integer in the range <span>$1, 2, \ldots, m-1$</span>, using <a href="#Entropies.encode_motif"><code>encode_motif</code></a>.</p><p>Optionally, the in-place <code>symbolize!</code> can be used to put symbols in a pre-allocated integer vector <code>s</code>, where <code>length(s) == length(s)</code>.</p><p><strong>Examples</strong></p><p>Symbolize a 7-dimensional dataset. Motif lengths (or order of the permutations) are inferred to be 7.</p><pre><code class="language-julia">using DelayEmbeddings, Entropies
D = Dataset([rand(7) for i = 1:1000])
s = symbolize(D, SymbolicPermutation())</code></pre><p>Symbolize a univariate time series by first embedding it in dimension 5 with embedding lag 2. Motif lengths (or order of the permutations) are therefore 5.</p><pre><code class="language-julia">using DelayEmbeddings, Entropies
n = 5000
x = rand(n)
s = symbolize(x, SymbolicPermutation(m = 5, τ = 2))</code></pre><p>The integer vector <code>s</code> now has length <code>n-(m-1)*τ = 4992</code>, and each <code>s[i]</code> contains the integer symbol for the ordinal pattern of state vector <code>x[i]</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.encode_motif" href="#Entropies.encode_motif"><code>Entropies.encode_motif</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">encode_motif(x, m::Int = length(x)) → s::Int</code></pre><p>Encode the length-<code>m</code> motif <code>x</code> (a vector of indices that would sort some vector <code>v</code>  in ascending order) into its unique integer symbol <span>$s \in \{1, 2, \ldots, m - 1 \}$</span>,  using Algorithm 1 in Berger et al. (2019)<sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>. </p><p><strong>Example</strong></p><pre><code class="language-julia">v = rand(5)

# The indices that would sort `v` in ascending order. This is now a permutation 
# of the index permutation (1, 2, ..., 5)
x = sortperm(v)

# Encode this permutation as an integer.
encode_motif(x)</code></pre></div></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural complexity measure for time series.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Zunino2017"><a class="tag is-link" href="#citeref-Zunino2017">Zunino2017</a>Zunino, L., Olivares, F., Scholkmann, F., &amp; Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.</li><li class="footnote" id="footnote-Rosso2001"><a class="tag is-link" href="#citeref-Rosso2001">Rosso2001</a>Rosso, O. A., Blanco, S., Yordanova, J., Kolev, V., Figliola, A., Schürmann, M., &amp; Başar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../api/">« Entropies &amp; Probabilities</a><a class="docs-footer-nextpage" href="../../embedding/reconstruction/">Delay Coordinates Embedding »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 11 January 2021 21:50">Monday 11 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
