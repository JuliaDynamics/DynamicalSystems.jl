var documenterSearchIndex = {"docs":
[{"location":"embedding/estimate/#Optimal-DCE-Parameters-1","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"This page discusses and provides algorithms for estimating optimal parameters to do Delay Coordinates Embedding (DCE) with.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"The approaches can be grouped into two schools:","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"Independent (also called traditional), where one tries to independently find the best value for a delay time œÑ and an embedding dimension d.\nUnified, where at the same time an optimal combination of œÑ, d is found.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"The independent approach is something \"old school\", while recent scientific research has shifted almost exclusively to unified approaches.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"In addition, the unified approaches are the only ones that can accommodate multi-variate inputs. This means that if you have multiple measured input timeseries, you should be able to take advantage of all of them for the best possible embedding of the dynamical system's set.","category":"page"},{"location":"embedding/estimate/#Independent-delay-time-1","page":"Optimal DCE Parameters","title":"Independent delay time","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"estimate_delay\nexponential_decay_fit","category":"page"},{"location":"embedding/estimate/#DelayEmbeddings.estimate_delay","page":"Optimal DCE Parameters","title":"DelayEmbeddings.estimate_delay","text":"estimate_delay(s, method::String [, œÑs = 1:100]; kwargs...) -> œÑ\n\nEstimate an optimal delay to be used in reconstruct or embed. The method can be one of the following:\n\n\"ac_zero\" : first delay at which the auto-correlation function becomes <0.\n\"ac_min\" : delay of first minimum of the auto-correlation function.\n\"mi_min\" : delay of first minimum of mutual information of s with itself (shifted for various œÑs). Keywords nbins, binwidth are propagated into mutualinformation.\n\"exp_decay\" : exponential_decay_fit of the correlation function rounded  to an integer (uses least squares on c(t) = exp(-t/œÑ) to find œÑ).\n\"exp_extrema\" : same as above but the exponential fit is done to the absolute value of the local extrema of the correlation function.\n\nBoth the mutual information and correlation function (autocor) are computed only for delays œÑs. This means that the min methods can never return the first value of œÑs!\n\nThe method mi_min is significantly more accurate than the others and also returns good results for most timeseries. It is however the slowest method (but still quite fast!).\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.exponential_decay_fit","page":"Optimal DCE Parameters","title":"DelayEmbeddings.exponential_decay_fit","text":"exponential_decay_fit(x, y, weight = :equal) -> œÑ\n\nPerform a least square fit of the form y = exp(-x/œÑ) and return œÑ. Taken from:  http://mathworld.wolfram.com/LeastSquaresFittingExponential.html. Assumes equal lengths of x, y and that y ‚â• 0.\n\nTo use the method that gives more weight to small values of y, use weight = :small.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#Mutual-Information-1","page":"Optimal DCE Parameters","title":"Mutual Information","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"mutualinformation","category":"page"},{"location":"embedding/estimate/#DelayEmbeddings.mutualinformation","page":"Optimal DCE Parameters","title":"DelayEmbeddings.mutualinformation","text":"mutualinformation(s, œÑs[; nbins, binwidth])\n\nCalculate the mutual information between the time series s and its images delayed by œÑ points for œÑ ‚àà œÑs, using an improvement of the method outlined by Fraser & Swinney in[Fraser1986].\n\nDescription\n\nThe joint space of s and its œÑ-delayed image (sœÑ) is partitioned as a rectangular grid, and the mutual information is computed from the joint and marginal frequencies of s and sœÑ in the grid as defined in [1]. The mutual information values are returned in a vector of the same length as œÑs.\n\nIf any of the optional keyword parameters is given, the grid will be a homogeneous partition of the space where s and sœÑ are defined. The margins of that partition will be divided in a number of bins equal to nbins, such that the width of each bin will be binwidth, and the range of nonzero values of s will be in the centre. If only of those two parameters is given, the other will be automatically calculated to adjust the size of the grid to the area where s and sœÑ are nonzero.\n\nIf no parameter is given, the space will be partitioned by a recursive bisection algorithm based on the method given in [1].\n\nNotice that the recursive method of [1] evaluates the joint frequencies of s and sœÑ in each cell resulting from a partition, and stops when the data points are uniformly distributed across the sub-partitions of the following levels. For performance and stability reasons, the automatic partition method implemented in this function is only used to divide the axes of the grid, using the marginal frequencies of s.\n\n[Fraser1986]: Fraser A.M. & Swinney H.L. \"Independent coordinates for strange attractors from mutual information\" Phys. Rev. A 33(2), 1986, 1134:1140.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#Independent-embedding-dimension-1","page":"Optimal DCE Parameters","title":"Independent embedding dimension","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"optimal_traditional_de\ndelay_afnn\ndelay_ifnn\ndelay_fnn\ndelay_f1nn","category":"page"},{"location":"embedding/estimate/#DelayEmbeddings.optimal_traditional_de","page":"Optimal DCE Parameters","title":"DelayEmbeddings.optimal_traditional_de","text":"optimal_traditional_de(s, method = \"afnn\", dmethod = \"mi_min\"; kwargs...) ‚Üí ùíü, œÑ, E\n\nProduce an optimal delay embedding ùíü of the given timeseries s by using the traditional approach of first finding an optimal (and constant) delay time using estimate_delay with the given dmethod, and then an optimal embedding dimension, by calculating an appropriate statistic for each dimension d ‚àà 1:dmax. Return the embedding ùíü, the optimal delay time œÑ (the optimal embedding dimension d is just size(ùíü, 2)) and the actual statistic E used to estimate optimal d.\n\nNotice that E is a function of the embedding dimension, which ranges from 1 to dmax.\n\nFor calculating E to estimate the dimension we use the given method which can be:\n\n\"afnn\" (default) is Cao's \"Averaged False Nearest Neighbors\" method[Cao1997],   which gives a ratio of distances between nearest neighbors.\n\"ifnn\" is the \"Improved False Nearest Neighbors\" from Hegger & Kantz[Hegger1999],   which gives the fraction of false nearest neighbors.\n\"fnn\" is Kennel's \"False Nearest Neighbors\" method[Kennel1992], which gives   the number of points that cease to be \"nearest neighbors\" when the dimension   increases.\n\"f1nn\" is Krakovsk√°'s \"False First Nearest Neighbors\" method[Krakovsk√°2015],   which gives the ratio of pairs of points that cease to be \"nearest neighbors\"   when the dimension increases.\n\nFor more details, see individual methods: delay_afnn, delay_ifnn, delay_fnn, delay_f1nn. The special keywords `` denote for which possible embedding dimensions should the statistics be computed for.\n\nwarn: Careful in automated methods\nWhile this method is automated if you want to be really sure of the results, you should directly calculate the statistic and plot its values versus the dimensions.\n\nKeywords\n\nThe keywords\n\nœÑs = 1:100, dmax = 10\n\ndenote which delay times and embedding dimensions ds ‚àà 1:dmax to consider when calculating optimal embedding. All remaining keywords are propagated to the low level functions:\n\nfnn_thres::Real = 0.05, slope_thres::Real= 0.2, w::Int=1,\nrtol=10.0, atol=2.0, œÑs = 1:100, metric = Euclidean(), r::Real=2.0\n\nDescription\n\nWe estimate the optimal embedding dimension based on the given delay time gained from dmethod as follows: For Cao's method the optimal dimension is reached, when the slope of the E‚ÇÅ-statistic (output from \"afnn\") falls below the threshold slope_thres (Default is .05) and the according stochastic test turns out to be false, i.e. when the E‚ÇÇ-statistic is not \"equal\" to 1 for all en- countered dimensions. We treat E‚ÇÇ-values as equal to 1, when 1-E‚ÇÇ ‚â§ fnn_thres. For all the other methods we return the optimal embedding dimension when the corresponding FNN-statistic (output from \"ifnn\", \"fnn\" or \"f1nn\") falls below the fnn-threshold fnn_thres (Default is .05) AND the slope of the statistic falls below the threshold slope_thres. Note that with noise contaminated time series, one might need to adjust fnn_thres according to the noise level.\n\nSee also the file test/compare_different_dimension_estimations.jl for a comparison.\n\n[Cao1997]: Liangyue Cao, Physica D, pp. 43-50 (1997)\n\n[Kennel1992]: M. Kennel et al., Phys. Review A 45(6), (1992).\n\n[Krakovsk√°2015]: Anna Krakovsk√° et al., J. Complex Sys. 932750 (2015)\n\n[Hegger1999]: Hegger & Kantz, Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.delay_afnn","page":"Optimal DCE Parameters","title":"DelayEmbeddings.delay_afnn","text":"delay_afnn(s::AbstractVector, œÑ:Int, ds = 2:6, metric=Euclidean()) ‚Üí E‚ÇÅ\n\nCompute the parameter E‚ÇÅ of Cao's \"averaged false nearest neighbors\" method for determining the minimum embedding dimension of the time series s, with a sequence of œÑ-delayed temporal neighbors.\n\nDescription\n\nGiven the scalar timeseries s and the embedding delay œÑ compute the values of E‚ÇÅ for each embedding dimension d ‚àà ds, according to Cao's Method (eq. 3 of[Cao1997]).\n\nThis quantity is a ratio of the averaged distances between the nearest neighbors of the reconstructed time series, which quantifies the increment of those distances when the embedding dimension changes from d to d+1.\n\nReturn the vector of all computed E‚ÇÅs. To estimate a good value for d from this, find d for which the value E‚ÇÅ saturates at some value around 1.\n\nNote: This method does not work for datasets with perfectly periodic signals.\n\nSee also: optimal_traditional_de and stochastic_indicator.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.delay_ifnn","page":"Optimal DCE Parameters","title":"DelayEmbeddings.delay_ifnn","text":"delay_ifnn(s::Vector, œÑ::Int, ds = 1:10; kwargs...) ‚Üí `FNNs`\n\nCompute and return the FNNs-statistic for the time series s and a uniform time delay œÑ and embedding dimensions ds after [Hegger1999]. In this notation Œ≥ ‚àà Œ≥s = d-1, if d is the embedding dimension. This fraction tends to 0 when the optimal embedding dimension with an appropriate lag is reached.\n\nKeywords\n\n*r = 2: Obligatory threshold, which determines the maximum tolerable spreading     of trajectories in the reconstruction space. *metric = Euclidean: The norm used for distance computations. *w = 1 = The Theiler window, which excludes temporally correlated points from     the nearest neighbor search.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.delay_fnn","page":"Optimal DCE Parameters","title":"DelayEmbeddings.delay_fnn","text":"delay_fnn(s::AbstractVector, œÑ:Int, ds = 2:6; rtol=10.0, atol=2.0) ‚Üí FNNs\n\nCalculate the number of \"false nearest neighbors\" (FNNs) of the datasets created from s with embed(s, d, œÑ) for d ‚àà ds.\n\nDescription\n\nGiven a dataset made by embed(s, d, œÑ) the \"false nearest neighbors\" (FNN) are the pairs of points that are nearest to each other at dimension d, but are separated at dimension d+1. Kennel's criteria for detecting FNN are based on a threshold for the relative increment of the distance between the nearest neighbors (rtol, eq. 4 in[Kennel1992]), and another threshold for the ratio between the increased distance and the \"size of the attractor\" (atol, eq. 5 in[Kennel1992]). These thresholds are given as keyword arguments.\n\nThe returned value is a vector with the number of FNN for each Œ≥ ‚àà Œ≥s. The optimal value for Œ≥ is found at the point where the number of FNN approaches zero.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.delay_f1nn","page":"Optimal DCE Parameters","title":"DelayEmbeddings.delay_f1nn","text":"delay_f1nn(s::AbstractVector, œÑ::Int, ds = 2:6, metric = Euclidean())\n\nCalculate the ratio of \"false first nearest neighbors\" (FFNN) of the datasets created from s with embed(s, d, œÑ) for d ‚àà ds.\n\nDescription\n\nGiven a dataset made by embed(s, d, œÑ) the \"false first nearest neighbors\" (FFNN) are the pairs of points that are nearest to each other at dimension d that cease to be nearest neighbors at dimension d+1.\n\nThe returned value is a vector with the ratio between the number of FFNN and the number of points in the dataset for each d ‚àà ds. The optimal value for d is found at the point where this ratio approaches zero.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#Example-1","page":"Optimal DCE Parameters","title":"Example","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"using DynamicalSystems, PyPlot\n\nds = Systems.roessler()\n# This trajectory is a chaotic attractor with fractal dim ‚âà 2\n# therefore the set needs at least embedding dimension of 3\ntr = trajectory(ds, 1000.0; dt = 0.05)\nx = tr[:, 1]\n\ndmax = 7\nfig = figure()\nfor (i, method) in enumerate([\"afnn\", \"fnn\", \"f1nn\", \"ifnn\"])\n    # Plot statistic used to estimate optimal embedding\n    # as well as the automated output embedding\n    ùíü, œÑ, E = optimal_traditional_de(x, method; dmax)\n    plot(1:dmax, E; label = method, marker = \"o\", ms = 5, color = \"C$(i-1)\")\n    optimal_d = size(ùíü, 2)\n    scatter(optimal_d, E[optimal_d]; marker = \"s\", s = 100, color = \"C$(i-1)\")\nend\nlegend(); xlabel(\"embedding dimension\")\nylabel(\"estimator\")\ntight_layout()\nfig.savefig(\"estimateD.png\"); nothing # hide","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"(Image: )","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"","category":"page"},{"location":"embedding/estimate/#Unified-approach-1","page":"Optimal DCE Parameters","title":"Unified approach","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"Several algorithms have been created to implement a unified approach to delay coordinates embedding. You can find some implementations below:","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"pecora\nuzal_cost\ngarcia_almeida_embedding\nmdop_embedding\npecuzal_embedding","category":"page"},{"location":"embedding/estimate/#DelayEmbeddings.pecora","page":"Optimal DCE Parameters","title":"DelayEmbeddings.pecora","text":"pecora(s, œÑs, js; kwargs...) ‚Üí ‚ü®Œµ‚òÖ‚ü©, ‚ü®Œì‚ü©\n\nCompute the (average) continuity statistic ‚ü®Œµ‚òÖ‚ü© and undersampling statistic ‚ü®Œì‚ü© according to Pecora et al.[Pecoral2007] (A unified approach to attractor reconstruction), for a given input s (timeseries or Dataset) and input generalized embedding defined by (œÑs, js), according to genembed. The continuity statistic represents functional independence between the components of the existing embedding and one additional timeseries. The returned results are matrices with size TxJ.\n\nKeyword arguments\n\ndelays = 0:50: Possible time delay values delays (in sampling time units). For each of the œÑ's in delays the continuity-statistic ‚ü®Œµ‚òÖ‚ü© gets computed. If undersampling = true (see further down), also the undersampling statistic ‚ü®Œì‚ü© gets returned for all considered delay values.\nJ = 1:dimension(s): calculate for all timeseries indices in J. If input s is a timeseries, this is always just 1.\nsamplesize::Real = 0.1: determine the fraction of all phase space points (=length(s)) to be considered (fiducial points v) to average Œµ‚òÖ to produce ‚ü®Œµ‚òÖ‚ü©, ‚ü®Œì‚ü©\nK::Int = 13: the amount of nearest neighbors in the Œ¥-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic). ‚ü®Œµ‚òÖ‚ü© is computed taking the minimum result over all k ‚àà K.\nmetric = Chebyshev(): metrix with which to find nearest neigbhors in the input embedding (‚Ñù·µà space, d = length(œÑs)).\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nundersampling = false : whether to calculate the undersampling statistic or not (if not, zeros are returned for ‚ü®Œì‚ü©). Calculating ‚ü®Œì‚ü© is thousands of times slower than ‚ü®Œµ‚òÖ‚ü©.\ndb::Int = 100: Amount of bins used into calculating the histograms of each timeseries (for the undersampling statistic).\nŒ±::Real = 0.05: The significance level for obtaining the continuity statistic\np::Real = 0.5: The p-parameter for the binomial distribution used for the computation of the continuity statistic.\n\nDescription\n\nNotice that the full algorithm is too large to discuss here, and is written in detail (several pages!) in the source code of pecora.\n\n[Pecora2007]: Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007). A unified approach to attractor reconstruction. Chaos 17(1).\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.uzal_cost","page":"Optimal DCE Parameters","title":"DelayEmbeddings.uzal_cost","text":"uzal_cost(Y::Dataset; kwargs...) ‚Üí L\n\nCompute the L-statistic L for input dataset Y according to Uzal et al.[Uzal2011], based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. It serves as a cost function of a state space trajectory/embedding and therefore allows to estimate a \"goodness of a embedding\" and also to choose proper embedding parameters, while minimizing L over the parameter space. For receiving the local cost function L_local (for each point in state space - not averaged), use uzal_cost_local(...).\n\nKeyword arguments\n\nsamplesize = 0.5: Number of considered fiducial points v as a fraction of input state space trajectory Y's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce L.\nK = 3: the amount of nearest neighbors considered, in order to compute œÉ_k^2 (read algorithm description). If given a vector, minimum result over all k ‚àà K is returned.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input state space trajectory `Y.\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nTw = 40: The time horizon (in sampling units) up to which E_k^2 gets computed and averaged over (read algorithm description).\n\nDescription\n\nThe L-statistic is based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. Technically, it is the logarithm of the product of œÉ-statistic and a normalization statistic Œ±:\n\nL = log10(œÉ*Œ±)\n\nThe œÉ-statistic is computed as follows. œÉ = ‚àöœÉ¬≤ = ‚àö(E¬≤/œµ¬≤). E¬≤ approximates the conditional variance at each point in state space and for a time horizon T ‚àà Tw, using K nearest neighbors. For each reference point of the state space trajectory, the neighborhood consists of the reference point itself and its K+1 nearest neighbors. E¬≤ measures how strong a neighborhood expands during T time steps. E¬≤ is averaged over many time horizons T = 1:Tw. Consequently, œµ¬≤ is the size of the neighborhood at the reference point itself and is defined as the mean pairwise distance of the neighborhood. Finally, œÉ¬≤ gets averaged over a range of reference points on the attractor, which is controlled by samplesize. This is just for performance reasons and the most accurate result will obviously be gained when setting samplesize=1.0\n\nThe Œ±-statistic is a normalization factor, such that œÉ's from different embeddings can be compared. Œ±¬≤ is defined as the inverse of the sum of the inverse of all œµ¬≤'s for all considered reference points.\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.garcia_almeida_embedding","page":"Optimal DCE Parameters","title":"DelayEmbeddings.garcia_almeida_embedding","text":"garcia_almeida_embedding(s; kwargs...) ‚Üí Y, œÑ_vals, ts_vals, FNNs ,NS\n\nA unified approach to properly embed a time series (Vector type) or a set of time series (Dataset type) based on the papers of Garcia & Almeida [Garcia2005a],[Garcia2005b].\n\nKeyword arguments\n\nœÑs= 0:50: Possible delay values œÑs (in sampling time units). For each of the œÑs's the N-statistic gets computed.\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nr1 = 10: The threshold, which defines the factor of tolerable stretching for the dE1-statistic (see algorithm description in [`garciaembedding_cycle`](@ref)).\nr2 = 2: The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension.\nfnn_thres= 0.05: A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ‚â§ fnn_thres < 1).\nT::Int = 1: The forward time step (in sampling units) in order to compute the d_E2-statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to T=1.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input phase space trajectory Y.\nmax_num_of_cycles = 50: The algorithm will stop after that many cycles no matter what.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding vectors Y. Based on the N-statistic garcia_embedding_cycle the algorithm picks an optimal delay value œÑ for each embedding cycle as the first local minimum of N. In case of multivariate embedding, i.e. when embedding a set of time series (s::Dataset), the optimal delay value œÑ is chosen as the first minimum from all minimum's of all considered N-statistics for each embedding cycle. The range of considered delay values is determined in œÑs and for the nearest neighbor search we respect the Theiler window w. After each embedding cycle the FNN-statistic FNNs [Hegger1999][Kennel1992] is being checked and as soon as this statistic drops below the threshold fnn_thres, the algorithm breaks. In order to increase the  practability of the method the algorithm also breaks, when the FNN-statistic FNNs increases . The final embedding vector is stored in Y (Dataset). The chosen delay values for each embedding cycle are stored in the œÑ_vals and the according time series number chosen for the according delay value in œÑ_vals is stored in ts_vals. For univariate embedding (s::Vector) ts_vals is a vector of ones of length œÑ_vals, because there is simply just one time series to choose from. The function also returns the N-statistic NS for each embedding cycle as an Array of Vectors.\n\nNotice that we were not able to reproduce the figures from the papers with our implementation (which nevertheless we believe is the correct one).\n\n[Garcia2005a]: Garcia, S. P., Almeida, J. S. (2005). Nearest neighbor embedding with different time delays. Physical Review E 71, 037204.\n\n[Garcia2005b]: Garcia, S. P., Almeida, J. S. (2005). Multivariate phase space reconstruction by nearest neighbor embedding with different time delays. Physical Review E 72, 027205.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.mdop_embedding","page":"Optimal DCE Parameters","title":"DelayEmbeddings.mdop_embedding","text":"mdop_embedding(s::Vector; kwargs...) ‚Üí Y, œÑ_vals, ts_vals, FNNs, Œ≤S\n\nMDOP (for \"maximizing derivatives on projection\") is a unified approach to properly embed a timeseries or a set of timeseries (Dataset) based on the paper of Chetan Nichkawde [Nichkawde2013].\n\nKeyword arguments\n\nœÑs= 0:50: Possible delay values œÑs. For each of the œÑs's the Œ≤-statistic gets computed.\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nfnn_thres::Real= 0.05: A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ‚â§ fnn_thres < 1).\nr::Real = 2: The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension.\nmax_num_of_cycles = 50: The algorithm will stop after that many cycles no matter what.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding Y. Based on the beta_statistic the algorithm picks an optimal delay value œÑ for each embedding cycle as the global maximum of Œ≤. In case of multivariate embedding, i.e. when embedding a set of time series (s::Dataset), the optimal delay value œÑ is chosen as the maximum from all maxima's of all considered Œ≤-statistics for each possible timeseries. The range of considered delay values is determined in œÑs and for the nearest neighbor search we respect the Theiler window w.\n\nAfter each embedding cycle the FNN-statistic FNNs [Hegger1999][Kennel1992] is being checked and as soon as this statistic drops below the threshold fnn_thres, the algorithm terminates. In order to increase the practability of the method the algorithm also terminates when the FNN-statistic FNNs increases.\n\nThe final embedding is returned as Y. The chosen delay values for each embedding cycle are stored in the œÑ_vals and the according timeseries index chosen for the the respective according delay value in œÑ_vals is stored in ts_vals. Œ≤S, FNNs are returned for clarity and double-checking, since they are computed anyway. In case of multivariate embedding, Œ≤S will store all Œ≤-statistics for all available time series in each embedding cycle. To double-check the actual used Œ≤-statistics in an embedding cycle 'k', simply Œ≤S[k][:,ts_vals[k+1]].\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Hegger1999]: Hegger, Rainer and Kantz, Holger (1999). Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970.\n\n[Kennel1992]: Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992). Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.pecuzal_embedding","page":"Optimal DCE Parameters","title":"DelayEmbeddings.pecuzal_embedding","text":"pecuzal_embedding(s; kwargs...) ‚Üí ùíü, œÑ_vals, ts_vals, Ls, ‚ü®Œµ‚òÖ‚ü©\n\nA unified approach to properly embed a time series or a set of time series (Dataset) based on the ideas of Pecora et al. [Pecoral2007] and Uzal et al. [Uzal2011]. For a detailled description of the algorithm see Kraemer et al. [Kraemer2020].\n\nKeyword arguments\n\nœÑs = 0:50: Possible delay values œÑs (in sampling time units). For each of the œÑs's the continuity statistic ‚ü®Œµ‚òÖ‚ü© gets computed and further processed in order to find optimal delays œÑ·µ¢ for each embedding cycle i (read algorithm description).\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nsamplesize::Real = 1: determine the fraction of all phase space points (=length(s)) to be considered (fiducial points v) to average Œµ‚òÖ, in order to produce ‚ü®Œµ‚òÖ‚ü©.\nK::Int = 13: the amount of nearest neighbors in the Œ¥-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic). ‚ü®Œµ‚òÖ‚ü© is computed taking the minimum result over all k ‚àà K.\nKNN::Int = 3: the amount of nearest neighbors considered, in order to compute œÉk^2 (read algorithm description [`uzalcost]@ref). If given a vector, the minimum result over allknn ‚àà KNN` is returned.\nTw::Int = 4*w: the maximal considered time horizon for obtaining œÉk^2 (read  algorithm description [`uzalcost`]@ref).\nŒ±::Real = 0.05: The significance level for obtaining the continuity statistic\np::Real = 0.5: The p-parameter for the binomial distribution used for the computation of the continuity statistic ‚ü®Œµ‚òÖ‚ü©.\nmax_cycles = 50: The algorithm will stop after that many cycles no matter what.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding vectors ùíü. Based on the ‚ü®Œµ‚òÖ‚ü©-statistic pecora the algorithm picks an optimal delay value œÑ·µ¢ for each embedding cycle i. For achieving that, we take the inpute time series s and compute the continuity statistic ‚ü®Œµ‚òÖ‚ü©. 1. Each local maxima in ‚ü®Œµ‚òÖ‚ü© is used for constructing a candidate embedding trajectory ùíü_trial with a delay corresponding to that specific peak in ‚ü®Œµ‚òÖ‚ü©. 2. We then compute the L-statistic uzal_cost for ùíü_trial. 3. We pick the peak/œÑ-value, for which L is minimal and construct the actual embedding trajectory ùíü_actual (steps 1.-3. correspond to an embedding cycle). 4. We repeat steps 1.-3. with ùíü_actual as input and stop the algorithm when L can not be reduced anymore. ùíü_actual -> ùíü.\n\nIn case of multivariate embedding, i.e. when embedding a set of M time series (s::Dataset), in each embedding cycle the continuity statistic ‚ü®Œµ‚òÖ‚ü© gets computed for all M time series available. The optimal delay value œÑ in each embedding cycle is chosen as the peak/œÑ-value for which L is minimal under all available peaks and under all M ‚ü®Œµ‚òÖ‚ü©'s. In the first embedding cycle there will be M^2 different ‚ü®Œµ‚òÖ‚ü©'s to consider, since it is not clear a priori which time series of the input should consitute the first component of the embedding vector and form ùíü_actual.\n\nThe range of considered delay values is determined in œÑs and for the nearest neighbor search we respect the Theiler window w. The final embedding vector is stored in ùíü (Dataset). The chosen delay values for each embedding cycle are stored in œÑ_vals and the according time series numbers chosen for each delay value in œÑ_vals are stored in ts_vals. For univariate embedding (s::Vector) ts_vals is a vector of ones of length œÑ_vals, because there is simply just one time series to choose from. The function also returns the L-statistic Ls for each embedding cycle and the continuity statistic ‚ü®Œµ‚òÖ‚ü© as an Array of Vectors.\n\nFor distance computations the Euclidean norm is used.\n\n[Pecora2007]: Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007). A unified approach to attractor reconstruction. Chaos 17(1).\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n[Kraemer2020]: Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2020). A unified and automated approach to attractor reconstruction. arXiv:2011.07040.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#Example-2","page":"Optimal DCE Parameters","title":"Example","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"In following we illustrate the functionality of the PECUZAL method on three examples. We start with a univariate case, i.e. we only feed in one time series, here the x-component of the Lorenz system.  ","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"using DelayEmbeddings\n\nlo = Systems.lorenz([1.0, 1.0, 50.0])\ntr = trajectory(lo, 100; dt = 0.01, Ttr = 10)\n\ns = vec(tr[:, 1]) # input timeseries = x component of lorenz\ntheiler = estimate_delay(s, \"mi_min\") # estimate a Theiler window\nTmax = 100 # maximum possible delay\n\nY, œÑ_vals, ts_vals, Ls , Œµs = pecuzal_embedding(s; œÑs = 0:Tmax , w = theiler)\n\nprintln(œÑ_vals)\nprintln(ts_vals)\nprintln(Ls)","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"The output reveals that PECUZAL suggests a 3-dimensional embedding out of the un-lagged time series as the 1st component of the reconstruction, the time series lagged by 18 samples as the 2nd component and the time series lagged by 9 samples as the 3rd component. The minimum obtained L-value in the 3rd embedding cycle has been ~-2.63, after which the algorithm breaks.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"using PyPlot\n\nfigure(figsize=(14., 8.))\nsubplot(1,2,1, projection=\"3d\")\nplot3D(Y[:,1], Y[:,2], Y[:,3],\"gray\")\ntitle(\"PECUZAL reconstructed x-component of Lorenz System\")\nxlabel(\"x(t+$(œÑ_vals[1]))\")\nylabel(\"x(t+$(œÑ_vals[2]))\")\nzlabel(\"x(t+$(œÑ_vals[3]))\")\ngrid()\n\nsubplot(1,2,2, projection=\"3d\")\nplot3D(tr[:,1], tr[:,2], tr[:,3],\"gray\")\ntitle(\"Original Lorenz System\")\nxlabel(\"x(t)\")\nylabel(\"y(t)\")\nzlabel(\"z(t)\")\ngrid()\n\ntight_layout()\nsavefig(\"pecuzal_uni.png\"); nothing # hide","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"(Image: )","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"We can also look at the output of the low-level function leading to the results, here the continuity statistic.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"using PyPlot\n\nfigure(figsize=(8., 5.))\nplot(Œµs[:,1], label=\"1st embedding cycle\")\nscatter([œÑ_vals[2]], [Œµs[œÑ_vals[2],1]])\nplot(Œµs[:,2], label=\"2nd embedding cycle\")\nscatter([œÑ_vals[3]], [Œµs[œÑ_vals[3],2]])\nplot(Œµs[:,3], label=\"3rd embedding cycle\")\ntitle(\"Continuity statistics for PECUZAL embedding of Lorenz x-component\")\nxlabel(\"delay œÑ\")\nylabel(\"‚ü®Œµ‚ãÜ‚ü©\")\nlegend(loc=\"upper left\")\ngrid()\nsavefig(\"continuity_uni.png\"); nothing # hide","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"(Image: )","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"Similar to the approach in the preceding example, we now highlight the capability of the PECUZAL embedding method for a multivariate input. The idea is now to feed in all three time series to the algorithm, even though this is a very far-from-reality example. We already have an adequate representation of the system we want to reconstruct, namely the three time series from the numerical integration. But let us see what PECUZAL suggests for a reconstruction.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"# compute Theiler window\nw1 = estimate_delay(tr[:,1], \"mi_min\")\nw2 = estimate_delay(tr[:,2], \"mi_min\")\nw3 = estimate_delay(tr[:,3], \"mi_min\")\nw = maximum(hcat(w1,w2,w3))\nY_m, œÑ_vals_m, ts_vals_m, Ls_m , Œµs_m = pecuzal_embedding(tr; œÑs = 0:Tmax , w = theiler)\n\nprintln(œÑ_vals_m)\nprintln(ts_vals_m)\nprintln(Ls_m)","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"PECUZAL offers a 3-dimensional embedding using the un-lagged z- and x-component as 1st and 3rd component of the reconstruction vectors, as well as the x-component lagged by 12 samples.","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"\nts_str = [\"x\", \"y\", \"z\"]\n\nfigure(figsize=(14., 8.))\nsubplot(1,2,1, projection=\"3d\")\nplot3D(Y_m[:,1], Y_m[:,2], Y_m[:,3],\"gray\")\ntitle(\"PECUZAL reconstructed Lorenz System\")\nxlabel(\"$(ts_str[ts_vals_m[1]])(t+$(œÑ_vals_m[1]))\")\nylabel(\"$(ts_str[ts_vals_m[2]])(t+$(œÑ_vals_m[2]))\")\nzlabel(\"$(ts_str[ts_vals_m[3]])(t+$(œÑ_vals_m[3]))\")\ngrid()\n\nsubplot(1,2,2, projection=\"3d\")\nplot3D(tr[:,1], tr[:,2], tr[:,3],\"gray\")\ntitle(\"Original Lorenz System\")\nxlabel(\"x(t)\")\nylabel(\"y(t)\")\nzlabel(\"z(t)\")\ngrid()\n\ntight_layout()\nsavefig(\"pecuzal_multi.png\"); nothing # hide","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"(Image: )","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"Finally we show what PECUZAL does with a non-deterministic source:","category":"page"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"using Random\n\n# Dummy input\nd1 = randn(1000)\nd2 = rand(1000)\nTmax = 100\ndummy_set = Dataset(hcat(d1,d2))\n\nw1 = estimate_delay(d1, \"mi_min\")\nw2 = estimate_delay(d2, \"mi_min\")\ntheiler = minimum(hcat(w1,w2))\n\nY_d, œÑ_vals_d, ts_vals_d, Ls_d , Œµ‚òÖ_d = pecuzal_embedding(dummy_set; œÑs = 0:Tmax , w = theiler)","category":"page"},{"location":"embedding/estimate/#Low-level-functions-of-unified-approach-1","page":"Optimal DCE Parameters","title":"Low-level functions of unified approach","text":"","category":"section"},{"location":"embedding/estimate/#","page":"Optimal DCE Parameters","title":"Optimal DCE Parameters","text":"DelayEmbeddings.n_statistic\nDelayEmbeddings.beta_statistic\nDelayEmbeddings.mdop_maximum_delay","category":"page"},{"location":"embedding/estimate/#DelayEmbeddings.n_statistic","page":"Optimal DCE Parameters","title":"DelayEmbeddings.n_statistic","text":"n_statistic(Y, s; kwargs...) ‚Üí N, d_E1\n\nPerform one embedding cycle according to the method proposed in [Garcia2005a] for a given phase space trajectory Y (of type Dataset) and a time series s (of typeVector). Return the proposed N-StatisticNand all nearest neighbor distancesd_E1for each point of the input phase space trajectoryY. Note thatY` is a single time series in case of the first embedding cycle.\n\nKeyword arguments\n\nœÑs= 0:50: Considered delay values œÑs (in sampling time units). For each of the œÑs's the N-statistic gets computed.\nr = 10: The threshold, which defines the factor of tolerable stretching for the d_E1-statistic (see algorithm description).\nT::Int = 1: The forward time step (in sampling units) in order to compute the d_E2-statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to T=1.\nw::Int = 0: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors. Note that in the paper this is not a free parameter and always w=0.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input phase space trajectory Y.\n\nDescription\n\nFor a range of possible delay values œÑs one constructs a temporary embedding matrix. That is, one concatenates the input phase space trajectory Y with the œÑ-lagged input time series s. For each point on the temporary trajectory one computes its nearest neighbor, which is denoted as the d_E1-statistic for a specific œÑ. Now one considers the distance between the reference point and its nearest neighbor T sampling units ahead and calls this statistic d_E2. [Garcia2005a] strictly use T=1, so they forward each reference point and its corresponding nearest neighbor just by one (!) sampling unit. Here it is a free parameter.\n\nThe N-statistic is then the fraction of d_E2/d_E1-pairs which exceed a threshold r.\n\nPlotted vs. the considered œÑs-values it is proposed to pick the œÑ-value for this embedding cycle as the value, where N has its first local minimum.\n\n[Garcia2005a]: Garcia, S. P., Almeida, J. S. (2005). Nearest neighbor embedding with different time delays. Physical Review E 71, 037204.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.beta_statistic","page":"Optimal DCE Parameters","title":"DelayEmbeddings.beta_statistic","text":"beta_statistic(Y::Dataset, s::Vector) [, œÑs, w]) ‚Üí Œ≤\n\nCompute the Œ≤-statistic Œ≤ for input state space trajectory Y and a timeseries s according to Nichkawde [Nichkawde2013], based on estimating derivatives on a projected manifold. For a range of delay values œÑs, Œ≤ gets computed and its maximum over all considered œÑs serves as the optimal delay considered in this embedding cycle.\n\nArguments œÑs, w as in mdop_embedding.\n\nDescription\n\nThe Œ≤-statistic is based on the geometrical idea of maximal unfolding of the reconstructed attractor and is tightly related to the False Nearest Neighbor method ([Kennel1992]). In fact the method eliminates the maximum amount of false nearest neighbors in each embedding cycle. The idea is to estimate the absolute value of the directional derivative with respect to a possible new dimension in the reconstruction process, and with respect to the nearest neighbor, for all points of the state space trajectory:\n\nœï'(œÑ) = Œîœïd(œÑ) / Œîxd\n\nŒîxd is simply the Euclidean nearest neighbor distance for a reference point with respect to the given Theiler window w. Œîœïd(œÑ) is the distance of the reference point to its nearest neighbor in the one dimensional time series s, for the specific œÑ. Œîœï_d(œÑ) = |s(i+œÑ)-s(j+œÑ)|, with i being the index of the considered reference point and j the index of its nearest neighbor.\n\nFinally,\n\nŒ≤ = log Œ≤(œÑ) = ‚ü®log‚ÇÅ‚ÇÄ œï'(œÑ)‚ü© ,\n\nwith ‚ü®.‚ü© being the mean over all reference points. When one chooses the maximum of Œ≤ over all considered œÑ's, one obtains the optimal delay value for this embedding cycle. Note that in the first embedding cycle, the input state space trajectory Y can also be just a univariate time series.\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Kennel1992]: Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992). Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403.\n\n\n\n\n\n","category":"function"},{"location":"embedding/estimate/#DelayEmbeddings.mdop_maximum_delay","page":"Optimal DCE Parameters","title":"DelayEmbeddings.mdop_maximum_delay","text":"mdop_maximum_delay(s, tw = 1:50, samplesize = 1.0)) -> œÑ_max, L\n\nCompute an upper bound for the search of optimal delays, when using mdop_embedding mdop_embedding or beta_statistic beta_statistic.\n\nDescription\n\nThe input time series s gets embedded with unit lag and increasing dimension, for dimensions (or time windows) tw (RangeObject). For each of such a time window the L-statistic from Uzal et al. [Uzal2011] will be computed. samplesize determines the fraction of points to be considered in the computation of L (see uzal_cost). When this statistic reaches its global minimum the maximum delay value œÑ_max gets returned. When s is a multivariate Dataset, œÑ_max will becomputed for all timeseries of that Dataset and the maximum value will be returned. The returned L-statistic has size (length(tw), size(s,2)).\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n\n\n\n\n","category":"function"},{"location":"contributors_guide/#Contributor-Guide-1","page":"Contributor Guide","title":"Contributor Guide","text":"","category":"section"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"*TL;DR: See \"good first issues\" or \"wanted features\". *","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"The ultimate goal for DynamicalSystems.jl is to be a useful library for scientists working on chaos, nonlinear dynamics and in general dynamical systems. We don't want to have \"just code\", but also detailed descriptions and references for as many methods as possible.","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"For this to be achieved, many of us should try to work together to improve the library!","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"If you want to help the cause, there are many ways to contribute to the DynamicalSystems.jl library:","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"Just use it. If you encountered unexpected behavior simply report it either on our gitter chatroom or using the DynamicalSystems.jl Issues page.\nSuggest methods that you think should be included in our library. This should be done by opening a new issue that describes the method, gives references to papers using the method and also justifies why the method should be included.\nContribute code by solving issues. The easiest issues to tackle are the ones with label \"good first issue\".\nContribute code by implementing new methods! That is the most awesome way to contribute! The individual packages that compose DynamicalSystems.jl have plenty of issues with the tag \"wanted feature\", which can get you started on a big contribution!\nContribute code by defining a new pre-defined dynamical system that you found useful.","category":"page"},{"location":"contributors_guide/#Contributing-Code-1","page":"Contributor Guide","title":"Contributing Code","text":"","category":"section"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"When contributing code, you should keep these things in mind:","category":"page"},{"location":"contributors_guide/#","page":"Contributor Guide","title":"Contributor Guide","text":"In general, the speed of the implementation is important, but not as important as the clarity of the implementation. One of cornerstones of all of DynamicalSystems.jl is to have clear and readable source code. Fortunately, Julia allows you to have perfectly readable code but also super fast ;) If necessary add comments to the code, so that somebody that knows the method, can also understand the code immediately.\nFor the documentation strings of new methods and systems please follow the convention of the documentation strings of DynamicalSystems.jl. Specifically, the first section should describe the function in a couple of sentences, its positional arguments and its return value. The next section ## Keyword Arguments describes the keywords. The next section ## Description describes the algorithm in detail if need be.\nAlways have a reference to the original work that introduces the method or the system that you are using. You should put this reference to the main function's documentation string. See the existing documentation strings and do it in a similar manner.","category":"page"},{"location":"contents/#Contents-1","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"The module DynamicalSystems re-exports all following functionality.","category":"page"},{"location":"contents/#Core-types-1","page":"Contents","title":"Core types","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Intuitive, consistent APIs for the definition of general dynamical systems under a unified struct DynamicalSystem. The following combinations are possible:\nContinuous or Discrete systems. Continuous systems use DifferentialEquations.jl for solving the ODE problem.\nIn-place or out-of-place (large versus small systems).\nAuto-differentiated or not (for the Jacobian function).","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Automatic \"completion\" of the dynamics of the system with numerically computed Jacobians, in case they are not provided by the user.\nRobust implementations of all kinds of integrators, that evolve the system, many states of the system, or even deviation vectors. See the Advanced documentation for this.\nLibrary of Predefined Dynamical Systems that have been used extensively in scientific research.\nUnified & dedicated interface for numerical data: Dataset.","category":"page"},{"location":"contents/#[Delay-Coordinates-Embedding](@ref)-1","page":"Contents","title":"Delay Coordinates Embedding","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Performing delay coordinate embeddings and finding optimal parameters for doing so.","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Flexible, super-efficient and abstracted Delay Coordinates Embedding interface.\nSupports multiple dimensions and multiple timescales.","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Methods that estimate optimal embedding parameters: Optimal DCE Parameters.\nUnified approach of finding optimal embeddings (advanced algorithms).\nFast calculation of mutual information: mutualinformation.\nUnified neighborhood interface.","category":"page"},{"location":"contents/#ChaosTools-1","page":"Contents","title":"ChaosTools","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"ChaosTools.jl is a collection of many algorithms for (chaotic or not) dynamical systems. All algorithms are independent of each other but they are also not expansive enough to be a standalone package.","category":"page"},{"location":"contents/#[Orbit-Diagrams-and-PSOS](@ref)-1","page":"Contents","title":"Orbit Diagrams & PSOS","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Orbit diagrams (aka bifurcation diagrams) of maps: orbitdiagram.\nPoincar√© surfaces of section for continuous systems: poincaresos.\nAutomated production of orbit diagrams for continuous systems: produce_orbitdiagram.","category":"page"},{"location":"contents/#[Lyapunov-Exponents](@ref)-1","page":"Contents","title":"Lyapunov Exponents","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"The following treat systems where the equations of motion are known:","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Maximum Lyapunov exponent for both discrete and continuous systems: lyapunov.\nLyapunov spectrum for both discrete and continuous systems: lyapunovs.","category":"page"},{"location":"contents/#[Detecting-and-Categorizing-Chaos](@ref)-1","page":"Contents","title":"Detecting & Categorizing Chaos","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"The Generalized Alignment Index: textGALI_k : gali.\nImplemented for both discrete and continuous systems.\nA test to categorize strong chaos, partially predictable chaos and regular behavior: predictability.\nImplemented for both discrete and continuous systems.\nThe 0-1 test for chaos: testchaos01\nThe expansion entropy: expansionentropy.","category":"page"},{"location":"contents/#[Entropies-and-Dimensions](@ref)-1","page":"Contents","title":"Entropies and Dimensions","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Generalized (Renyi) entropy: genentropy.\nPermutation entropy: permentropy.\nFast and cheap (memory-wise) method for computing entropies of large datasets.\nGeneralized dimensions (e.g. capacity dimension, information dimension, etc.): generalized_dim.\nKaplan-Yorke dimension: kaplanyorke_dim.","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"And, in order to automatically deduce dimensions, we also offer methods for:","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Partitioning a function y(x) vs. x into regions where it is approximated by a straight line, using a flexible algorithm with a lot of control over the outcome. See linear_regions.\nDetection of largest linear region of a function y(x) vs. x and extraction of the slope of this region.","category":"page"},{"location":"contents/#[Nonlinear-Timeseries-Analysis](@ref)-1","page":"Contents","title":"Nonlinear Timeseries Analysis","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Broomhead-King coordinates: broomhead_king.\nNumerically determining the maximum Lyapunov exponent of a (e.g. experimentally) measured timeseries: numericallyapunov.","category":"page"},{"location":"contents/#[Periodicity-and-Ergodicity](@ref)-1","page":"Contents","title":"Periodicity & Ergodicity","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Numerical method to find unstable and stable fixed points of any order n of a discrete map (of any dimensionality): periodicorbits.\nConvenience functions for defining and realizing all possible combinations of mathbfLambda_k matrices required in the above method.\nEstimating the period of a timeseries: estimate_period.\nReturn and transit time statistics for a subset of the state space: mean_return_times, exit_entry_times.","category":"page"},{"location":"contents/#Recurrence-Analysis-1","page":"Contents","title":"Recurrence Analysis","text":"","category":"section"},{"location":"contents/#","page":"Contents","title":"Contents","text":"RecurrenceAnalysis.jl offers tools to compute and analyze Recurrence Plots, a field called Recurrence Quantification Analysis.","category":"page"},{"location":"contents/#","page":"Contents","title":"Contents","text":"Recurrence Plots, with cross-recurrence and joint-recurrence.\nRecurrence Quantification Analysis (RQA):\nRecurrence rate, determinism, average/maximum diagonal length, divergence, laminarity, trend, entropy, trapping time, average/maximum vertical length.\nFine-tuning of the algorithms that compute the above (e.g. Theiler window and many more)\nWindowed RQA of the above","category":"page"},{"location":"chaos/orbitdiagram/#Orbit-Diagrams-and-PSOS-1","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"section"},{"location":"chaos/orbitdiagram/#Orbit-Diagrams-of-Maps-1","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams of Maps","text":"","category":"section"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"An orbit diagram (also called bifurcation diagram) is a way to visualize the asymptotic behavior of a map, when a parameter of the system is changed","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"orbitdiagram","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.orbitdiagram","page":"Orbit Diagrams & PSOS","title":"ChaosTools.orbitdiagram","text":"orbitdiagram(ds::DiscreteDynamicalSystem, i, p_index, pvalues; kwargs...)\n\nCompute the orbit diagram (also called bifurcation diagram) of the given system, saving the i variable(s) for parameter values pvalues. The p_index specifies which parameter of the equations of motion is to be changed.\n\ni can be Int or AbstractVector{Int}. If i is Int, returns a vector of vectors. Else it returns vectors of vectors of vectors. Each entry are the points at each parameter value.\n\nKeyword Arguments\n\nTtr::Int = 1000 : Transient steps; each orbit is evolved for Ttr first before saving output.\nn::Int = 100 : Amount of points to save for each initial condition.\ndt = 1 : Stepping time. Changing this will give you the orbit diagram of the dt order map.\nu0 = get_state(ds) : Initial condition. Besides a vector you can also give a vector of vectors such that length(u0) == length(pvalues). Then each parameter has a different initial condition.\nulims = (-Inf, Inf) : only record system states within ulims (only valid if i isa Int).\n\nSee also poincaresos and produce_orbitdiagram.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"For example, let's compute the famous orbit diagram of the logistic map:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems\nusing PyPlot\n\nds = Systems.logistic()\ni = 1\npvalues = 3:0.001:4\nics = [rand() for m in 1:10]\nn = 2000\nTtr = 2000\np_index = 1\noutput = orbitdiagram(ds, i, p_index, pvalues; n = n, Ttr = Ttr)\n\nL = length(pvalues)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= pvalues[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfigure()\nPyPlot.title(\"total points: $(L*n)\")\nplot(x, y, ls = \"None\", ms = 0.5, color = \"black\", marker = \"o\", alpha = 0.05)\nxlim(pvalues[1], pvalues[end]); ylim(0,1)\nxlabel(\"\\$r\\$\"); ylabel(\"\\$x\\$\")\ntight_layout()\nsavefig(\"logostic_od.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Notice that if you are using PyPlot, the plotting process will be slow, since it is slow at plotting big numbers of points.","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"The function is not limited to 1D maps, and can be applied just as well to any discrete system.","category":"page"},{"location":"chaos/orbitdiagram/#Poincar√©-Surface-of-Section-1","page":"Orbit Diagrams & PSOS","title":"Poincar√© Surface of Section","text":"","category":"section"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Also called Poincar√© map is a technique to reduce a continuous system into a discrete map with 1 less dimension. We are doing this using the function:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"poincaresos","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.poincaresos","page":"Orbit Diagrams & PSOS","title":"ChaosTools.poincaresos","text":"poincaresos(ds::ContinuousDynamicalSystem, plane, tfinal = 1000.0; kwargs...)\n\nCalculate the Poincar√© surface of section (also called Poincar√© map)[Tabor1989] of the given system with the given plane. The system is evolved for total time of tfinal. Return a Dataset of the points that are on the surface of section.\n\nIf the state of the system is mathbfu = (u_1 ldots u_D) then the equation defining a hyperplane is\n\na_1u_1 + dots + a_Du_D = mathbfacdotmathbfu=b\n\nwhere mathbfa b are the parameters of the hyperplane.\n\nIn code, plane can be either:\n\nA Tuple{Int, <: Number}, like (j, r) : the plane is defined as when the j variable of the system equals the value r.\nA vector of length D+1. The first D elements of the vector correspond to mathbfa while the last element is b.\n\nThis function uses ds and higher order interpolation from DifferentialEquations.jl to create a high accuracy estimate of the section. See also produce_orbitdiagram.\n\nKeyword Arguments\n\ndirection = -1 : Only crossings with sign(direction) are considered to belong to the surface of section. Positive direction means going from less than b to greater than b.\nidxs = 1:dimension(ds) : Optionally you can choose which variables to save. Defaults to the entire state.\nTtr = 0.0 : Transient time to evolve the system before starting to compute the PSOS.\nu0 = get_state(ds) : Specify an initial state.\nwarning = true : Throw a warning if the Poincar√© section was empty.\nrootkw = (xrtol = 1e-6, atol = 1e-6) : A NamedTuple of keyword arguments passed to find_zero from Roots.jl.\ndiffeq... : All other extra keyword arguments are propagated into init of DifferentialEquations.jl. See trajectory for examples.\n\nPerformance Notes\n\nThis function uses a standard integrator. For loops over initial conditions and/or parameters you should use the low level method that accepts an integrator and reinit! to new initial conditions. See the \"advanced documentation\" for more.\n\nThe low level call signature is:\n\npoincaresos(integ, planecrossing, tfinal, Ttr, idxs, rootkw)\n\nwhere\n\nplanecrossing = PlaneCrossing(plane, direction > 0)\n\nand idxs must be Int or SVector{Int}.\n\n[Tabor1989]: M. Tabor, Chaos and Integrability in Nonlinear Dynamics: An Introduction, ¬ß4.1, in pp. 118-126, New York: Wiley (1989)\n\n\n\n\n\npoincaresos(A::Dataset, plane; kwargs...)\n\nCalculate the Poincar√© surface of section of the given dataset with the given plane by performing linear interpolation betweeen points that sandwich the hyperplane.\n\nArgument plane and keywords direction, warning, idxs are the same as above.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Here is an example of the Henon-Heiles system showing the mixed nature of the phase space","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems, PyPlot\n\nhh = Systems.henonheiles()\n\nplane = (1, 0.0)\nu0s = [[0.0, -0.25, 0.42081, 0.0],\n[0.0, -0.31596, 0.354461, 0.0591255],\n[0.0, 0.1, 0.5, 0.0],\n[0.0, -0.0910355, 0.459522, -0.173339],\n[0.0, -0.205144, 0.449328, -0.0162098]]\n\nfigure()\nfor u0 in u0s\n    psos = poincaresos(hh, plane, 20000.0; u0 = u0)\n    scatter(psos[:, 2], psos[:, 4], s = 2.0)\nend\nxlabel(\"\\$q_2\\$\"); ylabel(\"\\$p_2\\$\")\nsavefig(\"hhpsos.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Here the surface of section was the (hyper-) plane that q_1 = 0. Some chaotic and regular orbits can be seen in the plot. You can tell the regular orbits apart because they look like a single connected curve. This is the result of cutting a 2-torus by a plane!","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Finally here is one more example with a more complex hyperplane:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"gis = Systems.gissinger([2.32865, 2.02514, 1.98312])\n\n# Define appropriate hyperplane for gissinger system\nconst ŒΩ = 0.1\nconst Œì = 0.9 # default parameters of the system\n\n# I want hyperperplane defined by these two points:\nNp(Œº) = SVector{3}(sqrt(ŒΩ + Œì*sqrt(ŒΩ/Œº)), -sqrt(Œº + Œì*sqrt(Œº/ŒΩ)), -sqrt(Œº*ŒΩ))\nNm(Œº) = SVector{3}(-sqrt(ŒΩ + Œì*sqrt(ŒΩ/Œº)), sqrt(Œº + Œì*sqrt(Œº/ŒΩ)), -sqrt(Œº*ŒΩ))\n\n# Create hyperplane passing through Np, Nm and 0:\nusing LinearAlgebra\ngis_plane(Œº) = [cross(Np(Œº), Nm(Œº))..., 0]\n\nŒº = 0.119\nset_parameter!(gis, 1, Œº)\nfigure(figsize = (8,6))\npsos = poincaresos(gis, gis_plane(Œº), 10000.0, Ttr = 200.0,)\nplot3D(columns(psos)..., marker = \"o\", ls = \"None\", ms = 2.0);\nxlabel(\"Q\"); ylabel(\"D\"); zlabel(\"V\");\nsavefig(\"gispsos.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#Stroboscopic-Map-1","page":"Orbit Diagrams & PSOS","title":"Stroboscopic Map","text":"","category":"section"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"A special case of a PSOS is a stroboscopic map, which is defined for non-autonomous systems with periodic time dependence, like e.g. the Systems.duffing oscillator.","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"A \"cut\" through the phase-space can be produced at every period T = 2piomega. There is no reason to use poincaresos for this though, because you can simply use trajectory and get the solution with a certain time sampling rate. For example, this piece of code:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems, Plots\n\nds = Systems.duffing(Œ≤ = -1, œâ = 1, f = 0.3) # non-autonomous chaotic system\n\nframes=120\na = trajectory(ds, 100000.0, dt = 2œÄ/frames, Ttr=20œÄ) # every period T = 2œÄ/œâ\n\norbit_length = div(size(a)[1], frames)\na = Matrix(a)\n\n@gif for i in 1:frames\n    orbit_points = i:frames:(orbit_length*frames)\n    scatter(a[orbit_points, 1], a[orbit_points, 2], markersize=1, html_output_format=:png,\n        leg=false, framestyle=:none, xlims=extrema(a[:,1]), ylims=extrema(a[:,2]))\nend","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Produces this nice animation:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#Producing-Orbit-Diagrams-for-Flows-1","page":"Orbit Diagrams & PSOS","title":"Producing Orbit Diagrams for Flows","text":"","category":"section"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"The orbitdiagram does not make much sense for continuous systems, besides the trivial case where the system is at a fixed point. In order for orbitdiagram to have meaning one must have a map.","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"If only there was a way to turn a continuous system into a map... OH WAIT! That is what poincaresos does! By performing successive surfaces of section at different parameter values, one can indeed \"produce\" an orbit diagram for a flow.","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"We have bundled this process in the following function:","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"produce_orbitdiagram","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.produce_orbitdiagram","page":"Orbit Diagrams & PSOS","title":"ChaosTools.produce_orbitdiagram","text":"produce_orbitdiagram(ds::ContinuousDynamicalSystem, plane, i::Int,\n                     p_index, pvalues; kwargs...)\n\nProduce an orbit diagram (also called bifurcation diagram) for the i variable(s) of the given continuous system by computing Poincar√© surfaces of section using plane for the given parameter values (see poincaresos).\n\ni can be Int or AbstractVector{Int}. If i is Int, returns a vector of vectors. Else it returns a vector of vectors of vectors. Each entry are the points at each parameter value.\n\nKeyword Arguments\n\nprintparams::Bool = false : Whether to print the parameter used during computation in order to keep track of running time.\ndirection, warning, Ttr, rootkw, diffeq... : Propagated into poincaresos.\nu0 = get_state(ds) : Initial condition. Besides a vector you can also give a vector of vectors such that length(u0) == length(pvalues). Then each parameter has a different initial condition.\n\nDescription\n\nFor each parameter, a PSOS reduces the system from a flow to a map. This then allows the formal computation of an \"orbit diagram\" for the i variable of the system, just like it is done in orbitdiagram.\n\nThe parameter change is done as p[p_index] = value taking values from pvalues and thus you must use a parameter container that supports this (either Array, LMArray, dictionary or other).\n\nSee also poincaresos, orbitdiagram.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"For example, we will calculate the orbit diagram of the Shinriki oscillator, a continuous system that undergoes a period doubling route to chaos, much like the logistic map!","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"ds = Systems.shinriki([-2, 0, 0.2])\n\npvalues = range(19, stop = 22, length = 401)\ni = 1\nplane = (2, 0.0)\ntf = 200.0\np_index = 1\n\noutput = produce_orbitdiagram(ds, plane, i, p_index, pvalues;\n                              tfinal = tf, Ttr = 200.0)\n\nfigure()\nfor (j, p) in enumerate(pvalues)\n    plot(fill(p, length(output[j])), output[j], lw = 0,\n    marker = \"o\", ms = 0.2, color = \"black\")\nend\nxlabel(\"\\$R_1\\$\"); ylabel(\"\\$V_1\\$\")\ntight_layout()\nsavefig(\"shinriki.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/#","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/periodicity/#Periodicity-and-Ergodicity-1","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"In this page we describe methods related to the periodic behavior of dynamical systems or univariate timeseries, or related to the ergodic property of chaotic sets.","category":"page"},{"location":"chaos/periodicity/#Stable-and-Unstable-Periodic-Orbits-of-Maps-1","page":"Periodicity & Ergodicity","title":"Stable and Unstable Periodic Orbits of Maps","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Chaotic behavior of low dimensional dynamical systems is affected by the position and the stability properties of the periodic orbits of a dynamical system.","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Finding unstable (or stable) periodic orbits of a discrete mapping analytically rapidly becomes impossible for higher orders of fixed points. Fortunately there is a numeric algorithm due to Schmelcher & Diakonos which allows such a computation. Notice that even though the algorithm can find stable fixed points, it is mainly aimed at unstable ones.","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"The functions periodicorbits and lambdamatrix implement the algorithm:","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"periodicorbits\nlambdamatrix\nlambdaperms","category":"page"},{"location":"chaos/periodicity/#ChaosTools.periodicorbits","page":"Periodicity & Ergodicity","title":"ChaosTools.periodicorbits","text":"periodicorbits(ds::DiscreteDynamicalSystem,\n               o, ics [, Œªs, indss, singss]; kwargs...) -> FP\n\nFind fixed points FP of order o for the map ds using the algorithm due to Schmelcher & Diakonos[Schmelcher1997]. ics is a collection of initial conditions (container of vectors) to be evolved.\n\nOptional Arguments\n\nThe optional arguments Œªs, indss, singss must be containers of appropriate values, besides Œªs which can also be a number. The elements of those containers are passed to: lambdamatrix(Œª, inds, sings), which creates the appropriate mathbfLambda_k matrix. If these arguments are not given, a random permutation will be chosen for them, with Œª=0.001.\n\nKeyword Arguments\n\nmaxiters::Int = 100000 : Maximum amount of iterations an i.c. will be iterated  before claiming it has not converged.\ndisttol = 1e-10 : Distance tolerance. If the 2-norm of a previous state with  the next one is ‚â§ disttol then it has converged to a fixed point.\ninftol = 10.0 : If a state reaches norm(state) ‚â• inftol it is assumed that  it has escaped to infinity (and is thus abandoned).\nroundtol::Int = 4 : The found fixed points are rounded  to roundtol digits before pushed into the list of returned fixed points FP,  if they are not already contained in FP.  This is done so that FP doesn't contain duplicate fixed points (notice  that this has nothing to do with disttol). Turn this to typemax(Int)  to get the full precision of the algorithm.\n\nDescription\n\nThe algorithm used can detect periodic orbits by turning fixed points of the original map ds to stable ones, through the transformation\n\nmathbfx_n+1 = mathbfx_n +\nmathbfLambda_kleft(f^(o)(mathbfx_n) - mathbfx_nright)\n\nwith f = eom. The index k counts the various possible mathbfLambda_k.\n\nPerformance Notes\n\nAll initial conditions are evolved for all mathbfLambda_k which can very quickly lead to long computation times.\n\n[Schmelcher1997]: P. Schmelcher & F. K. Diakonos, Phys. Rev. Lett. 78, pp 4733 (1997)\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.lambdamatrix","page":"Periodicity & Ergodicity","title":"ChaosTools.lambdamatrix","text":"lambdamatrix(Œª, inds::Vector{Int}, sings) -> Œõk\n\nReturn the matrix mathbfLambda_k used to create a new dynamical system with some unstable fixed points turned to stable in the function periodicorbits.\n\nArguments\n\nŒª<:Real : the multiplier of the C_k matrix, with 0<Œª<1.\ninds::Vector{Int} : The ith entry of this vector gives the row of the nonzero element of the ith column of C_k.\nsings::Vector{<:Real} : The element of the ith column of C_k is +1 if signs[i] > 0 and -1 otherwise (sings can also be Bool vector).\n\nCalling lambdamatrix(Œª, D::Int) creates a random mathbfLambda_k by randomly generating an inds and a signs from all possible combinations. The collections of all these combinations can be obtained from the function lambdaperms.\n\nDescription\n\nEach element of inds must be unique such that the resulting matrix is orthogonal and represents the group of special reflections and permutations.\n\nDeciding the appropriate values for Œª, inds, sings is not trivial. However, in ref.[Pingel2000] there is a lot of information that can help with that decision. Also, by appropriately choosing various values for Œª, one can sort periodic orbits from e.g. least unstable to most unstable, see[Diakonos1998] for details.\n\n[Pingel2000]: D. Pingel et al., Phys. Rev. E 62, pp 2119 (2000)\n\n[Diakonos1998]: F. K. Diakonos et al., Phys. Rev. Lett. 81, pp 4349 (1998)\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.lambdaperms","page":"Periodicity & Ergodicity","title":"ChaosTools.lambdaperms","text":"lambdaperms(D) -> indperms, singperms\n\nReturn two collections that each contain all possible combinations of indices (total of D) and signs (total of 2^D) for dimension D (see lambdamatrix).\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"","category":"page"},{"location":"chaos/periodicity/#Standard-Map-example-1","page":"Periodicity & Ergodicity","title":"Standard Map example","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"For example, let's find the fixed points of the Systems.standardmap of order 2, 3, 4, 5, 6 and 8. We will use all permutations for the signs but only one for the inds. We will also only use one Œª value, and a 21√ó21 density of initial conditions.","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"First, initialize everything","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"using DynamicalSystems, PyPlot, StaticArrays\n\nds = Systems.standardmap()\nxs = range(0, stop = 2œÄ, length = 21); ys = copy(xs)\nics = [SVector{2}(x,y) for x in xs for y in ys]\n\n# All permutations of [¬±1, ¬±1]:\nsingss = lambdaperms(2)[2] # second entry are the signs\n\n# I know from personal research I only need this `inds`:\nindss = [[1,2]] # <- must be container of vectors!!!\n\nŒªs = 0.005 # <- only this allowed to not be vector (could also be vector)\n\norders = [2, 3, 4, 5, 6, 8]\nALLFP = Dataset{2, Float64}[];","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Then, do the necessary computations for all orders","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"for o in orders\n    FP = periodicorbits(ds, o, ics, Œªs, indss, singss)\n    push!(ALLFP, FP)\nend","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Plot the phase space of the standard map","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"iters = 1000\ndataset = trajectory(ds, iters)\nfor x in xs\n    for y in ys\n        append!(dataset, trajectory(ds, iters, SVector{2}(x, y)))\n    end\nend\nfigure(figsize = (12,12))\nm = Matrix(dataset)\nPyPlot.scatter(view(m, :, 1), view(m, :, 2), s= 1, color = \"black\")\nPyPlot.xlim(xs[1], xs[end])\nPyPlot.ylim(ys[1], ys[end]);","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"and finally, plot the fixed points","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"markers = [\"D\", \"^\", \"s\", \"p\", \"h\", \"8\"]\ncolors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"grey\"]\n\nfor i in 1:6\n    FP = ALLFP[i]\n    o = orders[i]\n    PyPlot.plot(columns(FP)...,\n    marker=markers[i], color = colors[i], markersize=10.0 + (8-o), linewidth=0.0,\n    label = \"order $o\", markeredgecolor = \"yellow\", markeredgewidth = 0.5)\nend\nlegend(loc=\"upper right\", framealpha=0.9)\nxlabel(\"\\$\\\\theta\\$\")\nylabel(\"\\$p\\$\")\nsavefig(\"fixedpoints.png\"); nothing # hide","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"(Image: Fixed points of the standard map)","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"You can confirm for yourself that this is correct, for many reasons:","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"It is the same fig. 12 of this publication.\nFixed points of order n are also fixed points of order 2n 3n 4n \nBesides fixed points of previous orders, original fixed points of order n come in (possible multiples of) 2n-sized pairs (see e.g. order 5). This is a direct consequence of the Poincar√©‚ÄìBirkhoff theorem.","category":"page"},{"location":"chaos/periodicity/#Estimating-the-Period-1","page":"Periodicity & Ergodicity","title":"Estimating the Period","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"The function estimate_period from ChaosTools offers ways for estimating the period (either exact for periodic timeseries, or approximate for near-periodic ones) of a given timeseries. We offer five methods to estimate periods, some of which work on evenly sampled data only, and others which accept any data. The figure below summarizes this: (Image: )","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period","category":"page"},{"location":"chaos/periodicity/#ChaosTools.estimate_period","page":"Periodicity & Ergodicity","title":"ChaosTools.estimate_period","text":"estimate_period(v::Vector, method, t=0:length(v)-1; kwargs...)\n\nEstimate the period of the signal v, with accompanying time vector t, using the given method.\n\nIf t is an AbstractArray, then it is iterated through to ensure that it's evenly sampled (if necessary for the algorithm).  To avoid this, you can pass any AbstractRange, like a UnitRange or a LinRange, which are defined to be evenly sampled.\n\nMethods requiring evenly sampled data\n\nThese methods are faster, but some are error-prone.\n\n:periodogram or :pg: Use the fast Fourier transform to compute a  periodogram (power-spectrum) of the given data.  Data must be evenly sampled.\n:multitaper or mt: The multitaper method reduces estimation bias by using multiple independent estimates from the same sample. Data tapers are then windowed and the power spectra are obtained.  Available keywords follow: nw is the time-bandwidth product, and ntapers is the number of tapers. If window is not specified, the signal is tapered with ntapers discrete prolate spheroidal sequences with time-bandwidth product nw. Each sequence is equally weighted; adaptive multitaper is not (yet) supported. If window is specified, each column is applied as a taper. The sum of periodograms is normalized by the total sum of squares of window.\n:autocorrelation or :ac: Use the autocorrelation function (AC). The value where the AC first comes back close to 1 is the period of the signal. The keyword L = length(v)√∑10 denotes the length of the AC (thus, given the default setting, this method will fail if there less than 10 periods in the signal). The keyword œµ = 0.2 (\\epsilon) means that 1-œµ counts as \"1\" for the AC.\n\nMethods not requiring evenly sampled data\n\nThese methods tend to be slow, but versatile and low-error.\n\n:lombscargle or :ls: Use the Lomb-Scargle algorithm to compute a periodogram.  The advantage of the Lomb-Scargle method is that it does not require an equally sampled dataset and performs well on undersampled datasets. Constraints have been set on the period, since Lomb-Scargle tends to have false peaks at very low frequencies.  That being said, it's a very flexible method.  It is extremely customizable, and the keyword arguments that can be passed to it are given in the documentation.\n:zerocrossing or :zc: Find the zero crossings of the data, and use the average difference between zero crossings as the period.  This is a na√Øve implementation, with only linear interpolation; however, it's useful as a sanity check.  The keyword line controls where the \"crossing point\" is. It deffaults to mean(v).\n\nFor more information on the periodogram methods, see the documentation of DSP.jl and LombScargle.jl.\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#Example-1","page":"Periodicity & Ergodicity","title":"Example","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Here we will use a modified FitzHugh-Nagumo system that results in periodic behavior, and then try to estimate its period. First, let's see the trajectory:","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"using DynamicalSystems, PyPlot\n\nfunction FHN(u, p, t)\n    e, b, g = p\n    v, w = u\n    dv = min(max(-2 - v, v), 2 - v) - w\n    dw = e*(v - g*w + b)\n    return SVector(dv, dw)\nend\n\ng, e, b  = 0.8, 0.04, 0.0\np0 = [e, b, g]\n\nfhn = ContinuousDynamicalSystem(FHN, SVector(-2, -0.6667), p0)\nT, dt = 1000.0, 0.1\nv = trajectory(fhn, T; dt = dt)[:, 1]\nt = 0:dt:T\n\nfigure()\nplot(0:dt:T, v)\nsavefig(\"fhn_trajectory.png\"); nothing # hide","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"(Image: A periodic trajectory)","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Examining the figure, one can see that the period of the system is around 91 time units. To estimate it numerically let's use some of the methods:","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :autocorrelation, t)","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :periodogram, t)","category":"page"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :zerocrossing, t)","category":"page"},{"location":"chaos/periodicity/#Return-time-statistics-1","page":"Periodicity & Ergodicity","title":"Return time statistics","text":"","category":"section"},{"location":"chaos/periodicity/#","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"mean_return_times\nexit_entry_times","category":"page"},{"location":"chaos/periodicity/#ChaosTools.mean_return_times","page":"Periodicity & Ergodicity","title":"ChaosTools.mean_return_times","text":"mean_return_times(ds::DynamicalSystem, u‚ÇÄ, Œµs, T; kwargs...) ‚Üí œÑ, c\n\nReturn the mean return times to subsets of the state space of ds defined by u‚ÇÄ, Œµs as well as the amount of returns c for each subset. The ds is evolved for a maximum of T time. This function behaves similarly to exit_entry_times and thus see that one for the meaning of u‚ÇÄ and Œµs.\n\nThis function supports both discrete and continuous systems, however the optimizations done in discrete systems (where all nested Œµ-sets are checked at the same time), are not done here yet, which leads to disproportionally lower performance since each Œµ-related set is checked individually from start.\n\nContinuous systems allow for the following keywords:\n\ni=10 How many points to interpolate the trajectory in-between steps to find candidate crossing regions.\nm=10.0 A multiplier. If the trajectory is at least m*Œµ distance away from u0, the algorithm that checks for crossings of the Œµ-set is not initiated.\n\nFor continuous systems T, i, m can be vectors with same size as Œµs, to help increase accuracy of small Œµ.\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.exit_entry_times","page":"Periodicity & Ergodicity","title":"ChaosTools.exit_entry_times","text":"exit_entry_times(dds, u‚ÇÄ, Œµs, T) ‚Üí exits, entries\n\nCollect exit and entry times for a ball/box centered at u‚ÇÄ with radii Œµs (see below), in the state space of the given discrete dynamical system (function not yet available for continuous systems). Return the exit and (re-)entry return times to the set(s), where each of these is a vector containing all collected times for the respective Œµ-radius set, for Œµ ‚àà Œµs.\n\nUse transit_return(exits, entries) to transform the output into transit and return times, and see also mean_return_times for both continuous and discrete systems.\n\nDescription\n\nTransit time statistics are important for the transport properties of dynamical systems[Meiss1997] and can even be connected with the fractal dimension of chaotic sets[Boev2014].\n\nThe current algorithm collects exit and re-entry times to given sets in the state space, which are centered at u‚ÇÄ (algorithm always starts at u‚ÇÄ and the initial state of ds is irrelevant). Œµs is always a Vector.\n\nThe sets around u‚ÇÄ are nested hyper-spheres of radius Œµ ‚àà Œµs, if each entry of Œµs is a real number. The sets can also be hyper-rectangles (boxes), if each entry of Œµs is a vector itself. Then, the i-th box is defined by the space covered by u0 .¬± Œµs[i] (thus the actual box size is 2Œµs[i]!).\n\nThe reason to input multiple Œµs at once is purely for performance.\n\nFor discrete systems, exit time is recorded immediatelly after exitting of the set, and re-entry is recorded immediatelly on re-entry. This means that if an orbit needs 1 step to leave the set and then it re-enters immediatelly on the next step, the return time is 1. For continuous systems high-order interpolation is done to accurately record the time of exactly crossing the Œµ-ball/box.\n\n[Meiss1997]: Meiss, J. D. Average exit time for volume-preserving maps, Chaos (1997)](https://doi.org/10.1063/1.166245)\n\n[Boev2014]: Boev, Vadivasova, & Anishchenko, Poincar√© recurrence statistics as an indicator of chaos synchronization, Chaos (2014)](https://doi.org/10.1063/1.4873721)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#Predefined-Dynamical-Systems-1","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"","category":"section"},{"location":"ds/predefined/#","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"Predefined systems exist in the Systems submodule in the form of functions that return a DynamicalSystem. They are accessed like:","category":"page"},{"location":"ds/predefined/#","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"using DynamicalSystems # or DynamicalSystemsBase\nds = Systems.lorenz(œÅ = 32.0)","category":"page"},{"location":"ds/predefined/#","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"So far, the predefined systems that exist in the Systems sub-module are:","category":"page"},{"location":"ds/predefined/#","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"Modules = [Systems]\nOrder   = [:function]","category":"page"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.antidots","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.antidots","text":"antidots(u; B = 1.0, d0 = 0.3, c = 0.2)\n\nAn antidot \"superlattice\" is a Hamiltonian system that corresponds to a smoothened periodic Sinai billiard with disk diameter d0 and smooth factor c [1].\n\nThis version is the two dimensional classical form of the system, with quadratic equations of motion and a perpendicular magnetic field. Notice that the equations of motion are with respect to the velocity instead of momentum, i.e.:\n\nbeginaligned\ndotx = v_x \ndoty = v_y \ndotv_x = B*v_y - U_x \ndotv_y = -B*v_x - U_X \nendaligned\n\nwith U the potential energy:\n\nU = left(tfrac1c^4right) lefttfracd_02 + c - r_aright^4\n\nif r_a = sqrt(x mod 1)^2 + (y mod 1)^2  fracd_02 + c and 0 otherwise. I.e. the potential is periodic with period 1 in both x y and normalized such that for energy value of 1 it is a circle of diameter d0. The magnetic field is also normalized such that for value B=1 the cyclotron diameter is 1.\n\n[1] : G. Datseris et al, New Journal of Physics 2019\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.arnoldcat","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.arnoldcat","text":"arnoldcat(u0 = rand(2))\n\nf(xy) = (2x+yx+y) mod 1\n\nArnold's cat map. A chaotic map from the torus into itself, discovered by Vladimir Arnold in the 1960s. [1]\n\n[1] : Arnol'd, V. I., & Avez, A. (1968). Ergodic problems of classical mechanics.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.chua","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.chua","text":"chua(u0 = [0.7, 0.0, 0.0]; a = 15.6, b = 25.58, m0 = -8/7, m1 = -5/7)\n\nbeginaligned\ndotx = alpha (y - h(x))\ndoty = x - y+z \ndotz = beta y\nendaligned\n\nwhere h(x) is defined by\n\nh(x) = m_1 x + frac 1 2 (m_0 - m_1)(x + 1 - x - 1)\n\nThis is a 3D continuous system that exhibits chaos.\n\nChua designed an electronic circuit with the expressed goal of exhibiting chaotic motion, and this system is obtained by rescaling the circuit units to simplify the form of the equation. [1]\n\nThe parameters are a, b, m0 and m1. Setting a = 15.6, m0 = -8/7 and m1 = -5/7, and varying the parameter b from b = 25 to b = 51, one observes a classic period-doubling bifurcation route to chaos. [2]\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : Chua, Leon O. \"The genesis of Chua's circuit\", 1992.\n\n[2] : Leon O. Chua (2007) \"Chua circuit\", Scholarpedia, 2(10):1488.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.coupledstandardmaps","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.coupledstandardmaps","text":"coupledstandardmaps(M::Int, u0 = 0.001rand(2M); ks = ones(M), Œì = 1.0)\n\nbeginaligned\ntheta_i = theta_i + p_i \np_i = p_i + k_isin(theta_i) - Gamma left\nsin(theta_i+1 - theta_i) + sin(theta_i-1 - theta_i)\nright\nendaligned\n\nA discrete system of M nonlinearly coupled standard maps, first introduced in [1] to study diffusion and chaos thresholds. The total dimension of the system is 2M. The maps are coupled through Œì and the i-th map has a nonlinear parameter ks[i]. The first M parameters are the ks, the M+1th parameter is Œì.\n\nThe first M entries of the state are the angles, the last M are the momenta.\n\n[1] : H. Kantz & P. Grassberger, J. Phys. A 21, pp 127‚Äì133 (1988)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.double_pendulum","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.double_pendulum","text":"double_pendulum(u0 = [œÄ/2, 0, 0, rand()];\n                G=10.0, L1 = 1.0, L2 = 1.0, M1 = 1.0, M2 = 1.0)\n\nFamous chaotic double pendulum system (also used for our logo!). Keywords are gravity (G), lengths of each rod and mass of each ball (all assumed SI units).\n\nThe variables order is [Œ∏1, dŒ∏1/dt, Œ∏2, dŒ∏2/dt].\n\nJacobian is created automatically (thus methods that use the Jacobian will be slower)!\n\n(please contribute the Jacobian and the e.o.m. in LaTeX :smile:)\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.duffing","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.duffing","text":"duffing(u0 = [rand(), rand(), 0]; œâ = 2.2, f = 27.0, d = 0.2, Œ≤ = 1)\n\nThe (forced) duffing oscillator, that satisfies the equation\n\nddotx + dcdotdotx + Œ≤*x + x^3 = fcos(omega t)\n\nwith f, œâ the forcing strength and frequency and d the dampening.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.fitzhugh_nagumo","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.fitzhugh_nagumo","text":"fitzhugh_nagumo(u = 0.5ones(2); a=3.0, b=0.2, Œµ=0.01, I=0.0)\n\nFamous excitable system which emulates the firing of a neuron, with equations\n\nbeginaligned\ndotv = av(v-b)(1-v) - w + I \nddotw = varepsilon(v - w)\nendaligned\n\nMore details in the Scholarpedia entry.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.gissinger","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.gissinger","text":"gissinger(u0 = 3rand(3); Œº = 0.119, ŒΩ = 0.1, Œì = 0.9)\n\nbeginaligned\ndotQ = mu Q - VD \ndotD = -nu D + VQ \ndotV = Gamma -V + QD\nendaligned\n\nA continuous system that models chaotic reversals due to Gissinger [1], applied to study the reversals of the magnetic field of the Earth.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : C. Gissinger, Eur. Phys. J. B 85, 4, pp 1-12 (2012)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.henon","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.henon","text":"henon(u0=zeros(2); a = 1.4, b = 0.3)\n\nbeginaligned\nx_n+1 = 1 - ax^2_n+y_n \ny_n+1  = bx_n\nendaligned\n\nThe H√©non map is a two-dimensional mapping due to H√©non [1] that can display a strange attractor (at the default parameters). In addition, it also displays many other aspects of chaos, like period doubling or intermittency, for other parameters.\n\nAccording to the author, it is a system displaying all the properties of the Lorentz system (1963) while being as simple as possible. Default values are the ones used in the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : M. H√©non, Commun.Math. Phys. 50, pp 69 (1976)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.henonheiles","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.henonheiles","text":"henonheiles(u0=[0, -0.25, 0.42081,0])\n\nbeginaligned\ndotx = p_x \ndoty = p_y \ndotp_x = -x -2 xy \ndotp_y = -y - (x^2 - y^2)\nendaligned\n\nThe H√©non‚ÄìHeiles system [1] is a conservative dynamical system and was introduced as a simplification of the motion of a star around a galactic center. It was originally intended to study the existence of a \"third integral of motion\" (which would make this 4D system integrable). In that search, the authors encountered chaos, as the third integral existed for only but a few initial conditions.\n\nThe default initial condition is a typical chaotic orbit. The function Systems.henonheiles_ics(E, n) generates a grid of n√ón initial conditions, all having the same energy E.\n\n[1] : H√©non, M. & Heiles, C., The Astronomical Journal 69, pp 73‚Äì79 (1964)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.labyrinth","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.labyrinth","text":"labyrinth(u0 = [1.0, 0, 0])\n\nbeginaligned\ndotx = sin(y) \ndoty = sin(z) \ndotz = sin(x)\nendaligned\n\nThree dimensional conservative continuous system, whose evolution in 3D space looks like a speudo-random walk, the orbit moving around like in a labyrinth.\n\nFirst proposed by Ren√© Thomas (1999). [1] See discussion in Section 4.4.3 of \"Elegant Chaos\" by J. C. Sprott. [2]\n\n[1] : Thomas, R. (1999). International Journal of Bifurcation and Chaos, 9(10), 1889-1905.\n\n[2] : Sprott, J. C. (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.logistic","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.logistic","text":"logistic(x0 = rand(); r = 4.0)\n\nx_n+1 = rx_n(1-x_n)\n\nThe logistic map is an one dimensional unimodal mapping due to May [1] and is used by many as the archetypal example of how chaos can arise from very simple equations.\n\nOriginally intentend to be a discretized model of polulation dynamics, it is now famous for its bifurcation diagram, an immensely complex graph that that was shown be universal by Feigenbaum [2].\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : R. M. May, Nature 261, pp 459 (1976)\n\n[2] : M. J. Feigenbaum, J. Stat. Phys. 19, pp 25 (1978)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.lorenz","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.lorenz","text":"lorenz(u0=[0.0, 10.0, 0.0]; œÉ = 10.0, œÅ = 28.0, Œ≤ = 8/3) -> ds\n\nbeginaligned\ndotX = sigma(Y-X) \ndotY = -XZ + rho X -Y \ndotZ = XY - beta Z\nendaligned\n\nThe famous three dimensional system due to Lorenz [1], shown to exhibit so-called \"deterministic nonperiodic flow\". It was originally invented to study a simplified form of atmospheric convection.\n\nCurrently, it is most famous for its strange attractor (occuring at the default parameters), which resembles a butterfly. For the same reason it is also associated with the term \"butterfly effect\" (a term which Lorenz himself disliked) even though the effect applies generally to dynamical systems. Default values are the ones used in the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : E. N. Lorenz, J. atmos. Sci. 20, pp 130 (1963)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.lorenz96","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.lorenz96","text":"lorenz96(N::Int, u0 = rand(M); F=0.01)\n\nfracdx_idt = (x_i+1-x_i-2)x_i-1 - x_i + F\n\nN is the chain length, F the forcing. Jacobian is created automatically. (parameter container only contains F)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.magnetic_pendulum","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.magnetic_pendulum","text":"magnetic_pendulum(u=[cos(Œ∏),sin(Œ∏),0,0]; Œ≥=1, d=0.3, Œ±=0.2, œâ=0.5, N=3)\n\nCreate a pangetic pendulum with N magnetics, equally distributed along the unit circle, with equations of motion\n\nbeginaligned\nddotx = -omega ^2x - alpha dotx - sum_i=1^N fracgamma (x - x_i)D_i^3 \nddoty = -omega ^2y - alpha doty - sum_i=1^N fracgamma (y - y_i)D_i^3 \nD_i = sqrt(x-x_i)^2  + (y-y_i)^2 + d^2\nendaligned\n\nwhere Œ± is friction, œâ is eigenfrequency, d is distance of pendulum from the magnet's plane and Œ≥ is the magnetic strength. A random initial condition is initialized by default somewhere along the unit circle with zero velocity.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.more_chaos_example","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.more_chaos_example","text":"more_chaos_example(u = rand(3))\n\nA three dimensional chaotic system introduced in [Sprott2020] with rule\n\nbeginaligned\ndotx = y \ndoty = -x - sign(z)y \ndotz = y^2 - exp(-x^2)\nendaligned\n\nIt is noteworthy because its strange attractor is multifractal with fractal dimension ‚âà 3.\n\n[Sprott2020]: Sprott, J.C. 'Do We Need More Chaos Examples?', Chaos Theory and Applications 2(2),1-3, 2020\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.nosehoover","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.nosehoover","text":"nosehoover(u0 = [0, 0.1, 0])\n\nbeginaligned\ndotx = y \ndoty = yz - x \ndotz = 1 - y^2\nendaligned\n\nThree dimensional conservative continuous system, discovered in 1984 during investigations in thermodynamical chemistry by Nos√© and Hoover, then rediscovered by Sprott during an exhaustive search as an extremely simple chaotic system. [1]\n\nSee Chapter 4 of \"Elegant Chaos\" by J. C. Sprott. [2]\n\n[1] : Hoover, W. G. (1995). Remark on ‚Äò‚ÄòSome simple chaotic flows‚Äô‚Äô. Physical Review E, 51(1), 759.\n\n[2] : Sprott, J. C. (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.pomeau_manneville","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.pomeau_manneville","text":"pomaeu_manneville(u0 = 0.2; z = 2.5)\n\nThe Pomeau-Manneville map is a one dimensional discrete map which is characteristic for displaying intermittency [1]. Specifically, for z > 2 the average time between chaotic bursts diverges, while for z > 2.5, the map iterates are long range correlated [2].\n\nNotice that here we are providing the \"symmetric\" version:\n\nx_n+1 = begincases\n-4x_n + 3  quad x_n in (05 1 \nx_n(1 + 2x_n^z-1)  quad x_n le 05 \n-4x_n - 3  quad x_n in -1 05)\nendcases\n\n[1] : Manneville & Pomeau, Comm. Math. Phys. 74 (1980)\n\n[2] : Meyer et al., New. J. Phys 20 (2019)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.qbh","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.qbh","text":"qbh([u0]; A=1.0, B=0.55, D=0.4)\n\nA conservative dynamical system with rule\n\nbeginaligned\ndotq_0 = A p_0 \ndotq_2 = A p_2 \ndotp_0 = -A q_0 -3 fracBsqrt2 (q_2^2 - q_1^2) - D q_1 (q_1^2 + q_2^2) \ndotp_2 = -q_2 (A + 3sqrt2 B q_1 + D (q_1^2 + q_2^2)) (x^2 - y^2)\nendaligned\n\nThese equations of motion correspond to a Hamiltonian used in nuclear physics to study the quadrupole vibrations of the nuclear surface [1,2].\n\nH(p_0 p_2 q_0 q_2) = fracA2left(p_0^2+p_2^2right)+fracA2left(q_0^2+q_2^2right)\n\t\t\t +fracBsqrt2q_0left(3q_2^2-q_0^2right) +fracD4left(q_0^2+q_2^2right)^2\n\nThe Hamiltonian has a similar structure with the Henon-Heiles one, but it has an added fourth order term and presents a nontrivial dependence of chaoticity with the increase of energy [3]. The default initial condition is chaotic.\n\n[1]: Eisenberg, J.M., & Greiner, W., Nuclear theory 2 rev ed. Netherlands: North-Holland pp 80 (1975)\n\n[2]: Baran V. and Raduta A. A., International Journal of Modern Physics E, 7, pp 527‚Äì551 (1998)\n\n[3]: Micluta-Campeanu S., Raportaru M.C., Nicolin A.I., Baran V., Rom. Rep. Phys. 70, pp 105 (2018)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.rikitake","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.rikitake","text":"rikitake(u0 = [1, 0, 0.6]; Œº = 1.0, Œ± = 1.0)\n\nbeginaligned\ndotx = -mu x +yz \ndoty = -mu y +x(z-alpha) \ndotz = 1 - xz\nendaligned\n\nRikitake's dynamo is a system that tries to model the magnetic reversal events by means of a double-disk dynamo system.\n\n[1] : T. Rikitake Math. Proc. Camb. Phil. Soc. 54, pp 89‚Äì105, (1958)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.roessler","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.roessler","text":"roessler(u0=rand(3); a = 0.2, b = 0.2, c = 5.7)\n\nbeginaligned\ndotx = -y-z \ndoty = x+ay \ndotz = b + z(x-c)\nendaligned\n\nThis three-dimensional continuous system is due to R√∂ssler [1]. It is a system that by design behaves similarly to the lorenz system and displays a (fractal) strange attractor. However, it is easier to analyze qualitatively, as for example the attractor is composed of a single manifold. Default values are the same as the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : O. E. R√∂ssler, Phys. Lett. 57A, pp 397 (1976)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.shinriki","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.shinriki","text":"shinriki(u0 = [-2, 0, 0.2]; R1 = 22.0)\n\nShinriki oscillator with all other parameters (besides R1) set to constants. This is a stiff problem, be careful when choosing solvers and tolerances.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.standardmap","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.standardmap","text":"standardmap(u0=0.001rand(2); k = 0.971635)\n\nbeginaligned\ntheta_n+1 = theta_n + p_n+1 \np_n+1 = p_n + ksin(theta_n)\nendaligned\n\nThe standard map (also known as Chirikov standard map) is a two dimensional, area-preserving chaotic mapping due to Chirikov [1]. It is one of the most studied chaotic systems and by far the most studied Hamiltonian (area-preserving) mapping.\n\nThe map corresponds to the  Poincar√©'s surface of section of the kicked rotor system. Changing the non-linearity parameter k transitions the system from completely periodic motion, to quasi-periodic, to local chaos (mixed phase-space) and finally to global chaos.\n\nThe default parameter k is the critical parameter where the golden-ratio torus is destroyed, as was calculated by Greene [2]. The e.o.m. considers the angle variable Œ∏ to be the first, and the angular momentum p to be the second, while both variables are always taken modulo 2œÄ (the mapping is on the [0,2œÄ)¬≤ torus).\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : B. V. Chirikov, Preprint N. 267, Institute of Nuclear Physics, Novosibirsk (1969)\n\n[2] : J. M. Greene, J. Math. Phys. 20, pp 1183 (1979)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.towel","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.towel","text":"towel(u0 = [0.085, -0.121, 0.075])\n\nbeginaligned\nx_n+1 = a x_n (1-x_n) -005 (y_n +035) (1-2z_n) \ny_n+1 = 01 left( left( y_n +035 right)left( 1+2z_nright) -1 right)\nleft( 1 -19 x_n right) \nz_n+1 = 378 z_n (1-z_n) + b y_n\nendaligned\n\nThe folded-towel map is a hyperchaotic mapping due to R√∂ssler [1]. It is famous for being a mapping that has the smallest possible dimensions necessary for hyperchaos, having two positive and one negative Lyapunov exponent. The name comes from the fact that when plotted looks like a folded towel, in every projection.\n\nDefault values are the ones used in the original paper.\n\n[1] : O. E. R√∂ssler, Phys. Lett. 71A, pp 155 (1979)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.ueda","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.ueda","text":"ueda(u0 = [3.0, 0]; k = 0.1, B = 12.0)\n\nddotx + k dotx + x^3 = Bcost\n\nNonautonomous Duffing-like forced oscillation system, discovered by Ueda in\n\nIt is one of the first chaotic systems to be discovered.\n\nThe stroboscopic plot in the (x, Ãáx) plane with period 2œÄ creates a \"broken-egg attractor\" for k = 0.1 and B = 12. Figure 5 of [1] is reproduced by\n\nusing Plots\nds = Systems.ueda()\na = trajectory(ds, 2œÄ*5e3, dt = 2œÄ)\nscatter(a[:, 1], a[:, 2], markersize = 0.5, title=\"Ueda attractor\")\n\nFor more forced oscillation systems, see Chapter 2 of \"Elegant Chaos\" by J. C. Sprott. [2]\n\n[1] : Ruelle, David, ‚ÄòStrange Attractors‚Äô, The Mathematical Intelligencer, 2.3 (1980), 126‚Äì37\n\n[2] : Sprott, J. C. (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#Advanced-documentation-1","page":"Advanced Documentation","title":"Advanced documentation","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"This section overviews the various integrators available from DynamicalSystemsBase, as well as gives some insight into the internals, so that other developers that want to use this library can build upon it.","category":"page"},{"location":"advanced/#Integrators-1","page":"Advanced Documentation","title":"Integrators","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"integrator\nparallel_integrator\ntangent_integrator","category":"page"},{"location":"advanced/#DynamicalSystemsBase.integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.integrator","text":"integrator(ds::DynamicalSystem [, u0]; diffeq...) -> integ\n\nReturn an integrator object that can be used to evolve a system interactively using step!(integ [, Œît]). Optionally specify an initial state u0.\n\nThe state of this integrator is a vector.\n\ndiffeq... are keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.parallel_integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.parallel_integrator","text":"parallel_integrator(ds::DynamicalSystem, states; kwargs...)\n\nReturn an integrator object that can be used to evolve many states of a system in parallel at the exact same times, using step!(integ [, Œît]).\n\nstates are expected as vectors of vectors.\n\nKeyword Arguments\n\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems. These keywords can also include callback for event handling.\n\nIt is heavily advised to use the functions get_state and set_state! to manipulate the integrator. Provide i as a second argument to change the i-th state.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.tangent_integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.tangent_integrator","text":"tangent_integrator(ds::DynamicalSystem, Q0 | k::Int; kwargs...)\n\nReturn an integrator object that evolves in parallel both the system as well as deviation vectors living on the tangent space, also called linearized space.\n\nQ0 is a matrix whose columns are initial values for deviation vectors. If instead of a matrix Q0 an integer k is given, then k random orthonormal vectors are choosen as initial conditions.\n\nKeyword Arguments\n\nu0 : Optional different initial state.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems. These keywords can also include callback for event handling.\n\nIt is heavily advised to use the functions get_state, get_deviations, set_state!, set_deviations! to manipulate the integrator.\n\nDescription\n\nIf J is the jacobian of the system then the tangent dynamics are the equations that evolve in parallel the system as well as a deviation vector (or matrix) w:\n\nbeginaligned\ndotu = f(u p t) \ndotw = J(u p t) times w\nendaligned\n\nwith f being the equations of motion and u the system state. Similar equations hold for the discrete case.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"Notice that the state type integrator.u of each integrator is quite different and does change between the possible versions of a DynamicalSystem!","category":"page"},{"location":"advanced/#Integrator-state-functions-1","page":"Advanced Documentation","title":"Integrator state functions","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"There are four functions associated with the integrators that we export:","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"get_state\nset_state!\nget_deviations\nset_deviations!","category":"page"},{"location":"advanced/#DynamicalSystemsBase.get_state","page":"Advanced Documentation","title":"DynamicalSystemsBase.get_state","text":"get_state(ds::DynamicalSystem)\n\nReturn the state of ds.\n\nget_state(integ [, i::Int = 1])\n\nReturn the state of the integrator, in the sense of the state of the dynamical system.\n\nIf the integrator is a parallel_integrator, passing i will return the i-th state. The function also correctly returns the true state of the system for tangent integrators.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.set_state!","page":"Advanced Documentation","title":"DynamicalSystemsBase.set_state!","text":"set_state!(integ, u [, i::Int = 1])\n\nSet the state of the integrator to u, in the sense of the state of the dynamical system. Works for any integrator (normal, tangent, parallel).\n\nFor parallel integrator, you can choose which state to set (using i).\n\nAutomatically does u_modified!(integ, true).\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.get_deviations","page":"Advanced Documentation","title":"DynamicalSystemsBase.get_deviations","text":"get_deviations(tang_integ)\n\nReturn the deviation vectors of the tangent_integrator in a form of a matrix with columns the vectors.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.set_deviations!","page":"Advanced Documentation","title":"DynamicalSystemsBase.set_deviations!","text":"set_deviations!(tang_integ, Q)\n\nSet the deviation vectors of the tangent_integrator to Q, which must be a matrix with each column being a deviation vector.\n\nAutomatically does u_modified!(tang_integ, true).\n\n\n\n\n\n","category":"function"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"note: Note\nThese functions work with any possible integrator and it is best to use the to change states robustly!","category":"page"},{"location":"advanced/#Re-initializing-an-integrator-1","page":"Advanced Documentation","title":"Re-initializing an integrator","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"It is more efficient to re-initialize an integrator using reinit! than to create a new one. This can be very helpful when looping over initial conditions and/or parameter values.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"All high-level functions from ChaosTools have a set-up part that creates an integrator, and a low-level part that does the computation. The low level part is your friend! Use it! See the Using GALI page for an example as well as the section below.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"The reinit! call signature is the same for continuous and discrete systems. In the following, state is supposed to be a D dimensional vector (state of the dynamical system).","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"reinit!(integ, state) : to be used with standard integrator.\nreinit!(integ, Vector_of_states) : to be used with the parallel_integrator.\nreinit!(integ, state, Q0::AbstractMatrix) : to be used with the tangent_integrator. This three argument version of reinit! is exported from DynamicalSystemsBase.","category":"page"},{"location":"advanced/#Re-init-of-continuous-tangent-integrator-1","page":"Advanced Documentation","title":"Re-init of continuous tangent integrator","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"Here we compute the lyapunovs for many different initial conditions.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"ds = Systems.lorenz()\ntinteg = tangent_integrator(ds, 2)\nics = [rand(3) for i in 1:100]\nfor ic in ics\n  reinit!(tinteg, ic, orthonormal(3, 2))\n  Œª = lyapunovs(tinteg, 1000, 0.1, 10.0)\n  # reminder: lyapunovs(tinteg, N, dt::Real, Ttr::Real = 0.0)\nend","category":"page"},{"location":"advanced/#Re-init-of-discrete-parallel-integrator-1","page":"Advanced Documentation","title":"Re-init of discrete parallel integrator","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"Here we compute the lyapunov for many different parameters.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"ds = Systems.henon()\nu0 = rand(SVector{2})\nps = 1.2:0.01:1.4\npinteg = parallel_integrator(ds, [u0, u0 + 1e-9rand(SVector{2})])\nfor p in ps\n  set_parameter!(ds, 1, p)\n  reinit!(pinteg, [u0, u0 + 1e-9rand(SVector{2})])\n  Œª = lyapunov(pinteg, 1000, 10, 1, 1e-9, 1e-6, 1e-12)\n  # reminder: lyapunov(pinteg, T, Ttr, dt, d0, ut, lt)\nend","category":"page"},{"location":"advanced/#Using-callbacks-with-integrators-1","page":"Advanced Documentation","title":"Using callbacks with integrators","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"For the case of continuous systems you can add callbacks from the event handling of DifferentialEquations.jl. This is done simply as a keyword argument to the initializers.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"In this example we use a simple SavingCallback to save the distance between the two states of a parallel_integrator.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"using DynamicalSystems, DiffEqCallbacks\nusing LinearAlgebra: norm\n\nkwargs = (abstol=1e-14, reltol=1e-14, maxiters=1e9)\nds = Systems.lorenz()\nd0 = 1e-9\nT = 100.0\n\nsave_func(u, t, integrator) = norm(u[1] - u[2])\nsaved_values = SavedValues(eltype(ds.t0), eltype(get_state(ds)))\ncb = SavingCallback(save_func, saved_values)\n\nu0 = get_state(ds)\npinteg = parallel_integrator(ds, [u0, u0 + rand(SVector{3})*d0*‚àö3];\nkwargs..., callback = cb)\nstep!(pinteg, T)\nt = saved_values.t\nn = saved_values.saveval","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"As expected you can see that the recorded distance between two states is increasing.","category":"page"},{"location":"advanced/#DynamicalSystem-implementation-1","page":"Advanced Documentation","title":"DynamicalSystem implementation","text":"","category":"section"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"abstract type DynamicalSystem{\n        IIP,     # is in place , for dispatch purposes and clarity\n        S,       # state type\n        D,       # dimension\n        F,       # equations of motion\n        P,       # parameters\n        JAC,     # jacobian\n        JM,      # jacobian matrix\n        IAD}     # is auto-differentiated\n    # one-liner: {IIP, S, D, F, P, JAC, JM, IAD}\n    # Subtypes of DynamicalSystem have fields:\n    # 1. f\n    # 2. u0\n    # 3. p\n    # 4. t0\n    # 5. jacobian (function)\n    # 6. J (matrix)\nend","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"The DynamicalSystem stores only the absolutely necessary information. Every other functionality of DynamicalSystems.jl initializes an integrator.","category":"page"},{"location":"advanced/#","page":"Advanced Documentation","title":"Advanced Documentation","text":"The final type-parameter IAD is useful when creating the tangent_integrator, so that the vector field is not computed twice!","category":"page"},{"location":"ds/general/#Dynamical-System-Definition-1","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In DynamicalSystems.jl a Dynamical System can be either in continuous time","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"fracdvecudt = vecf(vecu p t)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"or discrete time","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"vecu_n+1 = vecf(vecu_n p n)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"where u is the state of the system and p contains the parameters of the system. The function f is called the dynamic rule of the system, also known as equations of motion.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In addition f, information about the Jacobian of the system J_f is also used throughout the library.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"warning: Warning\nKeep in mind that almost all functions of DynamicalSystems.jl that use a DynamicalSystem assume that f is differentiable!","category":"page"},{"location":"ds/general/#Creating-a-Dynamical-System-1","page":"Dynamical System Definition","title":"Creating a Dynamical System","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DynamicalSystem","category":"page"},{"location":"ds/general/#DynamicalSystemsBase.DynamicalSystem","page":"Dynamical System Definition","title":"DynamicalSystemsBase.DynamicalSystem","text":"DynamicalSystem\n\nThe central structure of DynamicalSystems.jl. All functions of the suite that can use known equations of motion expect an instance of this type.\n\nConstructing a DynamicalSystem\n\nDiscreteDynamicalSystem(eom, state, p [, jacobian [, J0]]; t0::Int = 0)\nContinuousDynamicalSystem(eom, state, p [, jacobian [, J0]]; t0 = 0.0)\n\nwith eom the equations of motion function (see below). p is a parameter container, which we highly suggest to use a mutable object like Array, LMArray or a dictionary. Pass nothing in the place of p if your system does not have parameters.\n\nt0, J0 allow you to choose the initial time and provide an initialized Jacobian matrix. See CDS_KWARGS for the default options used to evolve continuous systems (through OrdinaryDiffEq).\n\nEquations of motion\n\nThe are two \"versions\" for DynamicalSystem, depending on whether the equations of motion (eom) are in-place (iip) or out-of-place (oop). Here is how to define them (1-D systems are treated differently, see below):\n\noop : The eom must be in the form eom(x, p, t) -> SVector which means that given a state x::SVector and some parameter container p it returns an SVector (from the StaticArrays module) containing the next state.\niip : The eom must be in the form eom!(xnew, x, p, t) which means that given a state x::Vector and some parameter container p, it writes in-place the new state in xnew.\n\nt stands for time (integer for discrete systems). iip is suggested for big systems, whereas oop is suggested for small systems. The break-even point at around 100 dimensions, and for using functions that use the tangent space (like e.g. lyapunovs or gali), the break-even point is at around 10 dimensions.\n\nThe constructor deduces automatically whether eom is iip or oop. It is not possible however to deduce whether the system is continuous or discrete just from the equations of motion, hence the 2 constructors.\n\nJacobian\n\nThe optional argument jacobian for the constructors is a function and (if given) must also be of the same form as the eom, jacobian(x, p, n) -> SMatrix for the out-of-place version and jacobian!(Jnew, x, p, n) for the in-place version.\n\nIf jacobian is not given, it is constructed automatically using the module ForwardDiff. Even though ForwardDiff is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so we recommend it.\n\nComment on 1-D\n\nOne dimensional discrete systems expect the state always as a pure number, 0.8 instead of SVector(0.8). For continuous systems, the state can be in-place/out-of-place as in higher dimensions, however the derivative function must be always explicitly given.\n\nInterface to DifferentialEquations.jl\n\nContinuous systems are solved using DifferentialEquations.jl. The following two interfaces are provided:\n\nContinuousDynamicalSystem(prob::ODEProblem [, jacobian [, J0]])\nODEProblem(continuous_dynamical_system, tspan, args...)\n\nwhere in the second case args stands for the standard extra arguments of ODEProblem: callback, mass_matrix.\n\nIf you want to use callbacks with tangent_integrator or parallel_integrator, then invoke them with extra arguments as shown in the Advanced Documentation.\n\nRelevant Functions\n\ntrajectory, set_parameter!.\n\n\n\n\n\n","category":"type"},{"location":"ds/general/#Definition-Table-1","page":"Dynamical System Definition","title":"Definition Table","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Here is a handy table that summarizes in what form should be the functions required for the equations of motion and the Jacobian, for each system type:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"System Type equations of motion Jacobian\nin-place (big systems) eom!(du, u, p, t) jacobian!(J, u, p, t)\nout-of-place (small systems) eom(u, p, t) -> SVector jacobian(u, p, t) -> SMatrix","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tip: Use mutable containers for the parameters\nIt is highly suggested to use a subtype of Array,  LMArray or a dictionary for the container of the model's parameters. Some functions offered by DynamicalSystems.jl, like e.g. orbitdiagram, assume that the parameters can be first accessed by p[x] with x some qualifier as well as that this value can be set by p[x] = newvalue.The Labelled Arrays package offers Array implementations that can be accessed both by index as well as by some name.","category":"page"},{"location":"ds/general/#Convenience-functions-1","page":"Dynamical System Definition","title":"Convenience functions","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The following functions are defined for convenience for any dynamical system:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"dimension\njacobian\nset_parameter!","category":"page"},{"location":"ds/general/#DelayEmbeddings.dimension","page":"Dynamical System Definition","title":"DelayEmbeddings.dimension","text":"dimension(thing) -> D\n\nReturn the dimension of the thing, in the sense of state-space dimensionality.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#DynamicalSystemsBase.jacobian","page":"Dynamical System Definition","title":"DynamicalSystemsBase.jacobian","text":"jacobian(ds::DynamicalSystem, u = ds.u0, t = ds.t0)\n\nReturn the jacobian of the system at u, at t.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#DynamicalSystemsBase.set_parameter!","page":"Dynamical System Definition","title":"DynamicalSystemsBase.set_parameter!","text":"set_parameter!(ds::DynamicalSystem, index, value)\nset_parameter!(ds::DynamicalSystem, values)\n\nChange one or many parameters of the system by setting p[index] = value in the first case and p .= values in the second.\n\nThe same function also works for any integrator.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#Example:-continuous,-out-of-place-1","page":"Dynamical System Definition","title":"Example: continuous, out-of-place","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Let's see an example for a small system, which is a case where out-of-place equations of motion are preferred.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"using DynamicalSystems # also exports relevant StaticArrays names\n# Lorenz system\n# Equations of motion:\n@inline @inbounds function loop(u, p, t)\n    œÉ = p[1]; œÅ = p[2]; Œ≤ = p[3]\n    du1 = œÉ*(u[2]-u[1])\n    du2 = u[1]*(œÅ-u[3]) - u[2]\n    du3 = u[1]*u[2] - Œ≤*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n# Jacobian:\n@inline @inbounds function loop_jac(u, p, t)\n    œÉ, œÅ, Œ≤ = p\n    J = @SMatrix [-œÉ  œÉ  0;\n    œÅ - u[3]  (-1)  (-u[1]);\n    u[2]   u[1]  -Œ≤]\n    return J\nend\n\nds = ContinuousDynamicalSystem(loop, rand(3), [10.0, 28.0, 8/3], loop_jac)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"3-dimensional continuous dynamical system\n state:     [0.068248, 0.828095, 0.0743729]\n e.o.m.:    loop\n in-place?  false\n jacobian:  loop_jac","category":"page"},{"location":"ds/general/#Example:-discrete,-in-place-1","page":"Dynamical System Definition","title":"Example: discrete, in-place","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The following example is only 2-dimensional, and thus once again it is \"correct\" to use out-of-place version with SVector. For the sake of example though, we use the in-place version.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"# Henon map.\n# equations of motion:\nfunction hiip(dx, x, p, n)\n    dx[1] = 1.0 - p[1]*x[1]^2 + x[2]\n    dx[2] = p[2]*x[1]\n    return\nend\n# Jacobian:\nfunction hiip_jac(J, x, p, n)\n    J[1,1] = -2*p[1]*x[1]\n    J[1,2] = 1.0\n    J[2,1] = p[2]\n    J[2,2] = 0.0\n    return\nend\nds = DiscreteDynamicalSystem(hiip, zeros(2), [1.4, 0.3], hiip_jac)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"2-dimensional discrete dynamical system\n state:     [0.0, 0.0]\n e.o.m.:    hiip\n in-place?  true\n jacobian:  hiip_jac","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Or, if you don't want to write a Jacobian and want to use the auto-differentiation capabilities of DynamicalSystems.jl, which use the module ForwardDiff:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = DiscreteDynamicalSystem(hiip, zeros(2), [1.4, 0.3])","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"2-dimensional discrete dynamical system\n state:     [0.0, 0.0]\n e.o.m.:    hiip\n in-place?  true\n jacobian:  ForwardDiff","category":"page"},{"location":"ds/general/#Complex-Example-1","page":"Dynamical System Definition","title":"Complex Example","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In this example we will go through the implementation of the coupled standard maps from our Predefined Dynamical Systems. It is the most complex implementation and takes full advantage of the flexibility of the constructors. The example will use a function-like-object as equations of motion, as well as a sparse matrix for the Jacobian.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Coupled standard maps is a big mapping that can have arbitrary number of equations of motion, since you can couple N standard maps which are 2D maps, like:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"theta_i = theta_i + p_i \np_i = p_i + k_isin(theta_i) - Gamma leftsin(theta_i+1 - theta_i) + sin(theta_i-1 - theta_i) right","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"To model this, we will make a dedicated struct, which is parameterized on the number of coupled maps:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"struct CoupledStandardMaps{N}\n    idxs::SVector{N, Int}\n    idxsm1::SVector{N, Int}\n    idxsp1::SVector{N, Int}\nend","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"(what these fields are will become apparent later)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We initialize the struct with the amount of standard maps we want to couple, and we also define appropriate parameters:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"M = 5  # couple number\nu0 = 0.001rand(2M) #initial state\nks = 0.9ones(M) # nonlinearity parameters\nŒì = 1.0 # coupling strength\np = (ks, Œì) # parameter container\n\n# Create struct:\nSV = SVector{M, Int}\nidxs = SV(1:M...) # indexes of thetas\nidxsm1 = SV(circshift(idxs, +1)...)  #indexes of thetas - 1\nidxsp1 = SV(circshift(idxs, -1)...)  #indexes of thetas + 1\n# So that:\n# x[i] ‚â° Œ∏·µ¢\n# x[[idxsp1[i]]] ‚â° Œ∏·µ¢+‚ÇÅ\n# x[[idxsm1[i]]] ‚â° Œ∏·µ¢-‚ÇÅ\ncsm = CoupledStandardMaps{M}(idxs, idxsm1, idxsp1);","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We will now use this struct to define a function-like-object, a Type that also acts as a function.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function (f::CoupledStandardMaps{N})(xnew::AbstractVector, x, p, n) where {N}\n    ks, Œì = p\n    @inbounds for i in f.idxs\n\n        xnew[i+N] = mod2pi(\n            x[i+N] + ks[i]*sin(x[i]) -\n            Œì*(sin(x[f.idxsp1[i]] - x[i]) + sin(x[f.idxsm1[i]] - x[i]))\n        )\n\n        xnew[i] = mod2pi(x[i] + xnew[i+N])\n    end\n    return nothing\nend","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We will use the same struct to create a function for the Jacobian:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function (f::CoupledStandardMaps{M})(\n    J::AbstractMatrix, x, p, n) where {M}\n\n    ks, Œì = p\n    # x[i] ‚â° Œ∏·µ¢\n    # x[[idxsp1[i]]] ‚â° Œ∏·µ¢+‚ÇÅ\n    # x[[idxsm1[i]]] ‚â° Œ∏·µ¢-‚ÇÅ\n    @inbounds for i in f.idxs\n        cosŒ∏ = cos(x[i])\n        cosŒ∏p= cos(x[f.idxsp1[i]] - x[i])\n        cosŒ∏m= cos(x[f.idxsm1[i]] - x[i])\n        J[i+M, i] = ks[i]*cosŒ∏ + Œì*(cosŒ∏p + cosŒ∏m)\n        J[i+M, f.idxsm1[i]] = - Œì*cosŒ∏m\n        J[i+M, f.idxsp1[i]] = - Œì*cosŒ∏p\n        J[i, i] = 1 + J[i+M, i]\n        J[i, f.idxsm1[i]] = J[i+M, f.idxsm1[i]]\n        J[i, f.idxsp1[i]] = J[i+M, f.idxsp1[i]]\n    end\n    return nothing\nend","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The only reason that this is possible, is because the eom always takes a AbstractVector as first argument, while the Jacobian always takes an AbstractMatrix. Therefore we can take advantage of multiple dispatch!","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice in addition, that the Jacobian function accesses only half the elements of the matrix. This is intentional, and takes advantage of the fact that the other half is constant. We can leverage this further, by making the Jacobian a sparse matrix. Because the DynamicalSystem constructors allow us to give in a pre-initialized Jacobian matrix, we take advantage of that and create:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"J = zeros(eltype(u0), 2M, 2M)\n# Set ‚àÇ/‚àÇp entries (they are eye(M,M))\n# And they dont change they are constants\nfor i in idxs\n    J[i, i+M] = 1\n    J[i+M, i+M] = 1\nend\nsparseJ = sparse(J)\n\ncsm(sparseJ, u0, p, 0) # apply Jacobian to initial state","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"And finally, we are ready to create our dynamical system:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = DiscreteDynamicalSystem(csm, u0, p, csm, sparseJ)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"10-dimensional discrete dynamical system\n state:       [0.000803001, 0.00092095, 0.000313022, ‚Ä¶, 3.07769e-5, 0.000670152]\n e.o.m.:      CoupledStandardMaps\n in-place?    true\n jacobian:    CoupledStandardMaps\n parameters:  Tuple","category":"page"},{"location":"ds/general/#Automatic-Jacobians-1","page":"Dynamical System Definition","title":"Automatic Jacobians","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice that if you are using automatic differentiation for the Jacobian, you should take care to NOT define your equations of motion so that they explicitly use, or return, Float64 numbers. This is because ForwardDiff uses DualNumbers for differentiation. For example, if you did","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function lorenz(u,p,t)\n    œÉ, œÅ, Œ≤ = p\n    dx = zeros(3)\n    du1 = œÉ*(u[2] - u[1]) +\n    du2 = u[1]*(œÅ - u[3]) - u[2]\n    du3 = u[1]*u[2] - Œ≤*u[3]\n    return SVector{Float64, 3}(du1, du2, du3)\nend","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"this function could not be used to auto-differentiate, as you would get an error when adding dual numbers to SVector{Float64}. Instead, leave the number type untyped, or use eltype(u) as the number type.","category":"page"},{"location":"ds/general/#Time-Evolution-of-Systems-1","page":"Dynamical System Definition","title":"Time Evolution of Systems","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DynamicalSystems.jl provides a convenient function for getting a trajectory of a system at equally spaced time points:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"trajectory","category":"page"},{"location":"ds/general/#DynamicalSystemsBase.trajectory","page":"Dynamical System Definition","title":"DynamicalSystemsBase.trajectory","text":"trajectory(ds::DynamicalSystem, T [, u]; kwargs...) -> dataset\n\nReturn a dataset that will contain the trajectory of the system, after evolving it for total time T, optionally starting from state u. See Dataset for info on how to use this object.\n\nA W√óD dataset is returned, with W = length(t0:dt:T) with t0:dt:T representing the time vector (not returned) and D the system dimension. For discrete systems both T and dt must be integers.\n\nKeyword Arguments\n\ndt :  Time step of value output during the solving of the continuous system. For discrete systems it must be an integer. Defaults to 0.01 for continuous and 1 for discrete.\nTtr : Transient time to evolve the initial state before starting saving states.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. For example abstol = 1e-9.  Only valid for continuous systems. If you want to specify a solver, do so by using the name alg, e.g.: alg = Tsit5(), maxiters = 1000. This requires you to have been first using OrdinaryDiffEq to access the solvers. See DynamicalSystemsBase.CDS_KWARGS for default values. These keywords can also include callback for event handling. Using a SavingCallback with trajectory will lead to unexpected behavior!\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice that if you want to do repeated evolutions of different states of a continuous system, you should use the integrator interface instead.","category":"page"},{"location":"ds/general/#Example-1","page":"Dynamical System Definition","title":"Example","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"using DynamicalSystems\nds = Systems.towel()","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 100)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"To get every 3-rd point of the trajectory, do","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 100; dt = 3)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Identical syntax is used for continuous systems","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = Systems.lorenz()","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 10.0; dt = 0.01)","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"And a final example controlling the integrator accuracy:","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = Systems.lorenz()\ntr = trajectory(ds, 10.0; dt = 0.1, abstol = 1e-9, reltol = 1e-9)","category":"page"},{"location":"ds/general/#Solution-precision-for-continuous-systems-1","page":"Dynamical System Definition","title":"Solution precision for continuous systems","text":"","category":"section"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"A numerical solution of an ODE is not the \"true\" solution, uniquely defined by a (well-defined) ODE and an initial condition. Especially for chaotic systems, where deviations are amplified exponentially, one is left worried if the numerical solutions truly are part of the system and can truly give insight in understanding the system.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DifferentialEquations.jl offers a tool, called Uncertainty Quantification, which allows users to asses up to what time-scales the numerical solution is close to the \"true\" solution. For example, using the default solving parameters of DynamicalSystems.jl, the Lorenz system is accurate up to time t = 50.0.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"However, fortunately for us, there is not too much worry about the numerical solution diverging from the true solution. That is because of the shadowing theorem (or shadowing lemma):","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"quote: Shadowing Theorem\nAlthough a numerically computed chaotic trajectory diverges exponentially from the true trajectory with the same initial coordinates, there exists an errorless trajectory with a slightly different initial condition that stays near (\"shadows\") the numerically computed one.","category":"page"},{"location":"ds/general/#","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"This simply means that one can always numerically study chaos not only qualitatively but also quantitatively. For more information, see the book Chaos in Dynamical Systems by E. Ott, or the scholarpedia entry.","category":"page"},{"location":"embedding/reconstruction/#Delay-Coordinates-Embedding-1","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"","category":"section"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"A timeseries recorded in some manner from a dynamical system can be used to gain information about the dynamics of the entire state space of the system. This can be done by constructing a new state space from the timeseries. One method that can do this is what is known as delay coordinates embedding or delay coordinates reconstruction.","category":"page"},{"location":"embedding/reconstruction/#Timeseries-embedding-1","page":"Delay Coordinates Embedding","title":"Timeseries embedding","text":"","category":"section"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"Delay embeddings are done through embed:","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"embed","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.embed","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.embed","text":"embed(s, d, œÑ [, h])\n\nEmbed s using delay coordinates with embedding dimension d and delay time œÑ and return the result as a Dataset. Optionally use weight h, see below.\n\nHere œÑ > 0, use genembed for a generalized version.\n\nDescription\n\nIf œÑ is an integer, then the n-th entry of the embedded space is\n\n(s(n) s(n+tau) s(n+2tau) dots s(n+(d-1)tau))\n\nIf instead œÑ is a vector of integers, so that length(œÑ) == d-1, then the n-th entry is\n\n(s(n) s(n+tau1) s(n+tau2) dots s(n+taud-1))\n\nThe resulting set can have same invariant quantities (like e.g. lyapunov exponents) with the original system that the timeseries were recorded from, for proper d and œÑ. This is known as the Takens embedding theorem [Takens1981] [Sauer1991]. The case of different delay times allows embedding systems with many time scales, see[Judd1998].\n\nIf provided, h can be weights to multiply the entries of the embedded space. If h isa Real then the embedding is\n\n(s(n) h cdot s(n+tau) w^2 cdot s(n+2tau) dotsw^d-1 cdot s(n+Œ≥tau))\n\nOtherwise h can be a vector of length d-1, which the decides the weights of each entry directly.\n\nReferences\n\n[Takens1981] : F. Takens, Detecting Strange Attractors in Turbulence ‚Äî Dynamical Systems and Turbulence, Lecture Notes in Mathematics 366, Springer (1981)\n\n[Sauer1991] : T. Sauer et al., J. Stat. Phys. 65, pp 579 (1991)\n\n[Judd1998]: K. Judd & A. Mees, Physica D 120, pp 273 (1998)\n\n[Farmer1988]: Farmer & Sidorowich, Exploiting Chaos to Predict the Future and Reduce Noise\"\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"Here are some examples of embedding a 3D continuous chaotic system:","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"using DynamicalSystems, PyPlot\n\nds = Systems.gissinger(ones(3))\ndata = trajectory(ds, 1000.0, dt = 0.05)\n\nxyz = columns(data)\n\nfigure(figsize = (12,10))\nk = 1\nfor i in 1:3\n    for œÑ in [5, 30, 100]\n        R = embed(xyz[i], 2, œÑ)\n        ax = subplot(3,3,k)\n        plot(R[:, 1], R[:, 2], color = \"C$(k-1)\", lw = 0.8)\n        title(\"var = $i, œÑ = $œÑ\")\n        global k+=1\n    end\nend\n\ntight_layout()\nsuptitle(\"2D reconstructed space\")\nsubplots_adjust(top=0.9)\nsavefig(\"simple_reconstruction.png\"); nothing # hide","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"(Image: )","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"note: `œÑ` and `dt`\nKeep in mind that whether a value of œÑ is \"reasonable\" for continuous systems depends on dt. In the above example the value œÑ=30 is good, only for the case of using dt = 0.05. For shorter/longer dt one has to adjust properly œÑ so that their product œÑ*dt is the same.","category":"page"},{"location":"embedding/reconstruction/#Embedding-Functors-1","page":"Delay Coordinates Embedding","title":"Embedding Functors","text":"","category":"section"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"The high level function embed utilize a low-level interface for creating embedded vectors on-the-fly. The high level interface simply loops over the low level interface. The low level interface is composed of the following two structures:","category":"page"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"DelayEmbedding\nœÑrange","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.DelayEmbedding","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.DelayEmbedding","text":"DelayEmbedding(Œ≥, œÑ, h = nothing) ‚Üí `embedding`\n\nReturn a delay coordinates embedding structure to be used as a function-like-object, given a timeseries and some index. Calling\n\nembedding(s, n)\n\nwill create the n-th delay vector of the embedded space, which has Œ≥ temporal neighbors with delay(s) œÑ. Œ≥ is the embedding dimension minus 1, œÑ is the delay time(s) while h are extra weights, as in embed for more.\n\nBe very careful when choosing n, because @inbounds is used internally. Use œÑrange!\n\n\n\n\n\n","category":"type"},{"location":"embedding/reconstruction/#DelayEmbeddings.œÑrange","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.œÑrange","text":"œÑrange(s, de::AbstractEmbedding)\n\nReturn the range r of valid indices n to create delay vectors out of s using de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/#Generalized-embeddings-1","page":"Delay Coordinates Embedding","title":"Generalized embeddings","text":"","category":"section"},{"location":"embedding/reconstruction/#","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"genembed\nGeneralizedEmbedding","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.genembed","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.genembed","text":"genembed(s, œÑs, js = ones(...)) ‚Üí dataset\n\nCreate a generalized embedding of s which can be a timeseries or arbitrary Dataset, and return the result as a new dataset.\n\nThe generalized embedding works as follows:\n\nœÑs denotes what delay times will be used for each of the entries of the delay vector. It is recommended that œÑs[1] = 0. œÑs is allowed to have negative entries as well.\njs denotes which of the timeseries contained in s will be used for the entries of the delay vector. js can contain duplicate indices.\n\nœÑs, js are tuples (or vectors) of length D, which also coincides with the embedding dimension. For example, imagine input trajectory s = x y z where x y z are timeseries (the columns of the Dataset). If js = (1, 3, 2) and œÑs = (0, 2, -7) the created delay vector at each step n will be\n\n(x(n) z(n+2) y(n-7))\n\njs can be skipped, defaulting to index 1 (first timeseries) for all delay entries.\n\nSee also embed. Internally uses GeneralizedEmbedding.\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/#DelayEmbeddings.GeneralizedEmbedding","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.GeneralizedEmbedding","text":"GeneralizedEmbedding(œÑs [, js]) -> `embedding`\n\nReturn a delay coordinates embedding structure to be used as a functor. Given a timeseries or trajectory (i.e. Dataset) s and calling\n\nembedding(s, n)\n\nwill create the delay vector of the n-th point of s in the embedded space using generalized embedding (see genembed).\n\njs is ignored for timeseries input s (since all entries of js must be 1 in this case) and in addition js defaults to (1, ..., 1) for all œÑ.\n\nBe very careful when choosing n, because @inbounds is used internally. Use œÑrange!\n\n\n\n\n\n","category":"type"},{"location":"rqa/windowed/#Windowed-RQA-1","page":"Windowed RQA","title":"Windowed RQA","text":"","category":"section"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"In some cases, specially with very long time series, it may be suitable to perform the analysis at different points, considering only a limited window of data around each observation. The macro @windowed modifies the behaviour of the basic functions to calculate RQA parameters in that fashion. For instance, if rmat is a 10<sup>4</sup>&times;10<sup>4</sup> recurrence matrix, then","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed determinism(rmat, theiler=2, lmin=3) width=1000 step=100","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"will return a 91-element vector, such that each value is the determinism associated to a 1000-point fragment, starting at every 100 points (i.e. at 1, 101, &hellip; 9001).","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"The general syntax of that macro is:","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed expr w                 #1\n@windowed expr width=w step=s    #2","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"where:","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"expr is an expression used to calculate RQA parameters\nw is the width of the window for relevant data around each point.\ns is the step or distance between points where the calculations are done (starting in the first point).","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"To prevent syntax failures in the expansion of the macro, identify the RQA function (rqa, recurrencerate, determinism,...) directly by its name (avoid aliases), and use simple variable names (not complex expressions) for the arguments. On the other hand, the windowing options width and step can be given in any order. If step is ommitted, the calculations are done at every point, and the keyword width may be ommitted. (However, using step=1 may be computationally very expensive, and that will provide just overly redundant results around each point, so it is advisable to set step a relatively big fraction of the window width.)","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"The value returned by the macro will normally be a vector with the same type of numbers as expected by expr. In the case of @windowed rqa(...) ..., it will return a NamedTuple with a similar structure as in the default rqa function, but replacing scalar values by vectors.","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"The macro @windowed can also be applied to the functions that calculate recurrence matrices (RecurrenceMatrix, CrossRecurrenceMatrix, JointRecurrenceMatrix). That creates a sparse matrix with the same size as if the macro was not used, but only containing valid values for pairs of points that belong to the w first main diagonals (i.e. the separation in time from one point to the other is w or smaller). The &lsquo;step&rsquo; parameter s has no effect on those functions. Such &lsquo;windowed&rsquo; matrices can be used as the input arguments to calculate windowed RQA parameters, obtaining the same results as if the complete matrix was used (under certain conditions, see below). For instance, the following calculations are equivalent:","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"# Using complete matrix\nrmat = RecurrenceMatrix(x, 1.5)\nd = @windowed determinism(rmat) width=1000 step=250\n\n# Using windowed matrix\nrmatw = @windowed RecurrenceMatrix(x, 1.5) 1000\nd = @windowed determinism(rmatw) width=1000 step=250","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"The main difference between the two alternatives is that the second one will be faster and consume less memory. To ensure the equivalence between both approaches, the window width used to create the matrix must be greater than the one used to calculate the RQA parameters. Otherwise, the computation of RQA parameters might involve data points whose value is not well defined. Besides, the threshold to identify recurrences should be referred to a fixed scale. For instance:","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"rmat  =           RecurrenceMatrix(x, 0.1, scale=maximum)\nrmatw = @windowed RecurrenceMatrix(x, 0.1, scale=maximum) 1000\nrmat[1:1000,1:1000] == rmatw[1:1000,1:1000] # FALSE!!!","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"In this example, the 1000√ó1000 blocks of both matrices differ, because the threshold 0.1 is scaled with respect to the maximum distance between all points of x in rmat, but in the case of rmatw the scale changes between subsets of points. Something similar may happen if the recurrence matrix is calculated for a fixed recurrence rate (with the option fixedrate=true).","category":"page"},{"location":"rqa/windowed/#Docstring-1","page":"Windowed RQA","title":"Docstring","text":"","category":"section"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed","category":"page"},{"location":"rqa/windowed/#RecurrenceAnalysis.@windowed","page":"Windowed RQA","title":"RecurrenceAnalysis.@windowed","text":"@windowed(f(x,...), width)\n@windowed(f(x,...); width, step=1)\n\nCalculate windowed RQA parameters with a given window width.\n\nf(x,...) may be any call to RQA functions (e.g. recurrencerate, determinism, etc.), with x being a named variable that designates the recurrence matrix (do not use in-place calculations of the recurrence matrix). The results are returned in a vector with one value for each position of the window. By default the window moves at one-point intervals, but a longer step length may be specified, together with the window width, by declaring those options as keyword arguments.\n\nThis macro may be also used with recurrence matrix constructors (RecurrenceMatrix, CrossRecurrenceMatrix, JointRecurrenceMatrix), to create 'incomplete' matrices that are suitable for such windowed RQA. The values of the resulting matrix in the diagonals within the window width will be equal to those obtained without the @windowed macro, if the distances are not scaled (using the option scale=1, see RecurrenceMatrix). Outside the window width, the values of the recurrence matrix will be undefined (mostly zero).\n\n\n\n\n\n","category":"macro"},{"location":"rqa/windowed/#Alternative-syntax-for-@windowed-1","page":"Windowed RQA","title":"Alternative syntax for @windowed","text":"","category":"section"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"The following ways of using the macro @windowed are equivalent:","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"y = @windowed f(x,...) w\n@windowed y=f(x,...) w\ny = @windowed(f(x,...), w)\n@windowed(y=f(x,...), w)","category":"page"},{"location":"rqa/windowed/#","page":"Windowed RQA","title":"Windowed RQA","text":"In all four cases, the width parameter w might have been qualified with a keyword as width=w. If the step parameter is added, the keyword qualification is mandatory.","category":"page"},{"location":"chaos/lyapunovs/#Lyapunov-Exponents-1","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"section"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Lyapunov exponents measure exponential rates of separation of nearby trajectories in the flow of a dynamical system. The Wikipedia and the Scholarpedia entries have a lot of valuable information about the history and usage of these quantities.","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"This page treats systems where the equations of motion are known. If instead you have numerical data, see numericallyapunov.","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"info: Performance depends on the solver\nNotice that the performance of functions that use ContinuousDynamicalSystems depend crucially on the chosen solver. Please see the documentation page on Choosing a solver for an in-depth discussion.","category":"page"},{"location":"chaos/lyapunovs/#Concept-of-the-Lyapunov-exponent-1","page":"Lyapunov Exponents","title":"Concept of the Lyapunov exponent","text":"","category":"section"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Before providing the documentation of the offered functionality, it is good to demonstrate exactly what are the Lyapunov exponents.","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For chaotic systems, nearby trajectories separate in time exponentially fast (while for stable systems they come close exponentially fast). This happens at least for small separations, and is demonstrated in the following sketch:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: ).","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"In this sketch lambda is the maximum Lyapunov exponent (and in general a system has as many exponents as its dimensionality).","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Let's demonstrate these concepts using a real system, the Henon map:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"beginaligned\nx_n+1 = 1 - ax_n^2 + y_n \ny_n+1 = bx_n\nendaligned","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Let's get a trajectory","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\nhenon = Systems.henon()\ntr1 = trajectory(henon, 100)\nsummary(tr1)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"and create one more trajectory that starts very close to the first one","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"u2 = get_state(henon) + (1e-9 * ones(dimension(henon)))\ntr2 = trajectory(henon, 100, u2)\nsummary(tr2)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"We now want to demonstrate how the distance between these two trajectories increases with time:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using LinearAlgebra: norm\n\nfigure(figsize=(8,5))\n\n# Plot the x-coordinate of the two trajectories:\nax1 = subplot(2,1,1)\nplot(tr1[:, 1], alpha = 0.5)\nplot(tr2[:, 1], alpha = 0.5)\nylabel(\"x\")\n\n# Plot their distance in a semilog plot:\nax2 = subplot(2,1,2, sharex = ax1)\nd = [norm(tr1[i] - tr2[i]) for i in 1:length(tr2)]\nylabel(\"d\"); xlabel(\"n\"); semilogy(d);\ntight_layout() # hide\nsavefig(\"demonstration.png\"); nothing # hide","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: )","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The initial slope of the d vs n plot (before the curve saturates) is approximately the maximum Lyapunov exponent!","category":"page"},{"location":"chaos/lyapunovs/#Lyapunov-Spectrum-1","page":"Lyapunov Exponents","title":"Lyapunov Spectrum","text":"","category":"section"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The function lyapunovs calculates the entire spectrum of the Lyapunov exponents of a system:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovs","category":"page"},{"location":"chaos/lyapunovs/#ChaosTools.lyapunovs","page":"Lyapunov Exponents","title":"ChaosTools.lyapunovs","text":"lyapunovs(ds::DynamicalSystem, N [, k::Int | Q0]; kwargs...) -> Œªs\n\nCalculate the spectrum of Lyapunov exponents [Lyapunov1992] of ds by applying a QR-decomposition on the parallelepiped matrix N times. Return the spectrum sorted from maximum to minimum.\n\nThe third argument k is optional, and dictates how many lyapunov exponents to calculate (defaults to dimension(ds)). Instead of passing an integer k you can pass a pre-initialized matrix Q0 whose columns are initial deviation vectors (then k = size(Q0)[2]).\n\nKeyword Arguments\n\nu0 = get_state(ds) : State to start from.\nTtr = 0 : Extra \"transient\" time to evolve the system before application of the algorithm. Should be Int for discrete systems. Both the system and the deviation vectors are evolved for this time.\ndt = 1 : Time of individual evolutions between successive orthonormalization steps. For continuous systems this is approximate.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nThe method we employ is \"H2\" of [Geist1990], originally stated in [Benettin1980]. The deviation vectors defining a D-dimensional parallepiped in tangent space are evolved using the tangent dynamics of the system. A QR-decomposition at each step yields the local growth rate for each dimension of the parallepiped. The growth rates are then averaged over N successive steps, yielding the lyapunov exponent spectrum (at each step the parallepiped is re-normalized).\n\nPerformance Notes\n\nThis function uses a tangent_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nlyapunovs(tinteg, N, dt::Real, Ttr::Real)\n\nIf you want to obtain the convergence timeseries of the Lyapunov spectrum, use the method\n\nChaosTools.lyapunovs_convergence(tinteg, N, dt, Ttr)\n\n(not exported).\n\n[Lyapunov1992]: A. M. Lyapunov, The General Problem of the Stability of Motion, Taylor & Francis (1992)\n\n[Geist1990]: K. Geist et al., Progr. Theor. Phys. 83, pp 875 (1990)\n\n[Benettin1980]: G. Benettin et al., Meccanica 15, pp 9-20 & 21-30 (1980)\n\n\n\n\n\n","category":"function"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"As you can see, the documentation string is detailed and self-contained. For example, the Lyapunov spectrum of the folded towel map is calculated as:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems\n\nds = Systems.towel()\nŒªŒª = lyapunovs(ds, 10000)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Similarly, for a continuous system, e.g. the Lorenz system, you would do:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lor = Systems.lorenz(œÅ = 32.0) #this is not the original parameter!\nŒªŒª = lyapunovs(lor, 10000, dt = 0.1)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovs is also very fast:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using BenchmarkTools\nds = Systems.towel()\n@btime lyapunovs($ds, 2000);","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"  237.226 Œºs (45 allocations: 4.27 KiB)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Here is an example of plotting the exponents of the Henon map for various parameters:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\n\nhe = Systems.henon()\nas = 0.8:0.005:1.225; Œªs = zeros(length(as), 2)\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    Œªs[i, :] .= lyapunovs(he, 10000; Ttr = 500)\nend\n\nfigure()\nplot(as, Œªs); xlabel(\"\\$a\\$\"); ylabel(\"\\$\\\\lambda\\$\")\ntight_layout() # hide\nsavefig(\"heŒª.png\"); nothing # hide","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: )","category":"page"},{"location":"chaos/lyapunovs/#Maximum-Lyapunov-Exponent-1","page":"Lyapunov Exponents","title":"Maximum Lyapunov Exponent","text":"","category":"section"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"It is possible to get only the maximum Lyapunov exponent simply by giving 1 as the third argument of lyapunovs. However, there is a second algorithm that allows you to do the same thing, which is offered by the function lyapunov:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunov","category":"page"},{"location":"chaos/lyapunovs/#ChaosTools.lyapunov","page":"Lyapunov Exponents","title":"ChaosTools.lyapunov","text":"lyapunov(ds::DynamicalSystem, Œ§; kwargs...) -> Œª\n\nCalculate the maximum Lyapunov exponent Œª using a method due to Benettin [Benettin1976], which simply evolves two neighboring trajectories (one called \"given\" and one called \"test\") while constantly rescaling the test one. T  denotes the total time of evolution (should be Int for discrete systems).\n\nKeyword Arguments\n\nTtr = 0 : Extra \"transient\" time to evolve the trajectories before starting to measure the expontent. Should be Int for discrete systems.\nd0 = 1e-9 : Initial & rescaling distance between the two neighboring trajectories.\nupper_threshold = 1e-6 : Upper distance threshold for rescaling.\nlower_threshold = 1e-12 : Lower distance threshold for rescaling (in order to  be able to detect negative exponents).\ndt = 1 : Time of evolution between each check of distance exceeding the thresholds. For continuous systems this is approximate.\ninittest = (u1, d0) -> u1 .+ d0/sqrt(D) : A function that given (u1, d0) initializes the test state with distance d0 from the given state u1 (D is the dimension of the system). This function can be used when you want to avoid the test state appearing in a region of the phase-space where it would have e.g. different energy or escape to infinity.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nTwo neighboring trajectories with initial distance d0 are evolved in time. At time t_i their distance d(t_i) either exceeds the upper_threshold, or is lower than lower_threshold, which initializes a rescaling of the test trajectory back to having distance d0 from the given one, while the rescaling keeps the difference vector along the maximal expansion/contraction direction: u_2 to u_1+(u_2u_1)(d(t_i)d_0).\n\nThe maximum Lyapunov exponent is the average of the time-local Lyapunov exponents\n\nlambda = frac1t_n - t_0sum_i=1^n\nlnleft( a_i right)quad a_i = fracd(t_i)d_0\n\nPerformance Notes\n\nThis function uses a parallel_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nlyapunov(pinteg, T, Ttr, dt, d0, ut, lt)\n\n[Benettin1976]: G. Benettin et al., Phys. Rev. A 14, pp 2338 (1976)\n\n\n\n\n\n","category":"function"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For example:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\nhenon = Systems.henon()\nŒª = lyapunov(henon, 10000, d0 = 1e-7, upper_threshold = 1e-4, Ttr = 100)","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The same is done for continuous systems:","category":"page"},{"location":"chaos/lyapunovs/#","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lor = Systems.lorenz(œÅ = 32)\nŒª = lyapunov(lor, 10000.0, dt = 10.0, Ttr = 100.0)","category":"page"},{"location":"chaos/choosing/#Choosing-a-solver-1","page":"Choosing a solver","title":"Choosing a solver","text":"","category":"section"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"ContinuousDynamicalSystems are evolved using solvers from DifferentialEquations.jl. In this page we discuss the importance of which solver to choose.","category":"page"},{"location":"chaos/choosing/#Default-Solver-1","page":"Choosing a solver","title":"Default Solver","text":"","category":"section"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"The default solver is:","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"using DynamicalSystems\nDynamicalSystemsBase.DEFAULT_SOLVER","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"which is a Runge-Kutta-like solver. The number in the solver's name is the \"order\" of the solver.","category":"page"},{"location":"chaos/choosing/#Speed-of-a-solver-1","page":"Choosing a solver","title":"Speed of a solver","text":"","category":"section"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"Estimating a given solver's performance for a particular problem is not trivial. The following are general rules of thumb:","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"Higher order solvers call the equations of motion function more times per step.\nHigher order solvers can cover larger timespans per step.\nHigher order solvers do better at small tolerances.","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"This means that there is a delicate balance between how expensive is your function and how large of a step a solver can take while it is still efficient. In general you want to strike a point of taking large steps but also not calling the function exceedingly often.","category":"page"},{"location":"chaos/choosing/#How-do-I-pick?-1","page":"Choosing a solver","title":"How do I pick?","text":"","category":"section"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"The answer to this question is easy: benchmarks!","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"Here is a simple case: let's compute the Lyapunov spectrum of the Lorenz system using lyapunovs:","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"ds = Systems.lorenz()\ntols = (abstol = 1e-6, reltol = 1e-6)\nlyapunovs(ds, 2000; Ttr = 100.0, tols...)","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"The above uses the default solver. Let's now benchmark using two different solvers, SimpleATsit5 and Vern9. Since the SimpleATsit5 case is of lower order, naively one might think it is faster because it makes less function calls. This argument is not necessarily true though.","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"It is important to understand that when calling lyapunovs(ds, 2000) you want the system (and the tangent space) to be evolved so that it reaches a total time of 2000*dt, which by default is 2000.0 units of time. Even though SimpleATsit5 requires less function calls per step, Vern9 can cover larger timespans per step.","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"Here are the numbers:","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"using BenchmarkTools, OrdinaryDiffEq, SimpleDiffEq, Statistics\nb1 = @benchmark lyapunovs(ds, 2000; alg = SimpleATsit5(), Ttr = 100.0, tols...);\nb2 = @benchmark lyapunovs(ds, 2000; alg = Vern9(),        Ttr = 100.0, tols...);\nprintln(\"Timing for SimpleATsit5:\")\nprintln(mean(b1))\nprintln(\"Timing for Vern9:\")\nprintln(mean(b2))","category":"page"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"As you can see Vern9 is faster in doing the entire computation! Of course this does not have to be universally true. It is true for the Lorenz system, but for your specific system you should do dedicated benchmarks!","category":"page"},{"location":"chaos/choosing/#DifferentialEquations.jl-1","page":"Choosing a solver","title":"DifferentialEquations.jl","text":"","category":"section"},{"location":"chaos/choosing/#","page":"Choosing a solver","title":"Choosing a solver","text":"For more info about the possible solvers be sure to head over to the documentation of DifferentialEquations.jl!","category":"page"},{"location":"rqa/quantification/#Recurrence-Quantification-Analysis-1","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"A RecurrenceMatrix can be analyzed in several ways to yield information about the dynamics of the trajectory. All these various measures and functions are collectively called \"Recurrence Quantification Analysis\" (RQA).","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"To understand how each measure can be useful, we suggest to see the review articles listed in our documentation strings, namely:","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007).\nN. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"You can also check the wikipedia page for Recurrence quantification analysis.","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"The functions described in this page all accept a recurrence matrix (x), see RecurrenceMatrix.","category":"page"},{"location":"rqa/quantification/#RQA-Measures-1","page":"Recurrence Quantification Analysis","title":"RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/#All-in-one-Bundle-1","page":"Recurrence Quantification Analysis","title":"All-in-one Bundle","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"In case you need all of the RQA-related functions (see below) and you don't want to write 10 lines of code to compute them all (since they are so many) we provide an all-in-one function that computes all of them and returns a NamedTuple of the results!","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"rqa","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.rqa","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rqa","text":"rqa(R; kwargs...)\n\nCalculate all RQA parameters of a recurrence matrix R. See the functions referred to below for the definition of the different parameters and the default values of the arguments. Using this function is much more efficient than calling all individual functions one by one.\n\nReturn\n\nThe returned value is a NamedTuple with the following entries:\n\nRR: recurrence rate (see recurrencerate)\nTRANS: transitivity (see transitivity)\nDET: determinsm (see determinism)\nL: average length of diagonal structures (see dl_average)\nLmax: maximum length of diagonal structures (see dl_max)\nDIV: divergence (see divergence)\nENTR: entropy of diagonal structures (see dl_entropy)\nTREND: trend of recurrences (see trend)\nLAM: laminarity (see laminarity)\nTT: trapping time (see trappingtime)\nVmax: maximum length of vertical structures (see vl_max)\nVENTR: entropy of vertical structures (see vl_entropy)\nMRT: mean recurrence time (see meanrecurrencetime)\nRTE recurrence time entropy (see rt_entropy)\nNMPRT: number of the most probable recurrence time (see nmprt)\n\nIn the case of empty histograms (e.g. no existing vertical lines less than the keyword lminvert) the average and maximum values (L, Lmax, TT, Vmax, MRT) are returned as 0.0 but their respective entropies (ENTR, VENTR, RTE) are returned as NaN.\n\nKeyword Arguments\n\nStandard keyword arguments are the ones accepted by the functions listed below, i.e. theiler, lmin, and border:\n\ntheiler is used to define a \"Theiler window\" around the central diagonal or \"line of identity\" (LOI): a region of points that are excluded in the calculation of RQA parameters, in order to rule out self-recurrences and apparent recurrences for smooth or high resolution data. The LOI is excluded by default for matrices of the types RecurrenceMatrix or JointRecurrenceMatrix, but it is included for matrices of the type CrossRecurrenceMatrix. theiler=0 means that the whole matrix is scanned for lines. theiler=1 means that the LOI is excluded. In general, theiler=n means that the n central diagonals are excluded (at both sides of the LOI, i.e. actually 2n-1 diagonals are excluded).\nlmin is used to define the minimum line length in the parameters that describe the distributions of diagonal or vertical lines (it is set as 2 by default).\nborder is used to avoid border effects in the calculation of TREND (cf. trend).\n\nIn addition theilerdiag, lmindiag may be used to declare specific values that override the values of theiler and lmin in the calculation of parameters related to diagonal structures. Likewise, theilervert and lminvert can be used for the calculation of parameters related to vertical structures.\n\nThe keyword argument onlydiagonal (false by default) can be set to true in order to restrict the analysis to the recurrence rate and the parameters related to diagonal structures (RR, DET, L, Lmax, DIV and ENTR), which makes this function slightly faster.\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"note: Return values for empty histograms\nIt may be the case that for a given recurrence matrix some structures do not exist at all. For example there are recurrence matrices that have no vertical lengths (or no vertical lengths with length less than lmin). In such cases the behavior of our RQA pipeline is the following:Quantities that represent maximum or average values are 0.0.\nQuantities that represent entropies are NaN.","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"See also the @windowed macro for a windowed version of rqa.","category":"page"},{"location":"rqa/quantification/#Classical-RQA-Measures-1","page":"Recurrence Quantification Analysis","title":"Classical RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"recurrencerate\ndeterminism\ndl_average\ndl_max\ndl_entropy\ndivergence\ntrend","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.recurrencerate","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencerate","text":"recurrencerate(R[; theiler])\n\nCalculate the recurrence rate of the recurrence matrix R.\n\nDescription\n\nThe recurrence rate is calculated as:\n\nRR = frac1S sum R\n\nwhere S is the size of R or the region of R with potential recurrent points. There is not a unique definition of that denominator, which is defined as the full size of the matrix in many sources (e.g. [1]), whereas in others it is adjusted to remove the points of the LOI when they are excluded from the count [2,3].\n\nFor matrices of type RecurrenceMatrix or JointRecurrenceMatrix, where the points around the central diagonal are usually excluded, the denominator is adjusted to the size of the matrix outside the Theiler window (by default equal to the LOI, and adjustable with the keyword argument theiler; see rqa for details). For matrices of type CrossRecurrenceMatrix, where normally all points are analyzed, the denominator is always the full size of the matrix, regardless of the Theiler window that might be defined (none by default).\n\nHint: to reproduce the calculations done following the formulas that use the full size of the matrix in the denominator, use CrossRecurrenceMatrix(s,s,Œµ) to define the recurrence matrix, instead of RecurrenceMatrix(s,Œµ), setting theiler=1 (or theiler=n in general) to explicitly exclude the LOI or other diagonals around it.\n\nReferences\n\n[1] : N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007).\n\n[2] : C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 26-94 (2005). URL: https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf\n\n[3] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.determinism","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.determinism","text":"determinism(R[; lmin=2, theiler])\n\nCalculate the determinism of the recurrence matrix R:\n\nDescription\n\nThe determinism is calculated as:\n\nDET = fracsum_l=lminl P(l)sum_l=1l P(l) =\nfracsum_l=lminl P(l)sum R\n\nwhere l stands for the lengths of diagonal lines in the matrix, and P(l) is the number of lines of length equal to l.\n\nlmin is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_average","text":"dl_average(R[; lmin=2, theiler])\n\nCalculate the average of the diagonal lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_max","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_max","text":"dl_max(R[; lmin=2, theiler])\n\nCalculate the longest diagonal line contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_entropy","text":"dl_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the diagonal lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.divergence","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.divergence","text":"divergence(R[; theiler])\n\nCalculate the divergence of the recurrence matrix R (actually the inverse of dl_max).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.trend","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trend","text":"trend(R[; border=10, theiler])\n\nCalculate the trend of recurrences in the recurrence matrix R.\n\nDescription\n\nThe trend is the slope of the linear regression that relates the density of recurrent points in the diagonals parallel to the LOI and the distance between those diagonals and the LOI. It quantifies the degree of system stationarity, such that in recurrence plots where points \"fade away\" from the central diagonal, the trend will have a negative value.\n\nIt is calculated as:\n\nTREND = 10^3fracsum_d=tau^tildeNdeltadleft(RRd-langle RRdrangleright)sum_d=tau^tildeNdeltad^2\n\nwhere RRd is the local recurrence rate of the diagonal d, deltad is a balanced measure of the distance between that diagonal and the LOI, tau is the Theiler window (number of central diagonals that are excluded), and tildeN is the number of the outmost diagonal that is included.\n\nThis parameter is expressed in units of variation recurrence rate every 1000 data points, hence the factor 10^3 in the formula [1].\n\nThe 10 outermost diagonals (counting from the corners of the matrix) are excluded by default to avoid \"border effects\". Use the keyword argument border to define a different number of excluded lines, and theiler to define the size of the Theiler window (see rqa for details).\n\nNote: In rectangular cross-recurrence plots (i.e. when the time series that originate them are not of the same length), the limits of the formula for TREND are not clearly defined. For the sake of consistency, this function limits the calculations to the biggest square matrix that contains the LOI.\n\nReferences\n\n[1] C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 2005, 26-94. https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Extended-RQA-Measures-1","page":"Recurrence Quantification Analysis","title":"Extended RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"laminarity\ntrappingtime\nvl_average\nvl_max\nvl_entropy","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.laminarity","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.laminarity","text":"laminarity(R[; lmin=2, theiler])\n\nCalculate the laminarity of the recurrence matrix R.\n\nDescription\n\nThe laminarity is calculated as:\n\nLAM = fracsum_v=lminv P(l)sum_v=1v P(v) =\nfracsum_v=lminv P(l)sum R\n\nwhere v stands for the lengths of vertical lines in the matrix, and P(v) is the number of lines of length equal to v.\n\nlmin is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.trappingtime","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trappingtime","text":"trappingtime(R[; lmin=2, theiler])\n\nCalculate the trapping time of the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nThe trapping time is the average of the vertical line structures and thus equal to vl_average.\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_average","text":"vl_average(R[; lmin=2, theiler])\n\nCalculate the average of the vertical lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_max","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_max","text":"vl_max(R[; lmin=2, theiler])\n\nCalculate the longest vertical line contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_entropy","text":"vl_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the vertical lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Recurrence-Time-Measures-1","page":"Recurrence Quantification Analysis","title":"Recurrence Time Measures","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"meanrecurrencetime\nnmprt\nrt_entropy\nrt_average","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.meanrecurrencetime","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.meanrecurrencetime","text":"meanrecurrencetime(R[; lmin=2, theiler])\n\nCalculate the mean recurrence time of the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nEquivalent to rt_average.\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.nmprt","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.nmprt","text":"nmprt(R[; lmin=2, theiler])\n\nCalculate the number of the most probable recurrence time (NMPRT), ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nThis number indicates how many times the system has recurred using the recurrence time that appears most frequently, i.e it is the maximum value of the histogram of recurrence times [1].\n\nReferences\n\n[1] : E.J. Ngamga et al. \"Recurrence analysis of strange nonchaotic dynamics\", Physical Review E, 75(3), 036222(1-8), 2007, DOI:10.1103/physreve.75.036222\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.rt_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_entropy","text":"rt_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the recurrence times contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.rt_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_average","text":"rt_average(R[; lmin=2, theiler])\n\nCalculate the average of the recurrence times contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Network-inspired-RQA-Measures-1","page":"Recurrence Quantification Analysis","title":"Network-inspired RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"transitivity","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.transitivity","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.transitivity","text":"transitivity(R::AbstractRecurrenceMatrix) ‚Üí T\n\nReturns the network transitivity T of the Œµ-recurrence network R. Here the recurrence plot R is identified as the network adjacency matrix A.\n\nDescription\n\nWe quote from [Donner2011], where the authors provide a complete description: Transitivity is related to fundamental algebraic relationships between triples of discrete objects. Specifically, in graph-theoretical terms, we identify the set X with the set of vertices V , and the relation R with the mutual adjacency of pairs of vertices. Hence, for a given vertexi ‚àà V, transitivity refers to the fact that for two other verticesj, k ‚àà VwithAij = Aik = 1, A_jk = 1` also holds. In a general network, this is typically not the case for all vertices. Consequently, characterising the degree of transitivity (or, alternatively, the relative frequency of closed 3-loops, which are commonly referred to as triangles) with respect to some individual vertex or the whole network provides important information on the structural graph properties, which may be related to important general features of the underlying system.\n\nThe network transitivity averages the local transitivity or clustering coefficient\n\nmathcalC = fracnumber of triangles including vertex inumber of triples centred on vertex i\n\nover the all nodes in the network [Boccaletti2006]:\n\nmathcalT = fractrace(R^3)sum R^2\n\nReferences\n\n[Donner2011]: R.V. Donner et al., The geometry of chaotic dynamics ‚Äî a complex network perspective, Eur. Phys. J. B 84, 653‚Äì672 (2011)\n\n[Boccaletti2006]: S.Boccaletti et al., Complex networks: Structure and dynamics, Physics Reports Volume 424, Issues 4‚Äì5 (2006)\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Keyword-table-1","page":"Recurrence Quantification Analysis","title":"Keyword table","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"Since most of the above functions can be fined tuned with keyword arguments, here is a table summarizing them that could be of use:","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"Argument Default Functions Description\ntheiler 0 for CrossRecurrenceMatrix, 1 otherwise. recurrencerate<br/>determinism<br/>*_average<br/>*_max<br/>*_entropy<br/>divergence<br/>trend<br/>laminarity<br/>trappingtime<br/> meanrecurrencetime<br/>nmprt Theiler window: number of diagonals around the LOI excluded from the analysis. The value 0 means that the LOI is included in the analysis. Use 1 to exclude the LOI.\nlmin 2 determinism<br/>*_average<br/>*_max<br/>*_entropy<br/>divergence<br/>laminarity<br/>trappingtime<br/> meanrecurrencetime<br/>nmprt Minimum length of the recurrent structures (diagonal or vertical) considered in the analysis.\nborder 10 trend Number of diagonals excluded from the analysis near the border of the matrix.","category":"page"},{"location":"rqa/quantification/#Recurrence-Structures-Histograms-1","page":"Recurrence Quantification Analysis","title":"Recurrence Structures Histograms","text":"","category":"section"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"The functions that we list in this page internally compute histograms of some recurrence structures, like e.g. the vertical lengths. You can access these values directly with the following function:","category":"page"},{"location":"rqa/quantification/#","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"recurrencestructures","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.recurrencestructures","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencestructures","text":"recurrencestructures(x::AbstractRecurrenceMatrix;\n                         diagonal=true,\n                         vertical=true,\n                         recurrencetimes=true,\n                         kwargs...)\n\nReturn a dictionary with the histograms of the recurrence structures contained in the recurrence matrix x, with the keys \"diagonal\", \"vertical\" or \"recurrencetimes\", depending on what keyword arguments are given as true.\n\nDescription\n\nEach item of the dictionary is a vector of integers, such that the i-th element of the vector is the number of lines of length i contained in x.\n\n\"diagonal\" counts the diagonal lines, i.e. the recurrent trajectories.\n\"vertical\" counts the vertical lines, i.e. the laminar states.\n\"recurrencetimes\" counts the vertical distances between recurrent states,   i.e. the recurrence times.\n\nAll the points of the matrix are counted by default. The keyword argument theiler can be passed to rule out the lines around the main diagonal. See the arguments of the function rqa for further details.\n\n\"Empty\" histograms are represented always as [0].\n\nNotice: There is not a unique operational definition of \"recurrence times\". In the analysis of recurrence plots, usually the  \"second type\" of recurrence times as defined by Gao and Cai [1] are considered, i.e. the distance between consecutive (but separated) recurrent structures in the vertical direction of the matrix. But that distance is not uniquely defined when the vertical recurrent structures are longer than one point. The recurrence times calculated here are the distance between the midpoints of consecutive lines, which is a balanced estimator of the Poincar√© recurrence times [2].\n\nReferences\n\n[1] J. Gao & H. Cai. \"On the structures and quantification of recurrence plots\". Physics Letters A, 270(1-2), 75‚Äì87 (2000).\n\n[2] N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#Recurrence-Plots-1","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/#Recurrence-Matrices-1","page":"Recurrence Plots","title":"Recurrence Matrices","text":"","category":"section"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"A Recurrence plot (which refers to the plot of a matrix) is a way to quantify recurrences that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the phase space that it was at some previous time.","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"The central structure used in these recurrences is the (cross-) recurrence matrix:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"Ri j = begincases\n1 quad textifquad d(xi yj) le varepsilon\n0 quad textelse\nendcases","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"where d(xi yj) stands for the distance between trajectory x at point i and trajectory y at point j. Both x y can be single timeseries, full trajectories or embedded timeseries (which are also trajectories).","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"If xequiv y then R is called recurrence matrix, otherwise it is called cross-recurrence matrix. There is also the joint-recurrence variant, see below. With RecurrenceAnalysis you can use the following functions to access these matrices","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"RecurrenceMatrix\nCrossRecurrenceMatrix\nJointRecurrenceMatrix","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.RecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceMatrix","text":"RecurrenceMatrix(x, Œµ; kwargs...)\n\nCreate a recurrence matrix from trajectory x. Objects of type <:AbstractRecurrenceMatrix are displayed as a recurrenceplot.\n\nDescription\n\nThe recurrence matrix is a numeric representation of a \"recurrence plot\" [^1, ^2], in the form of a sparse square matrix of Boolean values.\n\nx must be a Vector or a Dataset or a Matrix with data points in rows (possibly representing and embedded phase space; see embed). If d(x[i], x[j]) ‚â§ Œµ (with d the distance function), then the cell (i, j) of the matrix will have a true value. The criteria to evaluate distances between data points are defined by the following keyword arguments:\n\nscale=1 : a function of the distance matrix (see distancematrix), or a fixed number, used to scale the value of Œµ. Typical choices are maximum or mean, such that the threshold Œµ is defined as a ratio of the maximum or the mean distance between data points, respectively (using mean or maximum calls specialized versions that are faster than the naive approach).  Use 1 to keep the distances unscaled (default).\nfixedrate::Bool=false : a flag that indicates if Œµ should be taken as a target fixed recurrence rate (see recurrencerate). If fixedrate is set to true, Œµ must be a value between 0 and 1, and scale is ignored.\nmetric=\"euclidean\" : metric of the distances, either Metric or a string,  as in distancematrix.\nparallel=false : whether to parallelize the computation of the recurrence  matrix.  This will split the computation of the matrix across multiple threads.  This may not work on Julia versions before v1.3, so be warned!\n\nSee also: CrossRecurrenceMatrix, JointRecurrenceMatrix and use recurrenceplot to turn the result of these functions into a plottable format.\n\nReferences\n\n[1] : N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007).\n\n[2] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#RecurrenceAnalysis.CrossRecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.CrossRecurrenceMatrix","text":"CrossRecurrenceMatrix(x, y, Œµ; kwargs...)\n\nCreate a cross recurrence matrix from trajectories x and y.\n\nThe cross recurrence matrix is a bivariate extension of the recurrence matrix. For the time series x, y, of length n and m, respectively, it is a sparse n√óm matrix of Boolean values, such that if d(x[i], y[j]) ‚â§ Œµ, then the cell (i, j) of the matrix will have a true value.\n\nSee RecurrenceMatrix for details, references and keywords. See also: JointRecurrenceMatrix.\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#RecurrenceAnalysis.JointRecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.JointRecurrenceMatrix","text":"JointRecurrenceMatrix(x, y, Œµ; kwargs...)\n\nCreate a joint recurrence matrix from x and y.\n\nThe joint recurrence matrix considers the recurrences of the trajectories of x and y separately, and looks for points where both recur simultaneously. It is calculated by the element-wise multiplication of the recurrence matrices of x and y. If x and y are of different length, the recurrences are only calculated until the length of the shortest one.\n\nSee RecurrenceMatrix for details, references and keywords. See also: CrossRecurrenceMatrix.\n\n\n\n\n\nJointRecurrenceMatrix(R1, R2; kwargs...)\n\nCreate a joint recurrence matrix from given recurrence matrices R1, R2.\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#Simple-Recurrence-Plots-1","page":"Recurrence Plots","title":"Simple Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"The recurrence matrices are internally stored as sparse matrices with boolean values. Typically in the literature one does not \"see\" the matrices themselves but instead a plot of them (hence \"Recurrence Plots\"). By default, when a Recurrence Matrix is created we \"show\" a mini plot of it which is a text-based scatterplot.","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"Here is an example recurrence plot/matrix of a full trajectory of the Roessler system:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"using DynamicalSystems\nro = Systems.roessler(ones(3), a=0.15, b=0.20, c=10.0)\nN = 2000; dt = 0.05\ntr = trajectory(ro, N*dt; dt = dt, Ttr = 10.0)\n\nR = RecurrenceMatrix(tr, 5.0; metric = \"euclidean\")\nrecurrenceplot(R; ascii = true)","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"typeof(R)","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"summary(R)","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"The above simple plotting functionality is possible through the package UnicodePlots. The following function creates the plot:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"recurrenceplot","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.recurrenceplot","page":"Recurrence Plots","title":"RecurrenceAnalysis.recurrenceplot","text":"recurrenceplot([io,] R; minh = 25, maxh = 0.5, ascii, kwargs...) -> u\n\nCreate a text-based scatterplot representation of a recurrence matrix R to be displayed in io (by default stdout) using UnicodePlots. The matrix spans at minimum minh rows and at maximum maxh*displaysize(io)[1] (i.e. by default half the display). As we always try to plot in equal aspect ratio, if the width of the plot is even less, the minimum height is dictated by the width.\n\nThe keyword ascii::Bool can ensure that all elements of the plot are ASCII characters (true) or Unicode (false).\n\nThe rest of the kwargs are propagated into UnicodePlots.scatterplot.\n\nNotice that the accuracy of this function drops drastically for matrices whose size is significantly bigger than the width and height of the display (assuming each index of the matrix is one character).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"Here is the same plot but using Unicode Braille characters","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"recurrenceplot(R; ascii = false)","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"As you can see, the Unicode based plotting doesn't display nicely everywhere. It does display perfectly in e.g. Juno, which is where it is the default printing type. Here is how it looks like in a dark background:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/#Advanced-Recurrence-Plots-1","page":"Recurrence Plots","title":"Advanced Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"A text-based plot is cool, fast and simple. But often one needs the full resolution offered by the data of a recurrence matrix.","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"There are two more ways to plot a recurrence matrix using RecurrenceAnalysis:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"coordinates\ngrayscale","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.coordinates","page":"Recurrence Plots","title":"RecurrenceAnalysis.coordinates","text":"coordinates(R) -> xs, ys\n\nReturn the coordinates of the recurrence points of R (in indices).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#RecurrenceAnalysis.grayscale","page":"Recurrence Plots","title":"RecurrenceAnalysis.grayscale","text":"grayscale(R [, bwcode]; width::Int, height::Int, exactsize=false)\n\nTransform the recurrence matrix R into a full matrix suitable for plotting as a grayscale image. By default it returns a matrix with the same size as R, but switched axes, containing \"black\" values in the cells that represent recurrent points, and \"white\" values in the empty cells and interpolating in-between for cases with both recurrent and empty cells, see below.\n\nThe numeric codes for black and white are given in a 2-element tuple as a second optional argument. Its default value is (0.0, 1.0), i.e. black is coded as 0.0 (no brightness) and white as 1.0 (full brightness). The type of the elements in the tuple defines the type of the returned matrix. This must be taken into account if, for instance, the image is coded as a matrix of integers corresponding to a grayscale; in such case the black and white codes must be given as numbers of the required integer type.\n\nThe keyword arguments width and height can be given to define a custom size of the image. If only one dimension is given, the other is automatically calculated. If both dimensions are given, by default they are adjusted to keep an aspect proportional to the original matrix, such that the returned matrix fits into a matrix of the given dimensions. This automatic adjustment can be disabled by passing the keyword argument exactsize=true.\n\nIf the image has different dimensions than R, the cells of R are distributed in a grid with the size of the image, and a gray level between white and black is calculated for each element of the grid, proportional to the number of recurrent points contained in it. The levels of gray are coded as numbers of the same type as the black and white codes.\n\nIt is advised to use width, height arguments for large matrices otherwise plots using functions like e.g. imshow could be misleading.\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"For example, here is the representation of the above R from the Roessler system using both plotting approaches:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot\nfigure(figsize = (10,5))\n\nax = subplot(121)\nxs, ys = coordinates(R)\nscatter(xs, ys, color = \"k\", s = 1)\nxlim(1, size(R)[1]); ylim(1, size(R)[2]);\nax.set_aspect(\"equal\")\n\nsubplot(122)\nRg = grayscale(R)\nimshow(Rg, cmap = \"binary_r\", extent = (1, size(R)[1], 1, size(R)[2]))\nsavefig(\"different_rplots.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"and here is exactly the same process, but using the embedded trajectory instead","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot # hide\ny = tr[:, 2]\nœÑ = estimate_delay(y, \"mi_min\")\nm = embed(y, 3, œÑ)\nR = RecurrenceMatrix(m, 5.0; metric = \"euclidean\")\n\nfigure(figsize = (5,5))\n\nxs, ys = coordinates(R)\nscatter(xs, ys, color = \"k\", s = 1)\nxlim(1, size(R)[1]); ylim(1, size(R)[2]);\nsavefig(\"rmatrix2.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"which justifies why recurrence plots are so fitting to be used in embedded timeseries.","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"warning: Careful when using Recurrence Plots\nIt is easy when using grayscale to not change the width/height parameters. These are however very important when the matrix size exceeds the display size! Most plotting libraries may resample arbitrarily or simply limit the displayed pixels, so one needs to be extra careful.Besides graphical problems there are also other potential pitfalls dealing with the conceptual understanding and use of recurrence plots. All of these are summarized in the following paper which we suggest users to take a look at:N. Marwan, How to avoid potential pitfalls in recurrence plot based data analysis, Int. J. of Bifurcations and Chaos (arXiv).","category":"page"},{"location":"rqa/rplots/#Example-1","page":"Recurrence Plots","title":"Example","text":"","category":"section"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"In the following we will plot recurrence plots of the Lorenz system for a periodic and chaotic regime (using scatter plot).","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot # hide\nlor = Systems.lorenz()\nfigure(figsize = (10,10))\n\nfor (i, œÅ) in enumerate((69.75, 28.0))\n    set_parameter!(lor, 2, œÅ)\n    t, dt = 20.0, 0.01\n    tr = trajectory(lor, t; dt = dt, Ttr = 2000.0)\n    tvec = 0:dt:t\n\n    subplot(2,2, i)\n    plot(tr[:, 1], tr[:, 3], color = \"C$(i+1)\", label = \"X vs Z\")\n    title(\"œÅ = $œÅ, \" * (i != 1 ? \"not periodic\" : \"periodic\")); legend()\n\n    Œµ = i == 1 ? 5.0 : 3.0\n    R = RecurrenceMatrix(tr, Œµ)\n\n    subplot(2,2,i+2)\n    x, y = coordinates(R)\n    scatter(tvec[x], tvec[y], s = 1, alpha = 0.2, color = \"C$(i+1)\")\n    xlim(0, t); ylim(0, t); gca().set_aspect(\"equal\")\n    xlabel(\"t\"); i == 1 && ylabel(\"t\");\nend\nPyPlot.tight_layout()\nsavefig(\"rplotexamples.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"On the left we see long (infinite) diagonals repeated over and over for different times. This is the case for periodic systems as they visit exactly the same area on the phase space again and again. The distance between the offset diagonals also coincides with the periodicity of the system, which is around t ‚âà 4.","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"On the right we see a structure typical of chaotic motion on a strange attractor such as the one of the Lorenz system: the orbit visits neighborhoods of previous points but then quickly diverges again. This results in many small diagonal lines.","category":"page"},{"location":"rqa/rplots/#Distances-1","page":"Recurrence Plots","title":"Distances","text":"","category":"section"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"The distance function used in RecurrenceMatrix and co. can be specified either as a string or as any Metric instance from Distances. In addition, the following function returns a matrix with the cross-distances across all points in one or two trajectories:","category":"page"},{"location":"rqa/rplots/#","page":"Recurrence Plots","title":"Recurrence Plots","text":"distancematrix","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.distancematrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.distancematrix","text":"distancematrix(x [, y = x], metric = \"euclidean\")\n\nCreate a matrix with the distances between each pair of points of the time series x and y using metric.\n\nThe time series x and y can be Datasets or vectors or matrices with data points in rows. The data point dimensions (or number of columns) must be the same for x and y. The returned value is a n√óm matrix, with n being the length (or number of rows) of x, and m the length of y.\n\nThe metric can be identified by a string, or any of the Metrics defined in the Distances package. The list of strings available to define the metric are:\n\n\"max\" or \"inf\" for the maximum or L‚àû norm (Chebyshev() in the Distances package).\n\"euclidean\" for the L2 or Euclidean norm, used by default (Euclidean() in Distances).\n\"manhattan\", \"cityblock\", \"taxicab\" or \"min\" for the Manhattan or L1 norm (Cityblock() in Distances).\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Detecting-and-Categorizing-Chaos-1","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Being able to detect and distinguish chaotic from regular behavior is crucial in the study of dynamical systems. Most of the time a positive maximum lyapunov exponent and a bounded system indicate chaos.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"However, the convergence of the Lyapunov exponent can be slow, or even misleading, as the types of chaotic behavior vary with respect to their predictability. There are many alternatives, some more efficient and some more accurate in characterizing chaotic and regular motion. Some of these methods are included in DynamicalSystems.jl.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"info: Performance depends on the solver\nNotice that the performance of functions that use ContinuousDynamicalSystems depend crucially on the chosen solver. Please see the documentation page on Choosing a solver for an in-depth discussion.","category":"page"},{"location":"chaos/chaos_detection/#Generalized-Alignment-Index-1","page":"Detecting & Categorizing Chaos","title":"Generalized Alignment Index","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"\"GALI\" for sort, is a method that relies on the fact that initially orthogonal deviation vectors tend to align towards the direction of the maximum Lyapunov exponent for chaotic motion. It is one of the most recent and cheapest methods for distinguishing chaotic and regular behavior, introduced first in 2007 by Skokos, Bountis & Antonopoulos.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"gali","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.gali","page":"Detecting & Categorizing Chaos","title":"ChaosTools.gali","text":"gali(ds::DynamicalSystem, tmax, k::Int | Q0; kwargs...) -> GALI_k, t\n\nCompute textGALI_k[Skokos2007] for a given k up to time tmax. Return textGALI_k(t) and time vector t.\n\nThe third argument, which sets the order of gali, can be an integer k, or a matrix with its columns being the deviation vectors (then k = size(Q0)[2]). In the first case random orthonormal vectors are chosen.\n\nKeyword Arguments\n\nthreshold = 1e-12 : If GALI_k falls below the threshold iteration is terminated.\ndt = 1 : Time-step between deviation vector normalizations. For continuous systems this is approximate.\nu0 : Initial state for the system. Defaults to get_state(ds).\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nThe Generalized Alignment Index, textGALI_k, is an efficient (and very fast) indicator of chaotic or regular behavior type in D-dimensional Hamiltonian systems (D is number of variables). The asymptotic behavior of textGALI_k(t) depends critically on the type of orbit resulting from the initial condition. If it is a chaotic orbit, then\n\ntextGALI_k(t) sim\nexpleftsum_j=1^k (lambda_1 - lambda_j)t right\n\nwith lambda_j being the j-th Lyapunov exponent (see lyapunov, lyapunovs). If on the other hand the orbit is regular, corresponding to movement in d-dimensional torus with 1 le d le D2 then it holds\n\ntextGALI_k(t) sim\n    begincases\n      textconst  textif  2 le k le d    textand\n       d  1 \n      t^-(k - d)  textif   d  k le D - d \n      t^-(2k - D)  textif   D - d  k le D\n    endcases\n\nTraditionally, if textGALI_k(t) does not become less than the threshold until tmax the given orbit is said to be chaotic, otherwise it is regular.\n\nOur implementation is not based on the original paper, but rather in the method described in[Skokos2016b], which uses the product of the singular values of A, a matrix that has as columns the deviation vectors.\n\nPerformance Notes\n\nThis function uses a tangent_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nChaosTools.gali(tinteg, tmax, dt, threshold)\n\n[Skokos2007]: Skokos, C. H. et al., Physica D 231, pp 30‚Äì54 (2007)\n\n[Skokos2016b]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 5\n\n(section 5.3.1 and ref. [85] therein), Lecture Notes in Physics 915, Springer (2016)\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"","category":"page"},{"location":"chaos/chaos_detection/#Discrete-Example-1","page":"Detecting & Categorizing Chaos","title":"Discrete Example","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We will use 3 coupled standard maps as an example for a discrete system:","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems\nusing PyPlot\nM = 3; ks = 3ones(M); Œì = 0.1;\nstable = [œÄ, œÄ, œÄ, 0.01, 0, 0] .+ 0.1\nchaotic = rand(2M)\n\nds = Systems.coupledstandardmaps(M, stable; ks=ks, Œì = Œì)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"First, let's see the behavior of GALI for a stable orbit","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 100000)\n\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,1+M], alpha = 0.5,\nlabel=\"stable\",marker=\"o\", ms=1, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [4, 5, 6]\n    g, t = gali(ds, 1e5, k; threshold=1e-12)\n    lt = log10.(t); lg = log10.(g)\n    plot(lt, lg, label=\"GALI_$(k)\")\nend\nlt = 2:0.5:5.5\nplot(lt, -2(lt .- 3), label=\"slope -2\")\nplot(lt, -4(lt .- 3), label=\"slope -4\")\nplot(lt, -6(lt .- 3), label=\"slope -6\")\n\nxlim(2, 5.5)\nylim(-12, 2)\nlegend()\ntight_layout()\nsavefig(\"gali_discrete_stable.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_discrete_stable)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Now do the same for a chaotic orbit","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 100000, chaotic)\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,1+M], alpha = 0.5,\nlabel=\"chaotic\",marker=\"o\", ms=1, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nls = lyapunovs(ds, 100000; u0 = chaotic)\nfor k in [2,3,6]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = chaotic)\n    semilogy(t, exp.(-ex.*t), label=\"exp. k=$k\")\n    semilogy(t, g, label=\"GALI_$(k)\")\nend\nlegend()\nxlim(0,100)\nylim(1e-12, 1)\nsavefig(\"gali_discrete_chaos.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_discrete_chaos)","category":"page"},{"location":"chaos/chaos_detection/#Continuous-Example-1","page":"Detecting & Categorizing Chaos","title":"Continuous Example","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"As an example of a continuous system, let's see the henonheiles:","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems\nusing PyPlot, OrdinaryDiffEq\nsp = [0, .295456, .407308431, 0] # stable periodic orbit: 1D torus\nqp = [0, .483000, .278980390, 0] # quasiperiodic orbit: 2D torus\nch = [0, -0.25, 0.42081, 0]      # chaotic orbit\nds = Systems.henonheiles(sp)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"First, we see the behavior with a stable periodic orbit","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\nsubplot(1,2,1)\ndt = 1.0\n\ndiffeq = (abstol=1e-9, reltol=1e-9, alg = Tsit5(), maxiters = typemax(Int))\ntr = trajectory(ds, 10000.0; dt=dt, diffeq...)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"sp\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; dt = dt, diffeq...)\n    loglog(t, g, label=\"GALI_$(k)\")\n    if k < 4\n        loglog(t, 100 ./ t.^(k-1), label=\"slope -$(k-1)\")\n    else\n        loglog(t, 10000 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    end\nend\nylim(1e-12, 2)\nlegend();\nsavefig(\"gali_cont_stable.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_stable)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Next, let's see what happens with a quasi-periodic orbit. Don't forget to change the u0 arguments!","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\nsubplot(1,2,1)\ntr = trajectory(ds, 10000.0, qp; dt=dt, diffeq...)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"qp\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; u0 = qp, dt = dt, diffeq...)\n    loglog(t, g, label=\"GALI_$(k)\")\n    if k == 2\n        loglog(t, 1 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    else\n        loglog(t, 100 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    end\nend\nylim(1e-12, 2)\nlegend()\ntight_layout()\nsavefig(\"gali_cont_quasi.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_quasi)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Finally, here is GALI of a continuous system with a chaotic orbit","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 10000.0, ch; dt=dt, diffeq...)\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"ch\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nls = lyapunovs(ds, 5000.0; dt=dt, u0 = ch, diffeq...)\nfor k in [2,3,4]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = ch, dt = dt, diffeq...)\n    semilogy(t, exp.(-ex.*t), label=\"exp. k=$k\")\n    semilogy(t, g, label=\"GALI_$(k)\")\nend\nlegend()\nylim(1e-16, 1)\ntight_layout()\nsavefig(\"gali_cont_chaos.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_chaos)","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"As you can see, the results of both discrete and continuous systems match very well the theory described in gali.","category":"page"},{"location":"chaos/chaos_detection/#Using-GALI-1","page":"Detecting & Categorizing Chaos","title":"Using GALI","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"No-one in their right mind would try to fit power-laws in order to distinguish between chaotic and regular behavior, like the above examples. These were just proofs that the method works as expected in all cases.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The most common usage of textGALI_k is to define a (sufficiently) small amount of time and a (sufficiently) small threshold and see whether textGALI_k stays below it, for a (sufficiently) big k.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The following is an example of advanced usage (see Advanced documentation):","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems, PyPlot\n\nfunction main(k)\n# Measure of chaoticity: final time of gali_2\ndens = 201\nchaoticity = zeros(Int, dens, dens)\n\nŒ∏s = ps = range(0, stop = 2œÄ, length = dens+1)\nds = Systems.standardmap(k = k)\n\ntinteg = tangent_integrator(ds, 2)\n\nfor (i, Œ∏) ‚àà enumerate(Œ∏s[1:dens])\n    println(\"i = $(i)\")\n    for (j, p) ‚àà enumerate(ps[1:dens])\n\n        # new initial state is the system initial state\n        u0 = SVector{2}(Œ∏, p)\n        reinit!(tinteg, u0, orthonormal(2,2))\n\n        # Low-level call signature of gali:\n        #  gali(tinteg, tmax, dt, threshold)\n        chaoticity[i, j] = gali(tinteg, 500, 1, 1e-12)[2][end]\n    end\nend\nfigure()\npcolormesh(Œ∏s .- (Œ∏s[2] - Œ∏s[1])/2, ps .- (ps[2] - ps[1])/2,\nchaoticity')\ncolorbar()\nxlabel(\"\\$\\\\theta\\$\")\nylabel(\"\\$p\\$\")\nreturn\nend\n\nmain(0.9);","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: )","category":"page"},{"location":"chaos/chaos_detection/#Regular-orbits-in-the-Henon-Heiles-system-1","page":"Detecting & Categorizing Chaos","title":"Regular orbits in the Henon-Heiles system","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"In this example we use the poincaresos function to produce surfaces of section of the Systems.henonheiles system at different energies. At each energy gali is used to color-code each initial condition according to how chaotic/regular it is, i.e. how much time does it need to exceed the threshold of gali.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"<video width=\"100%\" height=\"auto\" controls> <source src=\"https://raw.githubusercontent.com/JuliaDynamics/JuliaDynamics/master/videos/chaos/galipsoshenonhelies.mp4?raw=true\" type=\"video/mp4\"> </video>","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"You can download the video using this link.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"You can find the script that produced this animation in DynamicalSystems/docs/coolanimations/gali_psos_henonhelies.jl.","category":"page"},{"location":"chaos/chaos_detection/#Predictability-of-a-chaotic-system-1","page":"Detecting & Categorizing Chaos","title":"Predictability of a chaotic system","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Even if a system is \"formally\" chaotic, it can still be in phases where it is very predictable, because the correlation coefficient between nearby trajectories vanishes very slowly with time. Wernecke, S√°ndor & Gros have developed an algorithm that allows one to classify a dynamical system to one of three categories: strongly chaotic, partially predictable chaos or regular (called laminar in their paper).","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We have implemented their algorithm in the function predictability. Note that we set up the implementation to always return regular behavior for negative Lyapunov exponent. You may want to override this for research purposes.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"predictability","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.predictability","page":"Detecting & Categorizing Chaos","title":"ChaosTools.predictability","text":"predictability(ds::DynamicalSystem; kwargs...) -> chaos_type, ŒΩ, C\n\nDetermine whether ds displays strongly chaotic, partially-predictable chaotic or regular behaviour, using the method by Wernecke et al. described in[Wernecke2017].\n\nReturn the type of the behavior, the cross-distance scaling coefficient ŒΩ and the correlation coefficient C. Typical values for ŒΩ, C and chaos_type are given in Table 2 of[Wernecke2017]:\n\nchaos_type ŒΩ C\n:SC 0 0\n:PPC 0 1\n:REG 1 1\n\nKeyword Arguments\n\nTtr = 200 : Extra \"transient\" time to evolve the system before sampling from  the trajectory. Should be Int for discrete systems.\nT_sample = 1e4 : Time to evolve the system for taking samples. Should be Int for discrete systems.\nn_samples = 500 : Number of samples to take for use in calculating statistics.\nŒª_max = lyapunov(ds, 5000) : Value to use for largest Lyapunov exponent for finding the Lyapunov prediction time. If it is less than zero a regular result is returned immediatelly.\nd_tol = 1e-3 : tolerance distance to use for calculating Lyapunov prediction time.\nT_multiplier = 10 : Multiplier from the Lyapunov prediction time to the evaluation time.\nT_max = Inf : Maximum time at which to evaluate trajectory distance. If the internally  computed evaluation time is larger than T_max, stop at T_max instead.\nŒ¥_range = 10.0 .^ (-9:-6) : Range of initial condition perturbation distances  to use to determine scaling ŒΩ.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nSamples points from a trajectory of the system to be used as initial conditions. Each of these initial conditions is randomly perturbed by a distance Œ¥, and the trajectories for both the original and perturbed initial conditions are computed to the 'evaluation time' T.\n\nThe average (over the samples) distance and cross-correlation coefficient of the state at time T is computed. This is repeated for a range of Œ¥ (defined by Œ¥_range), and linear regression is used to determine how the distance and cross-correlation scale with Œ¥, allowing for identification of chaos type.\n\nThe evaluation time T is calculated as T = T_multiplier*TŒª, where the Lyapunov prediction time TŒª = log(d_tol/Œ¥)/Œª_max. This may be very large if the Œª_max is small, e.g. when the system is regular, so this internally computed time T can be overridden by a smaller T_max set by the user.\n\nPerformance Notes\n\nFor continuous systems, it is likely that the maxiters used by the integrators needs to be increased, e.g. to 1e9. This is part of the diffeq kwargs. In addition, be aware that this function does a lot of internal computations. It is operating in a different speed than e.g. lyapunov.\n\n[Wernecke2017]: Wernecke, H., S√°ndor, B. & Gros, C. How to test for partially predictable chaos. Scientific Reports 7, (2017).\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Example-H√©non-Map-1","page":"Detecting & Categorizing Chaos","title":"Example H√©non Map","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We will create something similar to figure 2 of the paper, but for the H√©non map.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure()\nhe = Systems.henon()\nas = 0.8:0.01:1.225\nod = orbitdiagram(he, 1, 1, as; n = 2000, Ttr = 2000)\ncolors = Dict(:REG => \"b\", :PPC => \"g\", :SC => \"r\")\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    chaos_type, ŒΩ, C = predictability(he; T_max = 400000, Ttr = 2000)\n    scatter(a .* ones(length(od[i])), od[i], c = colors[chaos_type], s = 2,\n    alpha = 0.05)\nend\nxlabel(\"\\$a\\$\"); ylabel(\"\\$x\\$\")\ntitle(\"predictability of H√©non map\"); tight_layout()\nsavefig(\"partial_henon.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: partial_henon)","category":"page"},{"location":"chaos/chaos_detection/#The-0-1-test-for-chaos-1","page":"Detecting & Categorizing Chaos","title":"The 0-1 test for chaos","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The methods mentioned in this page so far require a DynamicalSystem instance. But of course this is not always the case. The so-called \"0 to 1\" test for chaos, by Gottwald & Melbourne, takes as an input a timeseries and outputs a boolean true if the timeseries is chaotic or false if it is not.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Notice that the method does have a lot of caveats, so you should read the review paper before using.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"testchaos01","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.testchaos01","page":"Detecting & Categorizing Chaos","title":"ChaosTools.testchaos01","text":"testchaos01(œÜ::Vector [, cs, N0]) -> chaotic?\n\nPerform the so called \"0-1\" test for chaos introduced by Gottwald and Melbourne[Gottwald2016] on the timeseries œÜ. Return true if œÜ is chaotic, false otherwise.\n\nDescription\n\nThis method tests if the given timeseries is chaotic or not by transforming it into a two-dimensional diffusive process. If the timeseries is chaotic, the mean square displacement of the process grows as sqrt(length(œÜ)), while it stays constant if the timeseries is regular. The implementation here computes K, the correlation coefficient (median of Kc for c ‚àà cs), and simply checks if K > 0.5.\n\nIf you want to access the various Kc you should call the method testchaos01(œÜ, c::Real, N0) which returns Kc.\n\ncs defaults to 3œÄ/5*rand(10) + œÄ/4 and N0, the length of the two-dimensional process, is N0 = length(œÜ)/10.\n\nNotice that for data sampled from continous dynamical systems, some care must be taken regarding the values of cs, see [1].\n\n[Gottwald2016]: Gottwald & Melbourne, ‚ÄúThe 0-1 test for chaos: A review‚Äù Lect. Notes Phys., vol. 915, pp. 221‚Äì247, 2016.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Expansion-entropy-1","page":"Detecting & Categorizing Chaos","title":"Expansion entropy","text":"","category":"section"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The expansion entropy is a quantity that is suggested by B. Hunt and E. Ott as a measure that can define chaos (so far no widely accepted definition of chaos exists). Positive expansion entropy means chaos.","category":"page"},{"location":"chaos/chaos_detection/#","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"expansionentropy\nboxregion\nexpansionentropy_sample\nexpansionentropy_batch","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy","text":"expansionentropy(ds::DynamicalSystem, sampler, restraining; kwargs...)\n\nCalculate the expansion entropy[Hunt2015] of ds, in the restraining region S defined by restraining, by estimating the slope of the biggest linear region of the curve log E_t0+T t0(f S) versus T (using linear_region). This is an approximation of the expansion entropy H_0, according to[Hunt2015].\n\nsampler is a 0-argument function that generates a random initial condition (a sample) of ds. restraining is a 1-argument function restraining(u) that given the state u it returns true if the state is inside the restraining region S.\n\nUse boxregion for an easy way to define sampler and restraining on a multidimension box.\n\nKeyword Arguments\n\nN = 1000 : Number of samples taken at each batch (same as N of [1]).\nsteps = 40 : The maximal steps for which the system will be run.\nTtr = 0 : Transient time to evolve each initial condition before starting to comute E. This is t0 of [1] and of the following notation.\nbatches = 100 : Number of batches to run the calculation, see below.\ndiffeq... : Other keywords are propagated to the solvers of DifferentialEquations.jl.\n\nDescription\n\nN samples are initialized and propagated forwards in time (along with their tangent space). At every time t in [t0+dt, t0+2dt, ... t0+steps*dt] we calculate H:\n\nHt = log E_t0+T t0(f S)\n\nwith\n\nE_t0+T t0(f S) = frac 1 N sum_i G(Df_t0+t t0(x_i))\n\n(using same notation as [Hunt2015]). In principle E is the average largest possible growth ratio within the restraining region (sampled by the initial conditions). The summation is only over x_i that stay inside the region S defined by the boolean function restraining. This process is done by the expansionentropy_sample function.\n\nThen, this is repeated for batches amount of times, as recommended in[Hunt2015]. From all these batches, the mean and std of H is computed at every time point. This is done by the expansionentropy_batch function. When plotted versus t, these create the curves and error bars of e.g. Figs 2, 3 of [1].\n\nThis function expansionentropy simply returns the slope of the biggest linear region of the curve H versus t, which approximates the expansion entropy H_0. It is therefore recommended to use expansionentropy_batch directly and evaluate the result yourself, as this step is known to be inaccurate for non-chaotic systems (where H fluctuates strongly around 0).\n\n[Hunt2015]: B. Hunt & E. Ott, ‚ÄòDefining Chaos‚Äô, Chaos 25.9 (2015)\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.boxregion","page":"Detecting & Categorizing Chaos","title":"ChaosTools.boxregion","text":"boxregion(as, bs) -> sampler, restraining\n\nDefine a box in mathbbR^d with edges the as and bs and then return two functions: sampler, which generates a random initial condition in that box and restraining that returns true if a given state is in the box.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy_sample","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy_sample","text":"expansionentropy_sample(ds, sampler, restraining; kwargs...)\n\nReturn times, H for one sample of ds (see expansionentropy). Accepts the same argumets as expansionentropy, besides batches.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy_batch","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy_batch","text":"expansionentropy_batch(ds, sampler, restraining; kwargs...)\n\nRun expansionentropy_sample batch times, and return times, mean(H), std(H) for all resulting H, see expansionentropy.\n\nAccepts the same arguments as expansionentropy.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#Numerical-Data-1","page":"Numerical Data","title":"Numerical Data","text":"","category":"section"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"info: Trajectory and Timeseries\nThe word \"timeseries\" can be confusing, because it can mean a univariate (also called scalar or one-dimensional) timeseries or a multivariate (also called multi-dimensional) timeseries. To resolve this confusion, in DynamicalSystems.jl we have the following convention: \"timeseries\" always refers to a one-dimensional vector of numbers, which exists with respect to some other one-dimensional vector of numbers that corresponds to a time-vector. On the other hand, the word \"trajectory\" is used to refer to a multi-dimensional timeseries, which is of course simply a group/set of one-dimensional timeseries. A trajectory is represented by a Dataset and is the return type of trajectory.","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Trajectories (and in general sets in state space) in DynamicalSystems.jl are represented by a structure called Dataset (while timeseries are standard Julia Vectors).","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Dataset","category":"page"},{"location":"embedding/dataset/#DelayEmbeddings.Dataset","page":"Numerical Data","title":"DelayEmbeddings.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}.\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges).\n\ndata[i] gives the ith datapoint (returns an SVector)\ndata[v1] will return a vector of datapoints\ndata[v1, :] using a Colon as a second index will return a Dataset of these points\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"In essence a Dataset is simply a wrapper for a Vector of SVectors. However, it is visually represented as a matrix, similarly to how numerical data would be printed on a spreadsheet (with time being the column direction). It also offers a lot more functionality than just pretty-printing. Besides the examples in the documentation string, you can also do:","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"using DynamicalSystems\nhen = Systems.henon()\ndata = trajectory(hen, 10000) # this returns a dataset\nfor point in data\n    # do stuff with each data-point\n    # (vector with as many elements as system dimension)\nend","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Most functions from DynamicalSystems.jl that manipulate and use data are expecting a Dataset. This allows us to define efficient methods that coordinate well with each other, like e.g. embed.","category":"page"},{"location":"embedding/dataset/#Dataset-Functions-1","page":"Numerical Data","title":"Dataset Functions","text":"","category":"section"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Functions that operate on datasets.","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"minima\nmaxima\nminmaxima\ncolumns","category":"page"},{"location":"embedding/dataset/#DelayEmbeddings.minima","page":"Numerical Data","title":"DelayEmbeddings.minima","text":"minima(dataset)\n\nReturn an SVector that contains the minimum elements of each timeseries of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.maxima","page":"Numerical Data","title":"DelayEmbeddings.maxima","text":"maxima(dataset)\n\nReturn an SVector that contains the maximum elements of each timeseries of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.minmaxima","page":"Numerical Data","title":"DelayEmbeddings.minmaxima","text":"minmaxima(dataset)\n\nReturn minima(dataset), maxima(dataset) without doing the computation twice.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.columns","page":"Numerical Data","title":"DelayEmbeddings.columns","text":"columns(dataset) -> x, y, z, ...\n\nReturn the individual columns of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#Dataset-I/O-1","page":"Numerical Data","title":"Dataset I/O","text":"","category":"section"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Input/output functionality for an AbstractDataset is already achieved using base Julia, specifically writedlm and readdlm. To write and read a dataset, simply do:","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"using DelimitedFiles\n\ndata = Dataset(rand(1000, 2))\n\n# I will write and read using delimiter ','\nwritedlm(\"data.txt\", data, ',')\n\n# Don't forget to convert the matrix to a Dataset when reading\ndata = Dataset(readdlm(\"data.txt\", ',', Float64))","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"","category":"page"},{"location":"embedding/dataset/#Neighborhoods-1","page":"Numerical Data","title":"Neighborhoods","text":"","category":"section"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"Combining the excellent performance of NearestNeighbors.jl with the AbstractDataset allows us to define a function that calculates a \"neighborhood\" of a given point, i.e. finds other points near it. The different \"types\" of the neighborhoods are subtypes of AbstractNeighborhood.","category":"page"},{"location":"embedding/dataset/#","page":"Numerical Data","title":"Numerical Data","text":"neighborhood\nAbstractNeighborhood","category":"page"},{"location":"embedding/dataset/#DelayEmbeddings.neighborhood","page":"Numerical Data","title":"DelayEmbeddings.neighborhood","text":"neighborhood(point, tree, ntype)\nneighborhood(point, tree, ntype, n::Int, w::Int = 1)\n\nReturn a vector of indices which are the neighborhood of point in some data, where the tree was created using tree = KDTree(data [, metric]). The ntype is the type of neighborhood and can be any subtype of AbstractNeighborhood.\n\nUse the second method when the point belongs in the data, i.e. point = data[n]. Then w stands for the Theiler window (positive integer). Only points that have index abs(i - n) ‚â• w are returned as a neighborhood, to exclude close temporal neighbors. The default w=1 is the case of excluding the point itself.\n\nReferences\n\nneighborhood simply interfaces the functions NearestNeighbors.knn and inrange from NearestNeighbors.jl by using the argument ntype.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.AbstractNeighborhood","page":"Numerical Data","title":"DelayEmbeddings.AbstractNeighborhood","text":"AbstractNeighborhood\n\nSupertype of methods for deciding the neighborhood of points for a given point.\n\nConcrete subtypes:\n\nFixedMassNeighborhood(K::Int) : The neighborhood of a point consists of the K nearest neighbors of the point.\nFixedSizeNeighborhood(Œµ::Real) : The neighborhood of a point consists of all neighbors that have distance < Œµ from the point.\n\nSee neighborhood for more.\n\n\n\n\n\n","category":"type"},{"location":"chaos/entropies/#Entropies-and-Dimensions-1","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"section"},{"location":"chaos/entropies/#Generalized-Entropy-1","page":"Entropies and Dimensions","title":"Generalized Entropy","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"In the study of dynamical systems there are many quantities that identify as \"entropy\". Notice that these quantities are not the more commonly known thermodynamic ones, used in Statistical Physics. Rather, they are more like the to the entropies of information theory, which represents information contained within a dataset, or information about the dimensional scaling of a dataset.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"One way of computing entropies in DynamicalSystems.jl is the \"generalized entropy\":","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"genentropy","category":"page"},{"location":"chaos/entropies/#ChaosTools.genentropy","page":"Entropies and Dimensions","title":"ChaosTools.genentropy","text":"genentropy(Œ±, Œµ::Real, dataset::AbstractDataset; base = Base.MathConstants.e)\n\nCompute the Œ± order generalized (R√©nyi) entropy[R√©nyi1960] of a dataset, by first partitioning it into boxes of length Œµ using non0hist.\n\ngenentropy(Œ±, Œµs::AbstractVector, dataset::AbstractDataset; base = Base.MathConstants.e)\n\nSame as [genentropy(Œ±, Œµ, dataset) for Œµ in Œµs].\n\ngenentropy(Œ±, p::AbstractArray; base = Base.MathConstants.e)\n\nCompute the entropy of an array of probabilities p, assuming that p is sum-normalized.\n\nOptionally use base for the logarithms.\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the R√©nyi entropy is\n\nH_alpha(p) = frac11-alpha log left(sum_i pi^alpharight)\n\nand generalizes other known entropies, like e.g. the information entropy (alpha = 1, see [Shannon1948]), the maximum entropy (alpha=0, also known as Hartley entropy), or the correlation entropy (alpha = 2, also known as collision entropy).\n\n[R√©nyi1960]: A. R√©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"Basically, given a Dataset you can partition it into boxes to calculate an entropy. See below for a detailed example.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"tip: Worried about memory overflow? Don't be!\nPartitioning the dataset (i.e. doing a histogram) is in general a costly operation that depends exponentially on the number of dimensions of the data and algebraically to the box size Œµ.However, in this specific case the partition process has some special aspects that can be taken advantage of, reducing tremendously the memory allocation and spent time!","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"The function used internally by genentropy is non0hist:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"non0hist\nbinhist","category":"page"},{"location":"chaos/entropies/#ChaosTools.non0hist","page":"Entropies and Dimensions","title":"ChaosTools.non0hist","text":"non0hist(Œµ, dataset::AbstractDataset) ‚Üí p\n\nPartition a dataset into tabulated intervals (boxes) of size Œµ and return the sum-normalized histogram in an unordered 1D form, discarding all zero elements and bin edge information.\n\nPerformances Notes\n\nThis method has a linearithmic time complexity (n log(n) for n = length(data)) and a linear space complexity (l for l = dimension(data)). This allows computation of histograms of high-dimensional datasets and with small box sizes Œµ without memory overflow and with maximum performance.\n\nUse binhist to retain bin edge information.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.binhist","page":"Entropies and Dimensions","title":"ChaosTools.binhist","text":"binhist(Œµ, data) ‚Üí p, bins\n\nDo the same as non0hist but also return the bin edge information.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#Generalized-Dimension-Estimation-1","page":"Entropies and Dimensions","title":"Generalized Dimension Estimation","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"There are numerous methods that one can use to calculate a so-called \"dimension\" of a dataset, like for example the Fractal dimension. This real number can offer a lot of information about the object that the dataset represents.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"Based on the definition of the generalized entropy, one can calculate an appropriate dimension, called generalized dimension:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"generalized_dim\nestimate_boxsizes","category":"page"},{"location":"chaos/entropies/#ChaosTools.generalized_dim","page":"Entropies and Dimensions","title":"ChaosTools.generalized_dim","text":"generalized_dim(Œ±, dataset [, sizes]) -> D_Œ±\n\nReturn the Œ± order generalized dimension of the dataset, by calculating the genentropy for each Œµ ‚àà sizes.\n\nDescription\n\nThe returned dimension is approximated by the (inverse) power law exponent of the scaling of the genentropy versus the box size Œµ, where Œµ ‚àà sizes.\n\nCalling this function performs a lot of automated steps:\n\nA vector of box sizes is decided by calling sizes = estimate_boxsizes(dataset), if sizes is not given.\nFor each element of sizes the appropriate entropy is calculated, through d = genentropy.(Œ±, sizes, dataset). Let x = -log.(sizes).\nThe curve d(x) is decomposed into linear regions, using linear_regions(x, d).\nThe biggest linear region is chosen, and a fit for the slope of that region is performed using the function linear_region. This slope is the return value of generalized_dim.\n\nBy doing these steps one by one yourself, you can adjust the keyword arguments given to each of these function calls, refining the accuracy of the result.\n\nThe following aliases are provided:\n\nŒ± = 0 : boxcounting_dim, capacity_dim\nŒ± = 1 : information_dim\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.estimate_boxsizes","page":"Entropies and Dimensions","title":"ChaosTools.estimate_boxsizes","text":"estimate_boxsizes(data::AbstractDataset; k::Int = 12, z = -1, w = 1)\n\nReturn k exponentially spaced values: 10 .^ range(lower+w, upper+z, length = k).\n\nlower is the magnitude of the minimum pair-wise distance between datapoints while upper is the magnitude of the maximum difference between greatest and smallest number among each timeseries.\n\n\"Magnitude\" here stands for order of magnitude, i.e. round(log10(x)).\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"danger: Be wary when using `generalized_dim`\nAs stated clearly by the documentation string, calling generalized_dim performs a lot of automated steps by calling other functions (see below) with default arguments. It is actually more like a convenient bundle than an actual function and therefore you should be careful when considering the validity of the returned number.","category":"page"},{"location":"chaos/entropies/#Methods-to-identify-linear-regions-1","page":"Entropies and Dimensions","title":"Methods to identify linear regions","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"linear_regions\nlinear_region\nlinreg","category":"page"},{"location":"chaos/entropies/#ChaosTools.linear_regions","page":"Entropies and Dimensions","title":"ChaosTools.linear_regions","text":"linear_regions(x, y; dxi::Int = 1, tol = 0.2) -> (lrs, tangents)\n\nIdentify regions where the curve y(x) is linear, by scanning the x-axis every dxi indices (e.g. at x[1] to x[5], x[5] to x[10], x[10] to x[15] and so on if dxi=5).\n\nIf the slope (calculated via linear regression) of a region of width dxi is approximatelly equal to that of the previous region, within tolerance tol, then these two regions belong to the same linear region.\n\nReturn the indices of x that correspond to linear regions, lrs, and the approximated tangents at each region. lrs is a vector of Int. Notice that tangents is not accurate: it is not recomputed at every step, but only when its error exceeds the tolerance tol! Use linear_region to obtain a correct estimate for the slope of the largest linear region.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.linear_region","page":"Entropies and Dimensions","title":"ChaosTools.linear_region","text":"linear_region(x, y; dxi::Int = 1, tol = 0.2) -> ([ind1, ind2], slope)\n\nCall linear_regions, identify the largest linear region and approximate the slope of the entire region using linreg. Return the indices where the region starts and stops (x[ind1:ind2]) as well as the approximated slope.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.linreg","page":"Entropies and Dimensions","title":"ChaosTools.linreg","text":"linreg(x, y) -> a, b\n\nPerform a linear regression to find the best coefficients so that the curve: y = a + b*x has the least squared error.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#Example-1","page":"Entropies and Dimensions","title":"Example","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"For an example of using entropies to compute the dimension of an attractor let's use everyone's favorite system:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"using DynamicalSystems, PyPlot\nlor = Systems.lorenz()","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"Our goal is to compute entropies for many different partition sizes Œµ, so let's get down to it:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"tr = trajectory(lor, 100.0; Ttr = 10.0)\n\nŒµœÇ = ‚ÑØ .^ (-3.5:0.5:3.5) # semi-random guess\nHs = genentropy.(1, ŒµœÇ, Ref(tr))","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"xs = @. -log(ŒµœÇ)\nfigure()\nplot(xs, Hs)\nylabel(\"\\$H_1\\$\")\nxlabel(\"\\$-\\\\log (\\\\epsilon)\\$\");\nsavefig(\"genentropy1.png\"); nothing # hide","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"(Image: )","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"The slope of the linear scaling region of the above plot is the generalized dimension (of order Œ± = 2) for the attractor of the Lorenz system.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"Given that we see the plot, we can estimate where the linear scaling region starts and ends. However, we can use the function linear_region to get an estimate of the result as well. First let's visualize what it does:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"lrs, slopes = linear_regions(xs, Hs, tol = 0.25)\n\nfigure()\nfor i in 1:length(lrs)-1\n    plot(xs[lrs[i]:lrs[i+1]], Hs[lrs[i]:lrs[i+1]], marker = \"o\")\nend\nylabel(\"\\$H_1\\$\")\nxlabel(\"\\$-\\\\log (\\\\epsilon)\\$\");\nsavefig(\"genentropy2.png\"); nothing # hide","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"(Image: )","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"The linear_region function  computes the slope of the largest region:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"linear_region(xs, Hs)[2]","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"This result is an approximation of the information dimension (because we used Œ± = 1) of the Lorenz attractor.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"The above pipeline is bundled in generalized_dim. For example, the dimension of the strange attractor of the Systems.henon map, following the above approach but taking automated steps, is:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"using DynamicalSystems\nhen = Systems.henon()\nts = trajectory(hen, 200000)\nD_hen = generalized_dim(1, ts)","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"As a side note, be sure that you have enough data points, otherwise the values you will get will never be correct, as is demonstrated by J.-P. Eckmann and D. Ruelle (see Physica D 56, pp 185-187 (1992)).","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#Correlation-sum-(Grassberger-Proccacia)-1","page":"Entropies and Dimensions","title":"Correlation sum (Grassberger-Proccacia)","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"kernelprob\ncorrelationsum\ngrassberger\ntakens_best_estimate","category":"page"},{"location":"chaos/entropies/#ChaosTools.kernelprob","page":"Entropies and Dimensions","title":"ChaosTools.kernelprob","text":"kernelprob(X, Œµ, norm = Euclidean()) ‚Üí p\n\nAssociate each point in X (Dataset or timesries) with a probability p using the \"kernel estimation\" (also called \"nearest neighbor kernel estimation\" and other names):\n\np_j = frac1Nsum_i=1^N I(X_i - X_j  epsilon)\n\nwhere N is its length and I gives 1 if the argument is true. Because p is further normalized, it can be used as an alternative for the genentropy function (using the second method).\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.correlationsum","page":"Entropies and Dimensions","title":"ChaosTools.correlationsum","text":"correlationsum(X, Œµ::Real; w = 1, norm = Euclidean()) ‚Üí C(Œµ)\n\nCalculate the correlation sum of X (Dataset or timeseries) for a given radius Œµ and norm, using the formula:\n\nC(epsilon) = frac2(N-w)(N-w-1)sum_i=1^Nsum_j=1+w+i^N I(X_i - X_j  epsilon)\n\nwhere N is its length and I gives 1 if the argument is true. w is the Theiler window, a correction to the correlation sum that skips points that are temporally close with each other, with the aim of removing spurious correlations.\n\nSee the book \"Nonlinear Time Series Analysis\", Ch. 6, for a discussion around w and choosing best values.\n\nSee grassberger for more. See also takens_best_estimate.\n\n\n\n\n\ncorrelationsum(X, Œµs::AbstractVector; kwargs...) ‚Üí Cs\n\nCalculate the correlation sum for every Œµ ‚àà Œµs using an optimized version.\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.grassberger","page":"Entropies and Dimensions","title":"ChaosTools.grassberger","text":"grassberger(data, Œµs = estimate_boxsizes(data); kwargs...) ‚Üí D_C\n\nUse the method of Grassberger and Proccacia[Grassberger1983], and the correction by Theiler[Theiler1986], to estimate the correlation dimension D_C of the given data.\n\nThis function does something extrely simple:\n\ncm = correlationsum(data, Œµs; kwargs...)\nreturn linear_region(log.(sizes), log(cm))[2]\n\ni.e. it calculates correlationsum for various radii and then tries to find a linear region in the plot of the log of the correlation sum versus log(Œµ). See generalized_dim for a more thorough explanation.\n\nSee also takens_best_estimate.\n\n[Grassberger1983]: Grassberger and Proccacia, Characterization of strange attractors, PRL 50 (1983)\n\n[Theiler1986]: Theiler, Spurious dimension from correlation algorithms applied to limited time-series data. Physical Review A, 34\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#ChaosTools.takens_best_estimate","page":"Entropies and Dimensions","title":"ChaosTools.takens_best_estimate","text":"takens_best_estimate(X, Œµmax, metric = Chebyshev(),Œµmin = 0) ‚Üí D_C, D_C_95u, D_C_95l\n\nUse the so-called \"Takens' best estimate\" [Takens1985][Theiler1988] method for estimating the correlation dimension D_C and the upper (D_C_95u) and lower (D_C_95l) confidence limit for the given dataset X.\n\nThe original formula is\n\nD_C approx fracC(epsilon_textmax)int_0^epsilon_textmax(C(epsilon)  epsilon)  depsilon\n\nwhere C is the correlationsum and epsilon_textmax is an upper cutoff. Here we use the later expression\n\nD_C approx - frac1etaquad eta = frac1(N-1)^*sum_i j^*log(X_i - X_j  epsilon_textmax)\n\nwhere the sum happens for all i j so that i  j and X_i - X_j  epsilon_textmax. In the above expression, the bias in the original paper has already been corrected, as suggested in [Borovkova1999].\n\nThe confidence limits are estimated from the log-likelihood function by finding the values of D_C where the function has fallen by 2 from its maximum, see e.g. [Barlow] chapter 5.3 Because the CLT does not apply (no independent measurements), the limits are not neccesarily symmetric.\n\nAccording to [Borovkova1999], introducing a lower cutoff Œµmin can make the algorithm more stable (no divergence), this option is given but defaults to zero.\n\nIf X comes from a delay coordinates embedding of a timseries x, a recommended value for epsilon_textmax is std(x)/4.\n\n[Takens1985]: Takens, On the numerical determination of the dimension of an attractor, in: B.H.W. Braaksma, B.L.J.F. Takens (Eds.), Dynamical Systems and Bifurcations, in: Lecture Notes in Mathematics, Springer, Berlin, 1985, pp. 99‚Äì106.\n\n[Theiler1988]: Theiler, Lacunarity in a best estimator of fractal dimension. Physics Letters A, 133(4‚Äì5)\n\n[Borovkova1999]: Borovkova et al., Consistency of the Takens estimator for the correlation dimension. The Annals of Applied Probability, 9, 05 1999.\n\n[Barlow]: Barlow, R., Statistics - A Guide to the Use of Statistical Methods in the Physical Sciences. Vol 29. John Wiley & Sons, 1993\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#Permutation-Entropy-1","page":"Entropies and Dimensions","title":"Permutation Entropy","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"The permutation entropy is introduced by C. Bandt and B. Pompe as a \"A Natural Complexity Measure for Timeseries\", which directly applies to arbitrary real-world data and is particularly useful in the presence of dynamical or observational noise.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"permentropy","category":"page"},{"location":"chaos/entropies/#ChaosTools.permentropy","page":"Entropies and Dimensions","title":"ChaosTools.permentropy","text":"permentropy(x::AbstractVector, order [, interval=1]; base = Base.MathConstants.e)\n\nCompute the permutation entropy[Brandt2002] of given order from the x timeseries.\n\nOptionally, interval can be specified to use x[t0:interval:t1] when calculating permutation of the sliding windows between t0 and t1 = t0 + interval * (order - 1).\n\nOptionally use base for the logarithms.\n\n[Bandt2002]: C. Bandt, & B. Pompe, Phys. Rev. Lett. 88 (17), pp 174102 (2002)\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"For example, we will compute and compare the lyapunov exponent of the logistic map with the order-6 permutation entropy, like in the original paper.","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"using DynamicalSystems, PyPlot\nds = Systems.logistic()\nrs = 3.5:0.001:4\nls = Float64[]; hs = Float64[]\nfor r in rs\n    ds.p[1] = r\n    push!(ls, lyapunov(ds, 100000))\n    # For 1D systems `trajectory` returns a vector\n    push!(hs, permentropy(trajectory(ds, 10000), 6))\nend\n\nf = figure(figsize = (10,6))\na1 = subplot(211)\nplot(rs, ls); ylim(-2, log(2)); ylabel(\"\\$\\\\lambda\\$\")\na1.axes.get_xaxis().set_ticklabels([])\nxlim(rs[1], rs[end]);\n\na2 = subplot(212)\nplot(rs, hs; color = \"C1\"); ylabel(\"\\$h_6\\$\")\nxlim(rs[1], rs[end]); xlabel(\"\\$r\\$\")\ntight_layout()\nsavefig(\"permentropy.png\"); nothing # hide","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"(Image: )","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"info: Permutation Entropy performance\nEven though the current implementation is fine and runs reasonably fast for moderate orders, it can get slow for high orders. Issue ChaosTools.jl#22 keeps track of this, and contains information on how to improve performance.","category":"page"},{"location":"chaos/entropies/#Kaplan-Yorke-Dimension-1","page":"Entropies and Dimensions","title":"Kaplan-Yorke Dimension","text":"","category":"section"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"kaplanyorke_dim","category":"page"},{"location":"chaos/entropies/#ChaosTools.kaplanyorke_dim","page":"Entropies and Dimensions","title":"ChaosTools.kaplanyorke_dim","text":"kaplanyorke_dim(Œªs::AbstractVector)\n\nCalculate the Kaplan-Yorke dimension, a.k.a. Lyapunov dimension[Kaplan1970].\n\nDescription\n\nThe Kaplan-Yorke dimension is simply the point where cumsum(Œªs) becomes zero (interpolated):\n\n D_KY = k + fracsum_i=1^k lambda_ilambda_k+1quad k = max_j left sum_i=1^j lambda_i  0 right\n\nIf the sum of the exponents never becomes negative the function will return the length of the input vector.\n\nUseful in combination with lyapunovs.\n\n[Kaplan1970]: J. Kaplan & J. Yorke, Chaotic behavior of multidimensional difference equations, Lecture Notes in Mathematics vol. 730, Springer (1979)\n\n\n\n\n\n","category":"function"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"Notice that calling this function requires you to pass the Lyapunov exponents in an ordered vector form (largest to smallest). Example:","category":"page"},{"location":"chaos/entropies/#","page":"Entropies and Dimensions","title":"Entropies and Dimensions","text":"using DynamicalSystems\nhen = Systems.henon()\nD_kp = kaplanyorke_dim(lyapunovs(hen, 100000))","category":"page"},{"location":"chaos/nlts/#Nonlinear-Timeseries-Analysis-1","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"section"},{"location":"chaos/nlts/#Numerical-Lyapunov-Exponent-1","page":"Nonlinear Timeseries Analysis","title":"Numerical Lyapunov Exponent","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Given any timeseries, one can first embed it using delay coordinates, and then calculate a maximum Lyapunov exponent for it. This is done with","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"numericallyapunov","category":"page"},{"location":"chaos/nlts/#ChaosTools.numericallyapunov","page":"Nonlinear Timeseries Analysis","title":"ChaosTools.numericallyapunov","text":"numericallyapunov(R::Dataset, ks;  refstates, w, distance, ntype)\n\nReturn E = [E(k) for k ‚àà ks], where E(k) is the average logarithmic distance between states of a neighborhood that are evolved in time for k steps (k must be integer). Typically R is the result of delay coordinates of a single timeseries.\n\nKeyword Arguments\n\nrefstates = 1:(length(R) - ks[end]) : Vector of indices that notes which states of the reconstruction should be used as \"reference states\", which means that the algorithm is applied for all state indices contained in refstates.\nw::Int = 1 : The Theiler window, which determines whether points are separated enough in time to be considered separate trajectories (see[Skokos2016] and neighborhood).\nntype::AbstractNeighborhood = FixedMassNeighborhood(1) : The method to be used when evaluating the neighborhood of each reference state. See AbstractNeighborhood or neighborhood for more info.\ndistance::Metric = Cityblock() : The distance function used in the logarithmic distance of nearby states. The allowed distances are Cityblock() and Euclidean(). See below for more info.\n\nDescription\n\nIf the dataset/reconstruction exhibits exponential divergence of nearby states, then it should clearly hold\n\nE(k) approx lambdacdot k cdot Delta t + E(0)\n\nfor a well defined region in the k axis, where lambda is the approximated maximum Lyapunov exponent. Delta t is the time between samples in the original timeseries. You can use linear_region with arguments (ks .* Œît, E) to identify the slope (= lambda) immediatelly, assuming you have choosen sufficiently good ks such that the linear scaling region is bigger than the saturated region.\n\nThe algorithm used in this function is due to Parlitz[Skokos2016], which itself expands upon Kantz [Kantz1994]. In sort, for each reference state a neighborhood is evaluated. Then, for each point in this neighborhood, the logarithmic distance between reference state and neighborhood state is calculated as the \"time\" index k increases. The average of the above over all neighborhood states over all reference states is the returned result.\n\nIf the Metric is Euclidean() then use the Euclidean distance of the full D-dimensional points (distance d_E in ref.[Skokos2016]). If however the Metric is Cityblock(), calculate the absolute distance of only the first elements of the m+k and n+k points of the reconstruction R (distance d_F in ref.[Skokos2016]).\n\n[Skokos2016]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 1 (section 1.3.2), Lecture Notes in Physics 915, Springer (2016)\n\n[Kantz1994]: Kantz, H., Phys. Lett. A 185, pp 77‚Äì87 (1994)\n\n\n\n\n\n","category":"function"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The function numericallyapunov has a total of 4 different approaches for the algorithmic process, by combining 2 types of distances with 2 types of neighborhoods.","category":"page"},{"location":"chaos/nlts/#Example-of-Numerical-Lyapunov-computation-1","page":"Nonlinear Timeseries Analysis","title":"Example of Numerical Lyapunov computation","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nds = Systems.henon()\ndata = trajectory(ds, 100000)\nx = data[:, 1] #fake measurements for the win!\n\nks = 1:20\n‚Ñú = 1:10000\nfig = figure(figsize=(10,6))\n\nfor (i, di) in enumerate([Euclidean(), Cityblock()])\n    subplot(1, 2, i)\n    ntype = FixedMassNeighborhood(2)\n    title(\"Distance: $(di)\", size = 18)\n    for D in [2, 4, 7]\n        R = embed(x, D, 1)\n        E = numericallyapunov(R, ks;\n        refstates = ‚Ñú, distance = di, ntype = ntype)\n        Œît = 1\n        Œª = linear_region(ks.*Œît, E)[2]\n        # gives the linear slope, i.e. the Lyapunov exponent\n        plot(ks .- 1, E .- E[1], label = \"D=$D, Œª=$(round(Œª, digits = 3))\")\n        legend()\n        tight_layout()\n    end\nend\nfig.savefig(\"numerlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#Bad-Time-axis-(ks)-length-1","page":"Nonlinear Timeseries Analysis","title":"Bad Time-axis (ks) length","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"danger: Large `ks`\nThis simply cannot be stressed enough! It is just too easy to overshoot the range at which the exponential expansion region is valid!","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Let's revisit the example of the previous section:","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ds = Systems.henon()\ndata = trajectory(ds, 100000)\nx = data[:, 1]\nlength(x)","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The timeseries of such length could be considered big. A time length of 100 seems very small. Yet it turns out it is way too big! The following","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks = 1:100\nR = embed(x, 2, 1)\nE = numericallyapunov(R, ks, ntype = FixedMassNeighborhood(2))\nfig = figure()\nplot(ks .- 1, E .- E[1])\ntitle(\"Lyappunov: $(linear_region(ks, E)[2])\")\nfig.savefig(\"badlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Notice that even though this value for the Lyapunov exponent is correct, it happened to be correct simply due to the jitter of the saturated region. Since the saturated region is much bigger than the linear scaling region, if it wasn't that jittery the function linear_region would not give the scaling of the linear region, but instead a slope near 0! (or if you were to give bigger tolerance as a keyword argument)","category":"page"},{"location":"chaos/nlts/#Case-of-a-Continuous-system-1","page":"Nonlinear Timeseries Analysis","title":"Case of a Continuous system","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The process for continuous systems works identically with discrete, but one must be a bit more thoughtful when choosing parameters. The following example helps the users get familiar with the process:","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nntype = FixedMassNeighborhood(5) #5 nearest neighbors of each state\n\nds = Systems.lorenz()\n# create a timeseries of 1 dimension\ndt = 0.05\nx = trajectory(ds, 1000.0; dt = dt)[:, 1]","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"We know that we have to use much bigger ks than 1:20, because this is a continuous case! (See reference given in numericallyapunovs)","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks1 = 0:200","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"and in fact it is even better to not increment the ks one by one but instead do","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks2 = 0:4:200","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Now we plot some example computations","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"figure()\nfor D in [4, 8], œÑ in [7, 15]\n    r = embed(x, D, œÑ)\n\n    # E1 = numericallyapunov(r, ks1; ntype = ntype)\n    # Œª1 = linear_region(ks1 .* dt, E1)[2]\n    E2 = numericallyapunov(r, ks2; ntype = ntype)\n    Œª2 = linear_region(ks2 .* dt, E2)[2]\n\n    # plot(ks1,E1.-E1[1], label = \"dense, D=$(D), œÑ=$(œÑ), Œª=$(round(Œª1, 3))\")\n    plot(ks2,E2.-E2[1], label = \"D=$(D), œÑ=$(œÑ), Œª=$(round(Œª2, digits = 3))\")\nend\n\nlegend()\nxlabel(\"k (0.05√ót)\")\nylabel(\"E - E(0)\")\ntitle(\"Continuous Reconstruction Lyapunov\")\ntight_layout()\nsavefig(\"continuousnumlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"As you can see, using œÑ = 15 is not a great choice! The estimates with œÑ = 7 though are very good (the actual value is around Œª ‚âà 0.89...).","category":"page"},{"location":"chaos/nlts/#Broomhead-King-Coordinates-1","page":"Nonlinear Timeseries Analysis","title":"Broomhead-King Coordinates","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"broomhead_king","category":"page"},{"location":"chaos/nlts/#ChaosTools.broomhead_king","page":"Nonlinear Timeseries Analysis","title":"ChaosTools.broomhead_king","text":"broomhead_king(s::AbstractVector, d::Int) -> U, S, Vtr\n\nReturn the Broomhead-King coordinates of a timeseries s by performing svd on the so-called trajectory matrix with dimension d.\n\nDescription\n\nBroomhead and King coordinates is an approach proposed in [Broomhead1987] that applies the Karhunen‚ÄìLo√®ve theorem to delay coordinates embedding with smallest possible delay.\n\nThe function performs singular value decomposition on the d-dimensional trajectory matrix X of s,\n\nX = frac1sqrtNleft(\nbeginarraycccc\nx_1  x_2  ldots  x_d \nx_2  x_3  ldots  x_d+1\nvdots  vdots  vdots  vdots \nx_N-d+1  x_N-d+2 ldots  x_N\nendarray\nright) = Ucdot S cdot V^tr\n\nwhere x = s - bars. The columns of U can then be used as a new coordinate system, and by considering the values of the singular values S you can decide how many columns of U are \"important\". See the documentation page for example application.\n\n[Broomhead1987]: D. S. Broomhead, R. Jones and G. P. King, J. Phys. A 20, 9, pp L563 (1987)\n\n\n\n\n\n","category":"function"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"This alternative/improvement of the traditional delay coordinates can be a very powerful tool. An example where it shines is noisy data where there is the effect of superficial dimensions due to noise.","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Take the following example where we produce noisy data from a system and then use Broomhead-King coordinates as an alternative to \"vanilla\" delay coordinates:","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nds = Systems.gissinger()\ndata = trajectory(ds, 1000.0, dt = 0.05)\nx = data[:, 1]\n\nL = length(x)\ns = x .+ 0.5rand(L) #add noise\n\nU, S = broomhead_king(s, 40)\nsummary(U)","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Now let's simply compare the above result with the one you get from doing a \"standard\" call to embed:","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"fig=figure(figsize= (10,6))\nsubplot(1,2,1)\nplot(U[:, 1], U[:, 2])\ntitle(\"Broomhead-King of s\")\n\nsubplot(1,2,2)\nR = embed(s, 2, 30)\nplot(columns(R)...; color = \"C3\")\ntitle(\"2D embedding of s\")\ntight_layout()\nfig.savefig(\"broomhead_king.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"we have used the same system as in the Delay Coordinates Embedding example, and picked the optimal delay time of œÑ = 30 (for same dt = 0.05). Regardless, the vanilla delay coordinates is much worse than the Broomhead-King coordinates.","category":"page"},{"location":"chaos/nlts/#Nearest-Neighbor-Prediction-1","page":"Nonlinear Timeseries Analysis","title":"Nearest Neighbor Prediction","text":"","category":"section"},{"location":"chaos/nlts/#","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Nearest neighbor timeseries prediction is a method commonly listed under nonlinear timeseries analysis. This is not part of DynamicalSystems.jl, because in JuliaDynamics we have a dedicated package for this, TimeseriesPrediction.jl.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: DynamicalSystems.jl logo: The Double Pendulum)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl is an award-winning Julia software library for the exploration of chaos and nonlinear dynamics. It is part of JuliaDynamics, an organization dedicated to creating high quality scientific software.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"To learn how to use this library please see Getting started below, and subsequently, the Contents page to get an overview of all offered functionality of DynamicalSystems.jl.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"tip: Latest news\nRework and improvement of optimal embedding dimension: optimal_traditional_de!","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"info: Star us on GitHub!\nIf you have found this library useful, please consider starring it on GitHub. This gives us an accurate lower bound of the (satisfied) user count.","category":"page"},{"location":"#Getting-started-1","page":"Introduction","title":"Getting started","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl is a collection of Julia packages bundled together under a single package DynamicalSystems. To install this bundle you can do:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using Pkg; Pkg.add(\"DynamicalSystems\")","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The individual packages that compose DynamicalSystems interact flawlessly with each other because of the following two structures:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The DynamicalSystem represents a dynamical system with known dynamic rule f. The system can be in discrete time (often called a map), vecu_n+1 = vecf(vecu_n p n), or in continuous time (often called an ordinary differential equation) fracdvecudt = vecf(vecu p t). In both cases u is the state of the dynamical system and p a parameter container. You should have a look at the page Dynamical System Definition for how to create this object. A list of several pre-defined systems exists in the Predefined Dynamical Systems page.\nNumerical data, that can represent measured experiments, or sampled trajectories of dynamical systems, are represented by Dataset, which is a container of equally-sized data points. Timeseries in DynamicalSystems.jl are represented by the already existing Vector type of the Julia language.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"These core structures DynamicalSystem, Dataset are used throughout the package to do useful calculations often used in the field of nonlinear dynamics and chaos. For example, using lyapunovs and DynamicalSystem gives you the Lyapunov exponents of a dynamical system with known equations of motion. Alternatively, by using numericallyapunov and Dataset you can approximate the maximum Lyapunov exponent of a measured trajectory.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"All things possible in DynamicalSystems.jl are listed in the Contents page.","category":"page"},{"location":"#Tutorials-1","page":"Introduction","title":"Tutorials","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Tutorials for DynamicalSystems.jl exist in the form of Jupyter notebooks.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"In addition, a full 2-hours YouTube tutorial is available below:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"<iframe width=\"560\" height=\"400\" src=\"https://www.youtube.com/embed/A8g9rdEfdNg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"info: Introductory textbooks\nOur library assumes some basic knowledge of nonlinear dynamics and complex systems.If you are new to the field but want to learn more, we can suggest the following textbooks as introductions:Chaos in Dynamical Systems - E. Ott\nNonlinear Time series Analysis - H. Kantz & T. Schreiber","category":"page"},{"location":"#Advanced-installation-1","page":"Introduction","title":"Advanced installation","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"For more advanced users, you can choose which packages to install and use at a high level. All packages depend on DelayEmbeddings which defines core numeric data structures and methods. For example RecurrenceAnalysis and TimeseriesPrediction depend only on DelayEmbeddings. Packages that require equations of motion also depend on DynamicalSystemsBase, like for example ChaosTools.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"If you only need functionality of a specific package you can install only that one, e.g. ]add RecurrenceAnalysis and only the minimum amount of requirements will be installed.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The documentation you are reading now was built with the following stable versions:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.status([\"DelayEmbeddings\", \"RecurrenceAnalysis\", \"DynamicalSystemsBase\", \"ChaosTools\"])","category":"page"},{"location":"#Our-Goals-1","page":"Introduction","title":"Our Goals","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl was created with three goals in mind. The first was to fill the missing gap of a software for nonlinear dynamics and chaos of the highest quality (none exist in any programming language). The second was to create a useful library where students and scientists from different fields may come and learn about methods of nonlinear dynamics and chaos.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The third was to fundamentally change the perception of the role of code in both scientific education as well as research. It is rarely the case that real, runnable code is shown in the classroom, because it is often long and messy. This is especially hurtful for nonlinear dynamics, a field where computer-assisted exploration is critical. But published work in this field fares even worse, with the overwhelming majority of published research not sharing the code used to create the paper. This makes reproducing these papers difficult, while some times straight-out impossible.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"To achieve these goals we made DynamicalSystems.jl so that it is:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Transparent: extra care is taken so that the source code of all functions is clear and easy to follow, while remaining as small and concise as possible.\nIntuitive: a software simple to use and understand makes experimentation easier.\nEasy to install, easy to extend: This makes contributions more likely, and can motivate researchers to implement their method here, instead of leaving it in a cryptic script stored in some data server, never-to-be-published with the paper.\nReliable: the algorithm implementations are tested extensively.\nWell-documented: all implemented algorithms provide a high-level scientific description of their functionality in their documentation string as well as references to scientific papers.\nGeneral: all algorithms work just as well with any system, whether it is a simple continuous chaotic system, like the Lorenz model, or a high dimensional discrete system like coupled standard maps.\nPerformant: written entirely in Julia, and taking advantage of some of the best packages within the language, DynamicalSystems.jl is really fast.","category":"page"},{"location":"#Citing-1","page":"Introduction","title":"Citing","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"There is a (small) paper associated with DynamicalSystems.jl. If we have helped you in research that led to a publication, please be kind enough to cite it, using the DOI 10.21105/joss.00598 or the following BiBTeX entry:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"@article{Datseris2018,\n  doi = {10.21105/joss.00598},\n  url = {https://doi.org/10.21105/joss.00598},\n  year  = {2018},\n  month = {mar},\n  volume = {3},\n  number = {23},\n  pages = {598},\n  author = {George Datseris},\n  title = {DynamicalSystems.jl: A Julia software library for chaos and nonlinear dynamics},\n  journal = {Journal of Open Source Software}\n}","category":"page"},{"location":"#Issues-with-Bounties-1","page":"Introduction","title":"Issues with Bounties","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Money that DynamicalSystems.jl obtains from awards, sponsors or donators are converted into bounties for GitHub issues. The full list of issues that have a bounty is available here.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"By solving these issues you not only contribute to open source, but you also get some pocket money to boot :)","category":"page"},{"location":"#Contacting-1","page":"Introduction","title":"Contacting","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Feel free to open issues on GitHub if you have questions and/or suggestions. You can also join our chatroom for discussions and/or questions about the packages of the JuliaDynamics organization! If you are using the Julia Slack workplace, please join the channel #dynamics-bridged.","category":"page"},{"location":"#Contributing-and-Donating-1","page":"Introduction","title":"Contributing & Donating","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"TL;DR: See \"good first issues\" or \"wanted features\".","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Be sure to visit the Contributor Guide page, because you can help make this package better without having to write a single line of code! Also, if you find this package helpful please consider staring it on GitHub! This gives us an accurate lower bound of users that this package has already helped!","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Finally, you can donate for the development of DynamicalSystems.jl. You can do that by adding bounties to existing issues on the GitHub repositories (you can open new issues as well). Every issue has an automatic way to create a bounty using Bountysource, see the first comment of each issue.","category":"page"},{"location":"#Maintainers-and-Contributors-1","page":"Introduction","title":"Maintainers and Contributors","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The DynamicalSystems.jl software is maintained by George Datseris, who is also curating and writing this documentation page.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The software code however is built from the contributions of several individuals. For an accurate list of the names as well as contributions of each one, please visit the GitHub's contributor list for the sub-packages of DynamicalSystems.jl, e.g. ChaosTools.jl.","category":"page"}]
}
