var documenterSearchIndex = {"docs":
[{"location":"contributors_guide/#Contributor-Guide","page":"Contributor Guide","title":"Contributor Guide","text":"","category":"section"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"*TL;DR: See \"good first issues\" or \"wanted features\". *","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"The ultimate goal for DynamicalSystems.jl is to be a useful library for scientists working on chaos, nonlinear dynamics and in general dynamical systems. We don't want to have \"just code\", but also detailed descriptions and references for as many methods as possible.","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"For this to be achieved, many of us should try to work together to improve the library!","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"If you want to help the cause, there are many ways to contribute to the DynamicalSystems.jl library:","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"Just use it. If you encountered unexpected behavior simply report it either on our gitter chatroom or using the DynamicalSystems.jl Issues page.\nSuggest methods that you think should be included in our library. This should be done by opening a new issue that describes the method, gives references to papers using the method and also justifies why the method should be included.\nContribute code by solving issues. The easiest issues to tackle are the ones with label \"good first issue\".\nContribute code by implementing new methods! That is the most awesome way to contribute! The individual packages that compose DynamicalSystems.jl have plenty of issues with the tag \"wanted feature\", which can get you started on a big contribution!\nContribute code by defining a new pre-defined dynamical system that you found useful.","category":"page"},{"location":"contributors_guide/#Contributing-Code","page":"Contributor Guide","title":"Contributing Code","text":"","category":"section"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"When contributing code, you should keep these things in mind:","category":"page"},{"location":"contributors_guide/","page":"Contributor Guide","title":"Contributor Guide","text":"In general, the speed of the implementation is important, but not as important as the clarity of the implementation. One of cornerstones of all of DynamicalSystems.jl is to have clear and readable source code. Fortunately, Julia allows you to have perfectly readable code but also super fast ;) If necessary add comments to the code, so that somebody that knows the method, can also understand the code immediately.\nFor the documentation strings of new methods and systems please follow the convention of the documentation strings of DynamicalSystems.jl. Specifically, the first section should describe the function in a couple of sentences, its positional arguments and its return value. The next section ## Keyword Arguments describes the keywords. The next section ## Description describes the algorithm in detail if need be.\nAlways have a reference to the original work that introduces the method or the system that you are using. You should put this reference to the main function's documentation string. See the existing documentation strings and do it in a similar manner.","category":"page"},{"location":"contents/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"The module DynamicalSystems re-exports all following functionality.","category":"page"},{"location":"contents/#Core-types","page":"Contents","title":"Core types","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Intuitive, consistent APIs for the definition of general dynamical systems under a unified struct DynamicalSystem. The following combinations are possible:\nContinuous or Discrete systems. Continuous systems use DifferentialEquations.jl for solving the ODE problem.\nIn-place or out-of-place (large versus small systems).\nAuto-differentiated or not (for the Jacobian function).","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Automatic \"completion\" of the dynamics of the system with numerically computed Jacobians, in case they are not provided by the user.\nRobust implementations of all kinds of integrators, that evolve the system, many states of the system, or even deviation vectors. See the Advanced documentation for this.\nLibrary of Predefined Dynamical Systems that have been used extensively in scientific research.\nUnified & dedicated interface for numerical data: Dataset.","category":"page"},{"location":"contents/#Entropies","page":"Contents","title":"Entropies","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"An interface to estimate Entropies & Probabilities from trajectories or state space sets.\nFast and cheap (memory-wise) method for computing histograms of large datasets: binhist.\nDozens of Probabilities Estimators for doing so, including standard binning, counting, permutations, nearest neighbor based, time-scale based, among others.","category":"page"},{"location":"contents/#[Delay-Coordinates-Embedding](@ref)","page":"Contents","title":"Delay Coordinates Embedding","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Performing delay coordinate embeddings and finding optimal parameters for doing so.","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Flexible, super-efficient and abstracted Delay Coordinates Embedding interface.\nSupports multiple dimensions and multiple timescales.","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Methods that estimate optimal embedding parameters: Traditional Optimal Embedding.\nUnified Optimal Embedding approach (advanced algorithms).\nFast calculation of mutual information: mutualinformation.\nUnified neighborhood interface.","category":"page"},{"location":"contents/#ChaosTools","page":"Contents","title":"ChaosTools","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"ChaosTools.jl is a collection of many algorithms for (chaotic or not) dynamical systems. All algorithms are independent of each other but they are also not expansive enough to be a standalone package.","category":"page"},{"location":"contents/#[Orbit-Diagrams-and-PSOS](@ref)","page":"Contents","title":"Orbit Diagrams & PSOS","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Orbit diagrams (aka bifurcation diagrams) of maps: orbitdiagram.\nPoincaré surfaces of section for continuous systems: poincaresos.\nAutomated production of orbit diagrams for continuous systems: produce_orbitdiagram.","category":"page"},{"location":"contents/#[Lyapunov-Exponents](@ref)","page":"Contents","title":"Lyapunov Exponents","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"The following treat systems where the equations of motion are known:","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Maximum Lyapunov exponent for both discrete and continuous systems: lyapunov.\nLyapunov spectrum for both discrete and continuous systems: lyapunovspectrum.","category":"page"},{"location":"contents/#[Detecting-and-Categorizing-Chaos](@ref)","page":"Contents","title":"Detecting & Categorizing Chaos","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"The Generalized Alignment Index: textGALI_k : gali.\nImplemented for both discrete and continuous systems.\nA test to categorize strong chaos, partially predictable chaos and regular behavior: predictability.\nImplemented for both discrete and continuous systems.\nThe 0-1 test for chaos: testchaos01\nThe expansion entropy: expansionentropy.","category":"page"},{"location":"contents/#[Fractal-Dimension](@ref)","page":"Contents","title":"Fractal Dimension","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Dozens of methods to calculate a fractal dimension\nEntropy-based\nCorrelation-sum-based\nKaplan-Yorke dimension: kaplanyorke_dim.","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"And, in order to automatically deduce dimensions, we also offer methods for:","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Partitioning a function y(x) vs. x into regions where it is approximated by a straight line, using a flexible algorithm with a lot of control over the outcome. See linear_regions.\nDetection of largest linear region of a function y(x) vs. x and extraction of the slope of this region.","category":"page"},{"location":"contents/#[Nonlinear-Timeseries-Analysis](@ref)","page":"Contents","title":"Nonlinear Timeseries Analysis","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Broomhead-King coordinates: broomhead_king.\nNumerically determining the maximum Lyapunov exponent of a (e.g. experimentally) measured timeseries: numericallyapunov.","category":"page"},{"location":"contents/#[Periodicity-and-Ergodicity](@ref)","page":"Contents","title":"Periodicity & Ergodicity","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"Numerical method to find unstable and stable fixed points of any order n of a discrete map (of any dimensionality): periodicorbits.\nConvenience functions for defining and realizing all possible combinations of mathbfLambda_k matrices required in the above method.\nEstimating the period of a timeseries: estimate_period.\nReturn and transit time statistics for a subset of the state space: mean_return_times, exit_entry_times.","category":"page"},{"location":"contents/#Recurrence-Analysis","page":"Contents","title":"Recurrence Analysis","text":"","category":"section"},{"location":"contents/","page":"Contents","title":"Contents","text":"RecurrenceAnalysis.jl offers tools to compute and analyze Recurrence Plots, a field called Recurrence Quantification Analysis.","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Recurrence Plots, with cross-recurrence and joint-recurrence.\nRecurrence Quantification Analysis (RQA):\nRecurrence rate, determinism, average/maximum diagonal length, divergence, laminarity, trend, entropy, trapping time, average/maximum vertical length.\nFine-tuning of the algorithms that compute the above (e.g. Theiler window and many more)\nWindowed RQA of the above","category":"page"},{"location":"chaos/orbitdiagram/#Orbit-Diagrams-and-PSOS","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"section"},{"location":"chaos/orbitdiagram/#Orbit-Diagrams-of-Maps","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams of Maps","text":"","category":"section"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"An orbit diagram (also called bifurcation diagram) is a way to visualize the asymptotic behavior of a map, when a parameter of the system is changed","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"orbitdiagram","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.orbitdiagram","page":"Orbit Diagrams & PSOS","title":"ChaosTools.orbitdiagram","text":"orbitdiagram(ds::DiscreteDynamicalSystem, i, p_index, pvalues; kwargs...)\n\nCompute the orbit diagram (also called bifurcation diagram) of the given system, saving the i variable(s) for parameter values pvalues. The p_index specifies which parameter of the equations of motion is to be changed.\n\ni can be Int or AbstractVector{Int}. If i is Int, returns a vector of vectors. Else it returns vectors of vectors of vectors. Each entry are the points at each parameter value.\n\nKeyword Arguments\n\nTtr::Int = 1000 : Transient steps; each orbit is evolved for Ttr first before saving output.\nn::Int = 100 : Amount of points to save for each initial condition.\ndt = 1 : Stepping time. Changing this will give you the orbit diagram of the dt order map.\nu0 = nothing : Specify an initial state. If nothing, the previous state after each parameter is used to seed the new initial condition at the new parameter (with the very first state being the system's state). This makes convergence to the attractor faster, necessitating smaller Ttr. Otherwise u0 can be a standard state, or a vector of states, so that a specific state is used for each parameter.\nulims = (-Inf, Inf) : only record system states within ulims (only valid if i isa Int).\n\nSee also poincaresos and produce_orbitdiagram.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"For example, let's compute the famous orbit diagram of the logistic map:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems\nusing PyPlot\n\nds = Systems.logistic()\ni = 1\npvalues = 3:0.001:4\nics = [rand() for m in 1:10]\nn = 2000\nTtr = 2000\np_index = 1\noutput = orbitdiagram(ds, i, p_index, pvalues; n = n, Ttr = Ttr)\n\nL = length(pvalues)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= pvalues[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfigure()\nPyPlot.title(\"total points: $(L*n)\")\nplot(x, y, ls = \"None\", ms = 0.5, color = \"black\", marker = \"o\", alpha = 0.05)\nxlim(pvalues[1], pvalues[end]); ylim(0,1)\nxlabel(\"\\$r\\$\"); ylabel(\"\\$x\\$\")\ntight_layout()\nsavefig(\"logostic_od.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Notice that if you are using PyPlot, the plotting process will be slow, since it is slow at plotting big numbers of points.","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"The function is not limited to 1D maps, and can be applied just as well to any discrete system.","category":"page"},{"location":"chaos/orbitdiagram/#Poincaré-Surface-of-Section-and-Map","page":"Orbit Diagrams & PSOS","title":"Poincaré Surface of Section and Map","text":"","category":"section"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Also called Poincaré map is a technique to reduce a continuous system into a discrete map with 1 less dimension. We are doing this using the function:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"poincaresos\npoincaremap","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.poincaresos","page":"Orbit Diagrams & PSOS","title":"ChaosTools.poincaresos","text":"poincaresos(ds::ContinuousDynamicalSystem, plane, tfinal = 1000.0; kwargs...)\n\nCalculate the Poincaré surface of section (also called Poincaré map)[Tabor1989] of the given system with the given plane. The system is evolved for total time of tfinal. Return a Dataset of the points that are on the surface of section.\n\nIf the state of the system is mathbfu = (u_1 ldots u_D) then the equation defining a hyperplane is\n\na_1u_1 + dots + a_Du_D = mathbfacdotmathbfu=b\n\nwhere mathbfa b are the parameters of the hyperplane.\n\nIn code, plane can be either:\n\nA Tuple{Int, <: Number}, like (j, r) : the plane is defined as when the j variable of the system equals the value r.\nA vector of length D+1. The first D elements of the vector correspond to mathbfa while the last element is b.\n\nThis function uses ds and higher order interpolation from DifferentialEquations.jl to create a high accuracy estimate of the section. See also produce_orbitdiagram.\n\nKeyword Arguments\n\ndirection = -1 : Only crossings with sign(direction) are considered to belong to the surface of section. Positive direction means going from less than b to greater than b.\nidxs = 1:dimension(ds) : Optionally you can choose which variables to save. Defaults to the entire state.\nTtr = 0.0 : Transient time to evolve the system before starting to compute the PSOS.\nu0 = get_state(ds) : Specify an initial state.\nwarning = true : Throw a warning if the Poincaré section was empty.\nrootkw = (xrtol = 1e-6, atol = 1e-6) : A NamedTuple of keyword arguments passed to find_zero from Roots.jl.\ndiffeq... : All other extra keyword arguments are propagated into init of DifferentialEquations.jl. See trajectory for examples.\n\nPerformance Notes\n\nThis function uses a standard integrator. For loops over initial conditions and/or parameters you should use the low level method that accepts an integrator and reinit! to new initial conditions. See the \"advanced documentation\" for more.\n\nThe low level call signature is:\n\npoincaresos(integ, planecrossing, tfinal, Ttr, idxs, rootkw)\n\nwhere\n\nplanecrossing = PlaneCrossing(plane, direction > 0)\n\nand idxs must be Int or SVector{Int}.\n\n[Tabor1989]: M. Tabor, Chaos and Integrability in Nonlinear Dynamics: An Introduction, §4.1, in pp. 118-126, New York: Wiley (1989)\n\n\n\n\n\npoincaresos(A::Dataset, plane; kwargs...)\n\nCalculate the Poincaré surface of section of the given dataset with the given plane by performing linear interpolation betweeen points that sandwich the hyperplane.\n\nArgument plane and keywords direction, warning, idxs are the same as above.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Here is an example of the Henon-Heiles system showing the mixed nature of the phase space","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems, PyPlot\n\nhh = Systems.henonheiles()\n\nplane = (1, 0.0)\nu0s = [[0.0, -0.25, 0.42081, 0.0],\n[0.0, -0.31596, 0.354461, 0.0591255],\n[0.0, 0.1, 0.5, 0.0],\n[0.0, -0.0910355, 0.459522, -0.173339],\n[0.0, -0.205144, 0.449328, -0.0162098]]\n\nfigure()\nfor u0 in u0s\n    psos = poincaresos(hh, plane, 20000.0; u0 = u0)\n    scatter(psos[:, 2], psos[:, 4], s = 2.0)\nend\nxlabel(\"\\$q_2\\$\"); ylabel(\"\\$p_2\\$\")\ntight_layout(pad = 0.3) # hide\nsavefig(\"hhpsos.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Here the surface of section was the (hyper-) plane that q_1 = 0. Some chaotic and regular orbits can be seen in the plot. You can tell the regular orbits apart because they look like a single connected curve. This is the result of cutting a 2-torus by a plane!","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Finally here is one more example with a more complex hyperplane:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"gis = Systems.gissinger([2.32865, 2.02514, 1.98312])\n\n# Define appropriate hyperplane for gissinger system\nconst ν = 0.1\nconst Γ = 0.9 # default parameters of the system\n\n# I want hyperperplane defined by these two points:\nNp(μ) = SVector{3}(sqrt(ν + Γ*sqrt(ν/μ)), -sqrt(μ + Γ*sqrt(μ/ν)), -sqrt(μ*ν))\nNm(μ) = SVector{3}(-sqrt(ν + Γ*sqrt(ν/μ)), sqrt(μ + Γ*sqrt(μ/ν)), -sqrt(μ*ν))\n\n# Create hyperplane passing through Np, Nm and 0:\nusing LinearAlgebra\ngis_plane(μ) = [cross(Np(μ), Nm(μ))..., 0]\n\nμ = 0.119\nset_parameter!(gis, 1, μ)\nfigure(figsize = (8,6))\npsos = poincaresos(gis, gis_plane(μ), 10000.0, Ttr = 200.0,)\nplot3D(columns(psos)..., marker = \"o\", ls = \"None\", ms = 2.0);\nxlabel(\"Q\"); ylabel(\"D\"); zlabel(\"V\");\ntight_layout(pad = 0.3) # hide\nsavefig(\"gispsos.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#Stroboscopic-Map","page":"Orbit Diagrams & PSOS","title":"Stroboscopic Map","text":"","category":"section"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"A special case of a PSOS is a stroboscopic map, which is defined for non-autonomous systems with periodic time dependence, like e.g. the Systems.duffing oscillator.","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"A \"cut\" through the phase-space can be produced at every period T = 2piomega. There is no reason to use poincaresos for this though, because you can simply use trajectory and get the solution with a certain time sampling rate. For example, this piece of code:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"using DynamicalSystems, Plots\n\nds = Systems.duffing(β = -1, ω = 1, f = 0.3) # non-autonomous chaotic system\n\nframes=120\na = trajectory(ds, 100000.0, dt = 2π/frames, Ttr=20π) # every period T = 2π/ω\n\norbit_length = div(size(a)[1], frames)\na = Matrix(a)\n\n@gif for i in 1:frames\n    orbit_points = i:frames:(orbit_length*frames)\n    scatter(a[orbit_points, 1], a[orbit_points, 2], markersize=1, html_output_format=:png,\n        leg=false, framestyle=:none, xlims=extrema(a[:,1]), ylims=extrema(a[:,2]))\nend","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"Produces this nice animation:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/orbitdiagram/#Producing-Orbit-Diagrams-for-Flows","page":"Orbit Diagrams & PSOS","title":"Producing Orbit Diagrams for Flows","text":"","category":"section"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"The orbitdiagram does not make much sense for continuous systems, besides the trivial case where the system is at a fixed point. In order for orbitdiagram to have meaning one must have a map.","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"If only there was a way to turn a continuous system into a map... OH WAIT! That is what poincaresos does! By performing successive surfaces of section at different parameter values, one can indeed \"produce\" an orbit diagram for a flow.","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"We have bundled this process in the following function:","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"produce_orbitdiagram","category":"page"},{"location":"chaos/orbitdiagram/#ChaosTools.produce_orbitdiagram","page":"Orbit Diagrams & PSOS","title":"ChaosTools.produce_orbitdiagram","text":"produce_orbitdiagram(ds::ContinuousDynamicalSystem, plane, i::Int,\n                     p_index, pvalues; kwargs...)\n\nProduce an orbit diagram (also called bifurcation diagram) for the i variable(s) of the given continuous system by computing Poincaré surfaces of section using plane for the given parameter values (see poincaresos).\n\ni can be Int or AbstractVector{Int}. If i is Int, returns a vector of vectors. Else it returns a vector of vectors of vectors. Each entry are the points at each parameter value.\n\nKeyword Arguments\n\nprintparams::Bool = false : Whether to print the parameter used during computation in order to keep track of running time.\ndirection, warning, Ttr, rootkw, diffeq... : Propagated into poincaresos.\nu0 = nothing : Specify an initial state. If nothing, the previous state after each parameter is used to seed the new initial condition at the new parameter (with the very first state being the system's state). This makes convergence to the attractor faster, necessitating smaller Ttr. Otherwise u0 can be a standard state, or a vector of states, so that a specific state is used for each parameter.\n\nDescription\n\nFor each parameter, a PSOS reduces the system from a flow to a map. This then allows the formal computation of an \"orbit diagram\" for the i variable of the system, just like it is done in orbitdiagram.\n\nThe parameter change is done as p[p_index] = value taking values from pvalues and thus you must use a parameter container that supports this (either Array, LMArray, dictionary or other).\n\nSee also poincaresos, orbitdiagram.\n\n\n\n\n\n","category":"function"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"For example, we will calculate the orbit diagram of the Shinriki oscillator, a continuous system that undergoes a period doubling route to chaos, much like the logistic map!","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"ds = Systems.shinriki([-2, 0, 0.2])\n\npvalues = range(19, stop = 22, length = 401)\ni = 1\nplane = (2, 0.0)\ntf = 200.0\np_index = 1\n\noutput = produce_orbitdiagram(ds, plane, i, p_index, pvalues;\n                              tfinal = tf, Ttr = 200.0)\n\nfigure()\nfor (j, p) in enumerate(pvalues)\n    plot(fill(p, length(output[j])), output[j], lw = 0,\n    marker = \"o\", ms = 0.2, color = \"black\")\nend\nxlabel(\"\\$R_1\\$\"); ylabel(\"\\$V_1\\$\")\ntight_layout(pad = 0.3) # hide\nsavefig(\"shinriki.png\"); nothing # hide","category":"page"},{"location":"chaos/orbitdiagram/","page":"Orbit Diagrams & PSOS","title":"Orbit Diagrams & PSOS","text":"(Image: )","category":"page"},{"location":"chaos/periodicity/#Periodicity-and-Ergodicity","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"In this page we describe methods related to the periodic behavior of dynamical systems or univariate timeseries, or related to the ergodic property of chaotic sets.","category":"page"},{"location":"chaos/periodicity/#Stable-and-Unstable-Periodic-Orbits-of-Maps","page":"Periodicity & Ergodicity","title":"Stable and Unstable Periodic Orbits of Maps","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Chaotic behavior of low dimensional dynamical systems is affected by the position and the stability properties of the periodic orbits of a dynamical system.","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Finding unstable (or stable) periodic orbits of a discrete mapping analytically rapidly becomes impossible for higher orders of fixed points. Fortunately there is a numeric algorithm due to Schmelcher & Diakonos which allows such a computation. Notice that even though the algorithm can find stable fixed points, it is mainly aimed at unstable ones.","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"The functions periodicorbits and lambdamatrix implement the algorithm:","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"periodicorbits\nlambdamatrix\nlambdaperms","category":"page"},{"location":"chaos/periodicity/#ChaosTools.periodicorbits","page":"Periodicity & Ergodicity","title":"ChaosTools.periodicorbits","text":"periodicorbits(ds::DiscreteDynamicalSystem,\n               o, ics [, λs, indss, singss]; kwargs...) -> FP\n\nFind fixed points FP of order o for the map ds using the algorithm due to Schmelcher & Diakonos[Schmelcher1997]. ics is a collection of initial conditions (container of vectors) to be evolved.\n\nOptional Arguments\n\nThe optional arguments λs, indss, singss must be containers of appropriate values, besides λs which can also be a number. The elements of those containers are passed to: lambdamatrix(λ, inds, sings), which creates the appropriate mathbfLambda_k matrix. If these arguments are not given, a random permutation will be chosen for them, with λ=0.001.\n\nKeyword Arguments\n\nmaxiters::Int = 100000 : Maximum amount of iterations an i.c. will be iterated  before claiming it has not converged.\ndisttol = 1e-10 : Distance tolerance. If the 2-norm of a previous state with  the next one is ≤ disttol then it has converged to a fixed point.\ninftol = 10.0 : If a state reaches norm(state) ≥ inftol it is assumed that  it has escaped to infinity (and is thus abandoned).\nroundtol::Int = 4 : The found fixed points are rounded  to roundtol digits before pushed into the list of returned fixed points FP,  if they are not already contained in FP.  This is done so that FP doesn't contain duplicate fixed points (notice  that this has nothing to do with disttol). Turn this to typemax(Int)  to get the full precision of the algorithm.\n\nDescription\n\nThe algorithm used can detect periodic orbits by turning fixed points of the original map ds to stable ones, through the transformation\n\nmathbfx_n+1 = mathbfx_n +\nmathbfLambda_kleft(f^(o)(mathbfx_n) - mathbfx_nright)\n\nwith f = eom. The index k counts the various possible mathbfLambda_k.\n\nPerformance Notes\n\nAll initial conditions are evolved for all mathbfLambda_k which can very quickly lead to long computation times.\n\n[Schmelcher1997]: P. Schmelcher & F. K. Diakonos, Phys. Rev. Lett. 78, pp 4733 (1997)\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.lambdamatrix","page":"Periodicity & Ergodicity","title":"ChaosTools.lambdamatrix","text":"lambdamatrix(λ, inds::Vector{Int}, sings) -> Λk\n\nReturn the matrix mathbfLambda_k used to create a new dynamical system with some unstable fixed points turned to stable in the function periodicorbits.\n\nArguments\n\nλ<:Real : the multiplier of the C_k matrix, with 0<λ<1.\ninds::Vector{Int} : The ith entry of this vector gives the row of the nonzero element of the ith column of C_k.\nsings::Vector{<:Real} : The element of the ith column of C_k is +1 if signs[i] > 0 and -1 otherwise (sings can also be Bool vector).\n\nCalling lambdamatrix(λ, D::Int) creates a random mathbfLambda_k by randomly generating an inds and a signs from all possible combinations. The collections of all these combinations can be obtained from the function lambdaperms.\n\nDescription\n\nEach element of inds must be unique such that the resulting matrix is orthogonal and represents the group of special reflections and permutations.\n\nDeciding the appropriate values for λ, inds, sings is not trivial. However, in ref.[Pingel2000] there is a lot of information that can help with that decision. Also, by appropriately choosing various values for λ, one can sort periodic orbits from e.g. least unstable to most unstable, see[Diakonos1998] for details.\n\n[Pingel2000]: D. Pingel et al., Phys. Rev. E 62, pp 2119 (2000)\n\n[Diakonos1998]: F. K. Diakonos et al., Phys. Rev. Lett. 81, pp 4349 (1998)\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.lambdaperms","page":"Periodicity & Ergodicity","title":"ChaosTools.lambdaperms","text":"lambdaperms(D) -> indperms, singperms\n\nReturn two collections that each contain all possible combinations of indices (total of D) and signs (total of 2^D) for dimension D (see lambdamatrix).\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"","category":"page"},{"location":"chaos/periodicity/#Standard-Map-example","page":"Periodicity & Ergodicity","title":"Standard Map example","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"For example, let's find the fixed points of the Systems.standardmap of order 2, 3, 4, 5, 6 and 8. We will use all permutations for the signs but only one for the inds. We will also only use one λ value, and a 21×21 density of initial conditions.","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"First, initialize everything","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"using DynamicalSystems, PyPlot, StaticArrays\n\nds = Systems.standardmap()\nxs = range(0, stop = 2π, length = 21); ys = copy(xs)\nics = [SVector{2}(x,y) for x in xs for y in ys]\n\n# All permutations of [±1, ±1]:\nsingss = lambdaperms(2)[2] # second entry are the signs\n\n# I know from personal research I only need this `inds`:\nindss = [[1,2]] # <- must be container of vectors!!!\n\nλs = 0.005 # <- only this allowed to not be vector (could also be vector)\n\norders = [2, 3, 4, 5, 6, 8]\nALLFP = Dataset{2, Float64}[];","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Then, do the necessary computations for all orders","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"for o in orders\n    FP = periodicorbits(ds, o, ics, λs, indss, singss)\n    push!(ALLFP, FP)\nend","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Plot the phase space of the standard map","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"iters = 1000\ndataset = trajectory(ds, iters)\nfor x in xs\n    for y in ys\n        append!(dataset, trajectory(ds, iters, SVector{2}(x, y)))\n    end\nend\nfigure(figsize = (12,12))\nm = Matrix(dataset)\nPyPlot.scatter(view(m, :, 1), view(m, :, 2), s= 1, color = \"black\")\nPyPlot.xlim(xs[1], xs[end])\nPyPlot.ylim(ys[1], ys[end]);","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"and finally, plot the fixed points","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"markers = [\"D\", \"^\", \"s\", \"p\", \"h\", \"8\"]\ncolors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"grey\"]\n\nfor i in 1:6\n    FP = ALLFP[i]\n    o = orders[i]\n    PyPlot.plot(columns(FP)...,\n    marker=markers[i], color = colors[i], markersize=10.0 + (8-o), linewidth=0.0,\n    label = \"order $o\", markeredgecolor = \"yellow\", markeredgewidth = 0.5)\nend\nlegend(loc=\"upper right\", framealpha=0.9)\nxlabel(\"\\$\\\\theta\\$\")\nylabel(\"\\$p\\$\")\nsavefig(\"fixedpoints.png\"); nothing # hide","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"(Image: Fixed points of the standard map)","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"You can confirm for yourself that this is correct, for many reasons:","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"It is the same fig. 12 of this publication.\nFixed points of order n are also fixed points of order 2n 3n 4n \nBesides fixed points of previous orders, original fixed points of order n come in (possible multiples of) 2n-sized pairs (see e.g. order 5). This is a direct consequence of the Poincaré–Birkhoff theorem.","category":"page"},{"location":"chaos/periodicity/#Estimating-the-Period","page":"Periodicity & Ergodicity","title":"Estimating the Period","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"The function estimate_period from ChaosTools offers ways for estimating the period (either exact for periodic timeseries, or approximate for near-periodic ones) of a given timeseries. We offer five methods to estimate periods, some of which work on evenly sampled data only, and others which accept any data. The figure below summarizes this: (Image: )","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period","category":"page"},{"location":"chaos/periodicity/#ChaosTools.estimate_period","page":"Periodicity & Ergodicity","title":"ChaosTools.estimate_period","text":"estimate_period(v::Vector, method, t=0:length(v)-1; kwargs...)\n\nEstimate the period of the signal v, with accompanying time vector t, using the given method.\n\nIf t is an AbstractArray, then it is iterated through to ensure that it's evenly sampled (if necessary for the algorithm).  To avoid this, you can pass any AbstractRange, like a UnitRange or a LinRange, which are defined to be evenly sampled.\n\nMethods requiring evenly sampled data\n\nThese methods are faster, but some are error-prone.\n\n:periodogram or :pg: Use the fast Fourier transform to compute a  periodogram (power-spectrum) of the given data.  Data must be evenly sampled.\n:multitaper or mt: The multitaper method reduces estimation bias by using multiple independent estimates from the same sample. Data tapers are then windowed and the power spectra are obtained.  Available keywords follow: nw is the time-bandwidth product, and ntapers is the number of tapers. If window is not specified, the signal is tapered with ntapers discrete prolate spheroidal sequences with time-bandwidth product nw. Each sequence is equally weighted; adaptive multitaper is not (yet) supported. If window is specified, each column is applied as a taper. The sum of periodograms is normalized by the total sum of squares of window.\n:autocorrelation or :ac: Use the autocorrelation function (AC). The value where the AC first comes back close to 1 is the period of the signal. The keyword L = length(v)÷10 denotes the length of the AC (thus, given the default setting, this method will fail if there less than 10 periods in the signal). The keyword ϵ = 0.2 (\\epsilon) means that 1-ϵ counts as \"1\" for the AC.\n\nMethods not requiring evenly sampled data\n\nThese methods tend to be slow, but versatile and low-error.\n\n:lombscargle or :ls: Use the Lomb-Scargle algorithm to compute a periodogram.  The advantage of the Lomb-Scargle method is that it does not require an equally sampled dataset and performs well on undersampled datasets. Constraints have been set on the period, since Lomb-Scargle tends to have false peaks at very low frequencies.  That being said, it's a very flexible method.  It is extremely customizable, and the keyword arguments that can be passed to it are given in the documentation.\n:zerocrossing or :zc: Find the zero crossings of the data, and use the average difference between zero crossings as the period.  This is a naïve implementation, with only linear interpolation; however, it's useful as a sanity check.  The keyword line controls where the \"crossing point\" is. It deffaults to mean(v).\n\nFor more information on the periodogram methods, see the documentation of DSP.jl and LombScargle.jl.\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#Example","page":"Periodicity & Ergodicity","title":"Example","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Here we will use a modified FitzHugh-Nagumo system that results in periodic behavior, and then try to estimate its period. First, let's see the trajectory:","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"using DynamicalSystems, PyPlot\n\nfunction FHN(u, p, t)\n    e, b, g = p\n    v, w = u\n    dv = min(max(-2 - v, v), 2 - v) - w\n    dw = e*(v - g*w + b)\n    return SVector(dv, dw)\nend\n\ng, e, b  = 0.8, 0.04, 0.0\np0 = [e, b, g]\n\nfhn = ContinuousDynamicalSystem(FHN, SVector(-2, -0.6667), p0)\nT, dt = 1000.0, 0.1\nv = trajectory(fhn, T; dt = dt)[:, 1]\nt = 0:dt:T\n\nfigure()\nplot(0:dt:T, v)\nsavefig(\"fhn_trajectory.png\"); nothing # hide","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"(Image: A periodic trajectory)","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"Examining the figure, one can see that the period of the system is around 91 time units. To estimate it numerically let's use some of the methods:","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :autocorrelation, t)","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :periodogram, t)","category":"page"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"estimate_period(v, :zerocrossing, t)","category":"page"},{"location":"chaos/periodicity/#Return-time-statistics","page":"Periodicity & Ergodicity","title":"Return time statistics","text":"","category":"section"},{"location":"chaos/periodicity/","page":"Periodicity & Ergodicity","title":"Periodicity & Ergodicity","text":"mean_return_times\nexit_entry_times","category":"page"},{"location":"chaos/periodicity/#ChaosTools.mean_return_times","page":"Periodicity & Ergodicity","title":"ChaosTools.mean_return_times","text":"mean_return_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → τ, c\n\nReturn the mean return times to subsets of the state space of ds defined by u₀, εs as well as the amount of returns c for each subset. The ds is evolved for a maximum of T time. This function behaves similarly to exit_entry_times and thus see that one for the meaning of u₀ and εs.\n\nThis function supports both discrete and continuous systems, however the optimizations done in discrete systems (where all nested ε-sets are checked at the same time), are not done here yet, which leads to disproportionally lower performance since each ε-related set is checked individually from start.\n\nContinuous systems allow for the following keywords:\n\ni=10 How many points to interpolate the trajectory in-between steps to find candidate crossing regions.\nm=10.0 A multiplier. If the trajectory is at least m*ε distance away from u0, the algorithm that checks for crossings of the ε-set is not initiated.\n\nFor continuous systems T, i, m can be vectors with same size as εs, to help increase accuracy of small ε.\n\n\n\n\n\n","category":"function"},{"location":"chaos/periodicity/#ChaosTools.exit_entry_times","page":"Periodicity & Ergodicity","title":"ChaosTools.exit_entry_times","text":"exit_entry_times(dds, u₀, εs, T) → exits, entries\n\nCollect exit and entry times for a ball/box centered at u₀ with radii εs (see below), in the state space of the given discrete dynamical system (function not yet available for continuous systems). Return the exit and (re-)entry return times to the set(s), where each of these is a vector containing all collected times for the respective ε-radius set, for ε ∈ εs.\n\nUse transit_return(exits, entries) to transform the output into transit and return times, and see also mean_return_times for both continuous and discrete systems.\n\nDescription\n\nTransit time statistics are important for the transport properties of dynamical systems[Meiss1997] and can even be connected with the fractal dimension of chaotic sets[Boev2014].\n\nThe current algorithm collects exit and re-entry times to given sets in the state space, which are centered at u₀ (algorithm always starts at u₀ and the initial state of ds is irrelevant). εs is always a Vector.\n\nThe sets around u₀ are nested hyper-spheres of radius ε ∈ εs, if each entry of εs is a real number. The sets can also be hyper-rectangles (boxes), if each entry of εs is a vector itself. Then, the i-th box is defined by the space covered by u0 .± εs[i] (thus the actual box size is 2εs[i]!).\n\nThe reason to input multiple εs at once is purely for performance.\n\nFor discrete systems, exit time is recorded immediatelly after exitting of the set, and re-entry is recorded immediatelly on re-entry. This means that if an orbit needs 1 step to leave the set and then it re-enters immediatelly on the next step, the return time is 1. For continuous systems high-order interpolation is done to accurately record the time of exactly crossing the ε-ball/box.\n\n[Meiss1997]: Meiss, J. D. Average exit time for volume-preserving maps, Chaos (1997)](https://doi.org/10.1063/1.166245)\n\n[Boev2014]: Boev, Vadivasova, & Anishchenko, Poincaré recurrence statistics as an indicator of chaos synchronization, Chaos (2014)](https://doi.org/10.1063/1.4873721)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#Predefined-Dynamical-Systems","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"","category":"section"},{"location":"ds/predefined/","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"Predefined systems exist in the Systems submodule in the form of functions that return a DynamicalSystem. They are accessed like:","category":"page"},{"location":"ds/predefined/","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"using DynamicalSystems # or DynamicalSystemsBase\nds = Systems.lorenz(ρ = 32.0)","category":"page"},{"location":"ds/predefined/","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"So far, the predefined systems that exist in the Systems sub-module are:","category":"page"},{"location":"ds/predefined/","page":"Predefined Dynamical Systems","title":"Predefined Dynamical Systems","text":"Modules = [Systems]\nOrder   = [:function]","category":"page"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.antidots","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.antidots","text":"antidots([u]; B = 1.0, d0 = 0.3, c = 0.2)\n\nAn antidot \"superlattice\" is a Hamiltonian system that corresponds to a smoothened periodic Sinai billiard with disk diameter d0 and smooth factor c [1].\n\nThis version is the two dimensional classical form of the system, with quadratic dynamical rule and a perpendicular magnetic field. Notice that the dynamical rule is with respect to the velocity instead of momentum, i.e.:\n\nbeginaligned\ndotx = v_x \ndoty = v_y \ndotv_x = B*v_y - U_x \ndotv_y = -B*v_x - U_X \nendaligned\n\nwith U the potential energy:\n\nU = left(tfrac1c^4right) lefttfracd_02 + c - r_aright^4\n\nif r_a = sqrt(x mod 1)^2 + (y mod 1)^2  fracd_02 + c and 0 otherwise. I.e. the potential is periodic with period 1 in both x y and normalized such that for energy value of 1 it is a circle of diameter d0. The magnetic field is also normalized such that for value B=1 the cyclotron diameter is 1.\n\n[1] : G. Datseris et al, New Journal of Physics 2019\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.arnoldcat","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.arnoldcat","text":"arnoldcat(u0 = [0.001245, 0.00875])\n\nf(xy) = (2x+yx+y) mod 1\n\nArnold's cat map. A chaotic map from the torus into itself, discovered by Vladimir Arnold in the 1960s. [1]\n\n[1] : Arnol'd, V. I., & Avez, A. (1968). Ergodic problems of classical mechanics.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.chua","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.chua","text":"chua(u0 = [0.7, 0.0, 0.0]; a = 15.6, b = 25.58, m0 = -8/7, m1 = -5/7)\n\nbeginaligned\ndotx = alpha (y - h(x))\ndoty = x - y+z \ndotz = beta y\nendaligned\n\nwhere h(x) is defined by\n\nh(x) = m_1 x + frac 1 2 (m_0 - m_1)(x + 1 - x - 1)\n\nThis is a 3D continuous system that exhibits chaos.\n\nChua designed an electronic circuit with the expressed goal of exhibiting chaotic motion, and this system is obtained by rescaling the circuit units to simplify the form of the equation. [1]\n\nThe parameters are a, b, m0 and m1. Setting a = 15.6, m0 = -8/7 and m1 = -5/7, and varying the parameter b from b = 25 to b = 51, one observes a classic period-doubling bifurcation route to chaos. [2]\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : Chua, Leon O. \"The genesis of Chua's circuit\", 1992.\n\n[2] : Leon O. Chua (2007) \"Chua circuit\", Scholarpedia, 2(10):1488.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.coupledstandardmaps","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.coupledstandardmaps","text":"coupledstandardmaps(M::Int, u0 = 0.001rand(2M); ks = ones(M), Γ = 1.0)\n\nbeginaligned\ntheta_i = theta_i + p_i \np_i = p_i + k_isin(theta_i) - Gamma left\nsin(theta_i+1 - theta_i) + sin(theta_i-1 - theta_i)\nright\nendaligned\n\nA discrete system of M nonlinearly coupled standard maps, first introduced in [1] to study diffusion and chaos thresholds. The total dimension of the system is 2M. The maps are coupled through Γ and the i-th map has a nonlinear parameter ks[i]. The first M parameters are the ks, the M+1th parameter is Γ.\n\nThe first M entries of the state are the angles, the last M are the momenta.\n\n[1] : H. Kantz & P. Grassberger, J. Phys. A 21, pp 127–133 (1988)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.double_pendulum","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.double_pendulum","text":"double_pendulum(u0 = [π/2, 0, 0, 0.5];\n                G=10.0, L1 = 1.0, L2 = 1.0, M1 = 1.0, M2 = 1.0)\n\nFamous chaotic double pendulum system (also used for our logo!). Keywords are gravity (G), lengths of each rod and mass of each ball (all assumed SI units).\n\nThe variables order is [θ1, dθ1/dt, θ2, dθ2/dt].\n\nJacobian is created automatically (thus methods that use the Jacobian will be slower)!\n\n(please contribute the Jacobian and the e.o.m. in LaTeX :smile:)\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.duffing","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.duffing","text":"duffing(u0 = [0.1, 0.25]; ω = 2.2, f = 27.0, d = 0.2, β = 1)\n\nThe (forced) duffing oscillator, that satisfies the equation\n\nddotx + dcdotdotx + β*x + x^3 = fcos(omega t)\n\nwith f, ω the forcing strength and frequency and d the dampening.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.fitzhugh_nagumo","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.fitzhugh_nagumo","text":"fitzhugh_nagumo(u = 0.5ones(2); a=3.0, b=0.2, ε=0.01, I=0.0)\n\nFamous excitable system which emulates the firing of a neuron, with rule\n\nbeginaligned\ndotv = av(v-b)(1-v) - w + I \nddotw = varepsilon(v - w)\nendaligned\n\nMore details in the Scholarpedia entry.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.gissinger","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.gissinger","text":"gissinger(u0 = [3, 0.5, 1.5]; μ = 0.119, ν = 0.1, Γ = 0.9)\n\nbeginaligned\ndotQ = mu Q - VD \ndotD = -nu D + VQ \ndotV = Gamma -V + QD\nendaligned\n\nA continuous system that models chaotic reversals due to Gissinger [1], applied to study the reversals of the magnetic field of the Earth.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : C. Gissinger, Eur. Phys. J. B 85, 4, pp 1-12 (2012)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.henon","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.henon","text":"henon(u0=zeros(2); a = 1.4, b = 0.3)\n\nbeginaligned\nx_n+1 = 1 - ax^2_n+y_n \ny_n+1  = bx_n\nendaligned\n\nThe Hénon map is a two-dimensional mapping due to Hénon [1] that can display a strange attractor (at the default parameters). In addition, it also displays many other aspects of chaos, like period doubling or intermittency, for other parameters.\n\nAccording to the author, it is a system displaying all the properties of the Lorentz system (1963) while being as simple as possible. Default values are the ones used in the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : M. Hénon, Commun.Math. Phys. 50, pp 69 (1976)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.henonheiles","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.henonheiles","text":"henonheiles(u0=[0, -0.25, 0.42081,0])\n\nbeginaligned\ndotx = p_x \ndoty = p_y \ndotp_x = -x -2 xy \ndotp_y = -y - (x^2 - y^2)\nendaligned\n\nThe Hénon–Heiles system [1] is a conservative dynamical system and was introduced as a simplification of the motion of a star around a galactic center. It was originally intended to study the existence of a \"third integral of motion\" (which would make this 4D system integrable). In that search, the authors encountered chaos, as the third integral existed for only but a few initial conditions.\n\nThe default initial condition is a typical chaotic orbit. The function Systems.henonheiles_ics(E, n) generates a grid of n×n initial conditions, all having the same energy E.\n\n[1] : Hénon, M. & Heiles, C., The Astronomical Journal 69, pp 73–79 (1964)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.logistic","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.logistic","text":"logistic(x0 = 0.4; r = 4.0)\n\nx_n+1 = rx_n(1-x_n)\n\nThe logistic map is an one dimensional unimodal mapping due to May [1] and is used by many as the archetypal example of how chaos can arise from very simple equations.\n\nOriginally intentend to be a discretized model of polulation dynamics, it is now famous for its bifurcation diagram, an immensely complex graph that that was shown be universal by Feigenbaum [2].\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : R. M. May, Nature 261, pp 459 (1976)\n\n[2] : M. J. Feigenbaum, J. Stat. Phys. 19, pp 25 (1978)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.lorenz","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.lorenz","text":"lorenz(u0=[0.0, 10.0, 0.0]; σ = 10.0, ρ = 28.0, β = 8/3) -> ds\n\nbeginaligned\ndotX = sigma(Y-X) \ndotY = -XZ + rho X -Y \ndotZ = XY - beta Z\nendaligned\n\nThe famous three dimensional system due to Lorenz [1], shown to exhibit so-called \"deterministic nonperiodic flow\". It was originally invented to study a simplified form of atmospheric convection.\n\nCurrently, it is most famous for its strange attractor (occuring at the default parameters), which resembles a butterfly. For the same reason it is also associated with the term \"butterfly effect\" (a term which Lorenz himself disliked) even though the effect applies generally to dynamical systems. Default values are the ones used in the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : E. N. Lorenz, J. atmos. Sci. 20, pp 130 (1963)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.lorenz96","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.lorenz96","text":"lorenz96(N::Int, u0 = rand(M); F=0.01)\n\nfracdx_idt = (x_i+1-x_i-2)x_i-1 - x_i + F\n\nN is the chain length, F the forcing. Jacobian is created automatically. (parameter container only contains F)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.magnetic_pendulum","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.magnetic_pendulum","text":"magnetic_pendulum(u=[cos(θ),sin(θ),0,0]; γ=1, d=0.3, α=0.2, ω=0.5, N=3)\n\nCreate a pangetic pendulum with N magnetics, equally distributed along the unit circle, with dynamical rule\n\nbeginaligned\nddotx = -omega ^2x - alpha dotx - sum_i=1^N fracgamma (x - x_i)D_i^3 \nddoty = -omega ^2y - alpha doty - sum_i=1^N fracgamma (y - y_i)D_i^3 \nD_i = sqrt(x-x_i)^2  + (y-y_i)^2 + d^2\nendaligned\n\nwhere α is friction, ω is eigenfrequency, d is distance of pendulum from the magnet's plane and γ is the magnetic strength. A random initial condition is initialized by default somewhere along the unit circle with zero velocity.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.manneville_simple","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.manneville_simple","text":"manneville_simple(x0 = 0.4; ε = 1.1)\n\nx_n+1 =  (1+varepsilon)x_n + (1-varepsilon)x_n^2  mod 1\n\nA simple 1D map due to Mannevile[Manneville1980] that is useful in illustrating the concept and properties of intermittency.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[Manneville1980]: Manneville, P. (1980). Intermittency, self-similarity and 1/f spectrum in dissipative dynamical systems. Journal de Physique, 41(11), 1235–1243\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.more_chaos_example","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.more_chaos_example","text":"more_chaos_example(u = rand(3))\n\nA three dimensional chaotic system introduced in [Sprott2020] with rule\n\nbeginaligned\ndotx = y \ndoty = -x - sign(z)y \ndotz = y^2 - exp(-x^2)\nendaligned\n\nIt is noteworthy because its strange attractor is multifractal with fractal dimension ≈ 3.\n\n[Sprott2020]: Sprott, J.C. 'Do We Need More Chaos Examples?', Chaos Theory and Applications 2(2),1-3, 2020\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.nosehoover","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.nosehoover","text":"nosehoover(u0 = [0, 0.1, 0])\n\nbeginaligned\ndotx = y \ndoty = yz - x \ndotz = 1 - y^2\nendaligned\n\nThree dimensional conservative continuous system, discovered in 1984 during investigations in thermodynamical chemistry by Nosé and Hoover, then rediscovered by Sprott during an exhaustive search as an extremely simple chaotic system. [1]\n\nSee Chapter 4 of \"Elegant Chaos\" by J. C. Sprott. [2]\n\n[1] : Hoover, W. G. (1995). Remark on ‘‘Some simple chaotic flows’’. Physical Review E, 51(1), 759.\n\n[2] : Sprott, J. C. (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.pomeau_manneville","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.pomeau_manneville","text":"pomaeu_manneville(u0 = 0.2; z = 2.5)\n\nThe Pomeau-Manneville map is a one dimensional discrete map which is characteristic for displaying intermittency [1]. Specifically, for z > 2 the average time between chaotic bursts diverges, while for z > 2.5, the map iterates are long range correlated [2].\n\nNotice that here we are providing the \"symmetric\" version:\n\nx_n+1 = begincases\n-4x_n + 3  quad x_n in (05 1 \nx_n(1 + 2x_n^z-1)  quad x_n le 05 \n-4x_n - 3  quad x_n in -1 05)\nendcases\n\n[1] : Manneville & Pomeau, Comm. Math. Phys. 74 (1980)\n\n[2] : Meyer et al., New. J. Phys 20 (2019)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.qbh","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.qbh","text":"qbh([u0]; A=1.0, B=0.55, D=0.4)\n\nA conservative dynamical system with rule\n\nbeginaligned\ndotq_0 = A p_0 \ndotq_2 = A p_2 \ndotp_0 = -A q_0 -3 fracBsqrt2 (q_2^2 - q_1^2) - D q_1 (q_1^2 + q_2^2) \ndotp_2 = -q_2 (A + 3sqrt2 B q_1 + D (q_1^2 + q_2^2)) (x^2 - y^2)\nendaligned\n\nThis dynamical rule corresponds to a Hamiltonian used in nuclear physics to study the quadrupole vibrations of the nuclear surface [1,2].\n\nH(p_0 p_2 q_0 q_2) = fracA2left(p_0^2+p_2^2right)+fracA2left(q_0^2+q_2^2right)\n\t\t\t +fracBsqrt2q_0left(3q_2^2-q_0^2right) +fracD4left(q_0^2+q_2^2right)^2\n\nThe Hamiltonian has a similar structure with the Henon-Heiles one, but it has an added fourth order term and presents a nontrivial dependence of chaoticity with the increase of energy [3]. The default initial condition is chaotic.\n\n[1]: Eisenberg, J.M., & Greiner, W., Nuclear theory 2 rev ed. Netherlands: North-Holland pp 80 (1975)\n\n[2]: Baran V. and Raduta A. A., International Journal of Modern Physics E, 7, pp 527–551 (1998)\n\n[3]: Micluta-Campeanu S., Raportaru M.C., Nicolin A.I., Baran V., Rom. Rep. Phys. 70, pp 105 (2018)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.rikitake","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.rikitake","text":"rikitake(u0 = [1, 0, 0.6]; μ = 1.0, α = 1.0)\n\nbeginaligned\ndotx = -mu x +yz \ndoty = -mu y +x(z-alpha) \ndotz = 1 - xz\nendaligned\n\nRikitake's dynamo is a system that tries to model the magnetic reversal events by means of a double-disk dynamo system.\n\n[1] : T. Rikitake Math. Proc. Camb. Phil. Soc. 54, pp 89–105, (1958)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.roessler","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.roessler","text":"roessler(u0=[1, -2, 0.1]; a = 0.2, b = 0.2, c = 5.7)\n\nbeginaligned\ndotx = -y-z \ndoty = x+ay \ndotz = b + z(x-c)\nendaligned\n\nThis three-dimensional continuous system is due to Rössler [1]. It is a system that by design behaves similarly to the lorenz system and displays a (fractal) strange attractor. However, it is easier to analyze qualitatively, as for example the attractor is composed of a single manifold. Default values are the same as the original paper.\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : O. E. Rössler, Phys. Lett. 57A, pp 397 (1976)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.shinriki","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.shinriki","text":"shinriki(u0 = [-2, 0, 0.2]; R1 = 22.0)\n\nShinriki oscillator with all other parameters (besides R1) set to constants. This is a stiff problem, be careful when choosing solvers and tolerances.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.standardmap","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.standardmap","text":"standardmap(u0=[0.001245, 0.00875]; k = 0.971635)\n\nbeginaligned\ntheta_n+1 = theta_n + p_n+1 \np_n+1 = p_n + ksin(theta_n)\nendaligned\n\nThe standard map (also known as Chirikov standard map) is a two dimensional, area-preserving chaotic mapping due to Chirikov [1]. It is one of the most studied chaotic systems and by far the most studied Hamiltonian (area-preserving) mapping.\n\nThe map corresponds to the  Poincaré's surface of section of the kicked rotor system. Changing the non-linearity parameter k transitions the system from completely periodic motion, to quasi-periodic, to local chaos (mixed phase-space) and finally to global chaos.\n\nThe default parameter k is the critical parameter where the golden-ratio torus is destroyed, as was calculated by Greene [2]. The e.o.m. considers the angle variable θ to be the first, and the angular momentum p to be the second, while both variables are always taken modulo 2π (the mapping is on the [0,2π)² torus).\n\nThe parameter container has the parameters in the same order as stated in this function's documentation string.\n\n[1] : B. V. Chirikov, Preprint N. 267, Institute of Nuclear Physics, Novosibirsk (1969)\n\n[2] : J. M. Greene, J. Math. Phys. 20, pp 1183 (1979)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.thomas_cyclical","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.thomas_cyclical","text":"thomas_cyclical(u0 = [1.0, 0, 0]; b = 0.2)\n\nbeginaligned\ndotx = sin(y) - bx\ndoty = sin(z) - by\ndotz = sin(x) - bz\nendaligned\n\nThomas' cyclically symmetric attractor is a 3D strange attractor originally proposed by René Thomas[Thomas1999]. It has a simple form which is cyclically symmetric in the x,y, and z variables and can be viewed as the trajectory of a frictionally dampened particle moving in a 3D lattice of forces. For more see the Wikipedia page.\n\nReduces to the labyrinth system for b=0, see See discussion in Section 4.4.3 of \"Elegant Chaos\" by J. C. Sprott.\n\n[Thomas1999]: Thomas, R. (1999). International Journal of Bifurcation and Chaos, 9(10), 1889-1905.\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.towel","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.towel","text":"towel(u0 = [0.085, -0.121, 0.075])\n\nbeginaligned\nx_n+1 = 38 x_n (1-x_n) -005 (y_n +035) (1-2z_n) \ny_n+1 = 01 left( left( y_n +035 right)left( 1+2z_nright) -1 right)\nleft( 1 -19 x_n right) \nz_n+1 = 378 z_n (1-z_n) + b y_n\nendaligned\n\nThe folded-towel map is a hyperchaotic mapping due to Rössler [1]. It is famous for being a mapping that has the smallest possible dimensions necessary for hyperchaos, having two positive and one negative Lyapunov exponent. The name comes from the fact that when plotted looks like a folded towel, in every projection.\n\nDefault values are the ones used in the original paper.\n\n[1] : O. E. Rössler, Phys. Lett. 71A, pp 155 (1979)\n\n\n\n\n\n","category":"function"},{"location":"ds/predefined/#DynamicalSystemsBase.Systems.ueda","page":"Predefined Dynamical Systems","title":"DynamicalSystemsBase.Systems.ueda","text":"ueda(u0 = [3.0, 0]; k = 0.1, B = 12.0)\n\nddotx + k dotx + x^3 = Bcost\n\nNonautonomous Duffing-like forced oscillation system, discovered by Ueda in\n\nIt is one of the first chaotic systems to be discovered.\n\nThe stroboscopic plot in the (x, ̇x) plane with period 2π creates a \"broken-egg attractor\" for k = 0.1 and B = 12. Figure 5 of [1] is reproduced by\n\nusing Plots\nds = Systems.ueda()\na = trajectory(ds, 2π*5e3, dt = 2π)\nscatter(a[:, 1], a[:, 2], markersize = 0.5, title=\"Ueda attractor\")\n\nFor more forced oscillation systems, see Chapter 2 of \"Elegant Chaos\" by J. C. Sprott. [2]\n\n[1] : Ruelle, David, ‘Strange Attractors’, The Mathematical Intelligencer, 2.3 (1980), 126–37\n\n[2] : Sprott, J. C. (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#Advanced-documentation","page":"Advanced Documentation","title":"Advanced documentation","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"This section overviews the various integrators available from DynamicalSystemsBase, as well as gives some insight into the internals, so that other developers that want to use this library can build upon it.","category":"page"},{"location":"advanced/#Integrators","page":"Advanced Documentation","title":"Integrators","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"integrator\nparallel_integrator\ntangent_integrator","category":"page"},{"location":"advanced/#DynamicalSystemsBase.integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.integrator","text":"integrator(ds::DynamicalSystem [, u0]; diffeq...) -> integ\n\nReturn an integrator object that can be used to evolve a system interactively using step!(integ [, Δt]). Optionally specify an initial state u0.\n\nThe state of this integrator is a vector.\n\ndiffeq... are keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.parallel_integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.parallel_integrator","text":"parallel_integrator(ds::DynamicalSystem, states; kwargs...)\n\nReturn an integrator object that can be used to evolve many states of a system in parallel at the exact same times, using step!(integ [, Δt]).\n\nstates are expected as vectors of vectors.\n\nKeyword Arguments\n\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems. These keywords can also include callback for event handling.\n\nIt is heavily advised to use the functions get_state and set_state! to manipulate the integrator. Provide i as a second argument to change the i-th state.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.tangent_integrator","page":"Advanced Documentation","title":"DynamicalSystemsBase.tangent_integrator","text":"tangent_integrator(ds::DynamicalSystem, Q0 | k::Int; kwargs...)\n\nReturn an integrator object that evolves in parallel both the system as well as deviation vectors living on the tangent space, also called linearized space.\n\nQ0 is a matrix whose columns are initial values for deviation vectors. If instead of a matrix Q0 an integer k is given, then k random orthonormal vectors are choosen as initial conditions.\n\nKeyword Arguments\n\nu0 : Optional different initial state.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems. These keywords can also include callback for event handling.\n\nIt is heavily advised to use the functions get_state, get_deviations, set_state!, set_deviations! to manipulate the integrator.\n\nDescription\n\nIf J is the jacobian of the system then the tangent dynamics are the equations that evolve in parallel the system as well as a deviation vector (or matrix) w:\n\nbeginaligned\ndotu = f(u p t) \ndotw = J(u p t) times w\nendaligned\n\nwith f being the equations of motion and u the system state. Similar equations hold for the discrete case.\n\n\n\n\n\n","category":"function"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"Notice that the state type integrator.u of each integrator is quite different and does change between the possible versions of a DynamicalSystem!","category":"page"},{"location":"advanced/#Integrator-state-functions","page":"Advanced Documentation","title":"Integrator state functions","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"There are four functions associated with the integrators that we export:","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"get_state\nset_state!\nget_deviations\nset_deviations!","category":"page"},{"location":"advanced/#DynamicalSystemsBase.get_state","page":"Advanced Documentation","title":"DynamicalSystemsBase.get_state","text":"get_state(ds::DynamicalSystem)\n\nReturn the state of ds.\n\nget_state(integ [, i::Int = 1])\n\nReturn the state of the integrator, in the sense of the state of the dynamical system.\n\nIf the integrator is a parallel_integrator, passing i will return the i-th state. The function also correctly returns the true state of the system for tangent integrators.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.set_state!","page":"Advanced Documentation","title":"DynamicalSystemsBase.set_state!","text":"set_state!(integ, u [, i::Int = 1])\n\nSet the state of the integrator to u, in the sense of the state of the dynamical system. Works for any integrator (normal, tangent, parallel).\n\nFor parallel integrator, you can choose which state to set (using i).\n\nAutomatically does u_modified!(integ, true).\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.get_deviations","page":"Advanced Documentation","title":"DynamicalSystemsBase.get_deviations","text":"get_deviations(tang_integ)\n\nReturn the deviation vectors of the tangent_integrator in a form of a matrix with columns the vectors.\n\n\n\n\n\n","category":"function"},{"location":"advanced/#DynamicalSystemsBase.set_deviations!","page":"Advanced Documentation","title":"DynamicalSystemsBase.set_deviations!","text":"set_deviations!(tang_integ, Q)\n\nSet the deviation vectors of the tangent_integrator to Q, which must be a matrix with each column being a deviation vector.\n\nAutomatically does u_modified!(tang_integ, true).\n\n\n\n\n\n","category":"function"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"note: Note\nThese functions work with any possible integrator and it is best to use the to change states robustly!","category":"page"},{"location":"advanced/#Re-initializing-an-integrator","page":"Advanced Documentation","title":"Re-initializing an integrator","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"It is more efficient to re-initialize an integrator using reinit! than to create a new one. This can be very helpful when looping over initial conditions and/or parameter values.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"All high-level functions from ChaosTools have a set-up part that creates an integrator, and a low-level part that does the computation. The low level part is your friend! Use it! See the Using GALI page for an example as well as the section below.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"The reinit! call signature is the same for continuous and discrete systems. In the following, state is supposed to be a D dimensional vector (state of the dynamical system).","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"reinit!(integ, state) : to be used with standard integrator.\nreinit!(integ, Vector_of_states) : to be used with the parallel_integrator.\nreinit!(integ, state, Q0::AbstractMatrix) : to be used with the tangent_integrator. This three argument version of reinit! is exported from DynamicalSystemsBase.","category":"page"},{"location":"advanced/#Re-init-of-continuous-tangent-integrator","page":"Advanced Documentation","title":"Re-init of continuous tangent integrator","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"Here we compute the lyapunovspectrum for many different initial conditions.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"ds = Systems.lorenz()\ntinteg = tangent_integrator(ds, 2)\nics = [rand(3) for i in 1:100]\nfor ic in ics\n  reinit!(tinteg, ic, orthonormal(3, 2))\n  λ = lyapunovspectrum(tinteg, 1000, 0.1, 10.0)\n  # reminder: lyapunovspectrum(tinteg, N, dt::Real, Ttr::Real = 0.0)\nend","category":"page"},{"location":"advanced/#Re-init-of-discrete-parallel-integrator","page":"Advanced Documentation","title":"Re-init of discrete parallel integrator","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"Here we compute the lyapunov for many different parameters.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"ds = Systems.henon()\nu0 = rand(SVector{2})\nps = 1.2:0.01:1.4\npinteg = parallel_integrator(ds, [u0, u0 + 1e-9rand(SVector{2})])\nfor p in ps\n  set_parameter!(ds, 1, p)\n  reinit!(pinteg, [u0, u0 + 1e-9rand(SVector{2})])\n  λ = lyapunov(pinteg, 1000, 10, 1, 1e-9, 1e-6, 1e-12)\n  # reminder: lyapunov(pinteg, T, Ttr, dt, d0, ut, lt)\nend","category":"page"},{"location":"advanced/#Using-callbacks-with-integrators","page":"Advanced Documentation","title":"Using callbacks with integrators","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"For the case of continuous systems you can add callbacks from the event handling of DifferentialEquations.jl. This is done simply as a keyword argument to the initializers.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"In this example we use a simple SavingCallback to save the distance between the two states of a parallel_integrator.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"using DynamicalSystems, DiffEqCallbacks\nusing LinearAlgebra: norm\n\nkwargs = (abstol=1e-14, reltol=1e-14, maxiters=1e9)\nds = Systems.lorenz()\nd0 = 1e-9\nT = 100.0\n\nsave_func(u, t, integrator) = norm(u[1] - u[2])\nsaved_values = SavedValues(eltype(ds.t0), eltype(get_state(ds)))\ncb = SavingCallback(save_func, saved_values)\n\nu0 = get_state(ds)\npinteg = parallel_integrator(ds, [u0, u0 + rand(SVector{3})*d0*√3];\nkwargs..., callback = cb)\nstep!(pinteg, T)\nt = saved_values.t\nn = saved_values.saveval","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"As expected you can see that the recorded distance between two states is increasing.","category":"page"},{"location":"advanced/#DynamicalSystem-implementation","page":"Advanced Documentation","title":"DynamicalSystem implementation","text":"","category":"section"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"abstract type DynamicalSystem{\n        IIP,     # is in place , for dispatch purposes and clarity\n        S,       # state type\n        D,       # dimension\n        F,       # equations of motion\n        P,       # parameters\n        JAC,     # jacobian\n        JM,      # jacobian matrix\n        IAD}     # is auto-differentiated\n    # one-liner: {IIP, S, D, F, P, JAC, JM, IAD}\n    # Subtypes of DynamicalSystem have fields:\n    # 1. f\n    # 2. u0\n    # 3. p\n    # 4. t0\n    # 5. jacobian (function)\n    # 6. J (matrix)\nend","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"The DynamicalSystem stores only the absolutely necessary information. Every other functionality of DynamicalSystems.jl initializes an integrator.","category":"page"},{"location":"advanced/","page":"Advanced Documentation","title":"Advanced Documentation","text":"The final type-parameter IAD is useful when creating the tangent_integrator, so that the vector field is not computed twice!","category":"page"},{"location":"embedding/traditional/#Traditional-Optimal-Embedding","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"","category":"section"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"This page discusses and provides algorithms for estimating optimal parameters to do Delay Coordinates Embedding (DCE) with.","category":"page"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"The approaches can be grouped into two schools:","category":"page"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"Traditional, where one tries to find the best value for a delay time τ and then an optimal embedding dimension d.\nUnified, where at the same time an optimal combination of τ, d is found, and is discussed in the Unified Optimal Embedding page.","category":"page"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"The independent approach is something \"old school\", while recent scientific research has shifted almost exclusively to unified approaches.","category":"page"},{"location":"embedding/traditional/#Optimal-delay-time","page":"Traditional Optimal Embedding","title":"Optimal delay time","text":"","category":"section"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"estimate_delay\nexponential_decay_fit","category":"page"},{"location":"embedding/traditional/#DelayEmbeddings.estimate_delay","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.estimate_delay","text":"estimate_delay(s, method::String [, τs = 1:100]; kwargs...) -> τ\n\nEstimate an optimal delay to be used in embed. The method can be one of the following:\n\n\"ac_zero\" : first delay at which the auto-correlation function becomes <0.\n\"ac_min\" : delay of first minimum of the auto-correlation function.\n\"mi_min\" : delay of first minimum of mutual information of s with itself (shifted for various τs). Keywords nbins, binwidth are propagated into selfmutualinfo.\n\"exp_decay\" : exponential_decay_fit of the correlation function rounded  to an integer (uses least squares on c(t) = exp(-t/τ) to find τ).\n\"exp_extrema\" : same as above but the exponential fit is done to the absolute value of the local extrema of the correlation function.\n\nBoth the mutual information and correlation function (autocor) are computed only for delays τs. This means that the min methods can never return the first value of τs!\n\nThe method mi_min is significantly more accurate than the others and also returns good results for most timeseries. It is however the slowest method (but still quite fast!).\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.exponential_decay_fit","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.exponential_decay_fit","text":"exponential_decay_fit(x, y, weight = :equal) -> τ\n\nPerform a least square fit of the form y = exp(-x/τ) and return τ. Taken from:  http://mathworld.wolfram.com/LeastSquaresFittingExponential.html. Assumes equal lengths of x, y and that y ≥ 0.\n\nTo use the method that gives more weight to small values of y, use weight = :small.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#Self-Mutual-Information","page":"Traditional Optimal Embedding","title":"Self Mutual Information","text":"","category":"section"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"selfmutualinfo","category":"page"},{"location":"embedding/traditional/#DelayEmbeddings.selfmutualinfo","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.selfmutualinfo","text":"selfmutualinfo(s, τs; kwargs...) → m\n\nCalculate the mutual information between the time series s and itself delayed by τ points for τ ∈ τs, using an improvement of the method outlined by Fraser & Swinney in[Fraser1986].\n\nDescription\n\nThe joint space of s and its τ-delayed image (sτ) is partitioned as a rectangular grid, and the mutual information is computed from the joint and marginal frequencies of s and sτ in the grid as defined in [1]. The mutual information values are returned in a vector m of the same length as τs.\n\nIf any of the optional keyword parameters is given, the grid will be a homogeneous partition of the space where s and sτ are defined. The margins of that partition will be divided in a number of bins equal to nbins, such that the width of each bin will be binwidth, and the range of nonzero values of s will be in the centre. If only of those two parameters is given, the other will be automatically calculated to adjust the size of the grid to the area where s and sτ are nonzero.\n\nIf no parameter is given, the space will be partitioned by a recursive bisection algorithm based on the method given in [1].\n\nNotice that the recursive method of [1] evaluates the joint frequencies of s and sτ in each cell resulting from a partition, and stops when the data points are uniformly distributed across the sub-partitions of the following levels. For performance and stability reasons, the automatic partition method implemented in this function is only used to divide the axes of the grid, using the marginal frequencies of s.\n\n[Fraser1986]: Fraser A.M. & Swinney H.L. \"Independent coordinates for strange attractors from mutual information\" Phys. Rev. A 33(2), 1986, 1134:1140.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"Notice that mutual information between two different timeseries x, y exists in JuliaDynamics as well, but in the package TransferEntropy.jl. It is also trivial to define it yourself using genentropy by doing","category":"page"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"function mutualinfo(x, y, est; base = 2, α = 1)\n    X = genentropy(Dataset(x), est; base = base, α = α)\n    Y = genentropy(Dataset(y), est; base = base, α = α)\n    XY = genentropy(Dataset(x, y), est; base = base, α = α)\n    return X + Y - XY\nend","category":"page"},{"location":"embedding/traditional/#Optimal-embedding-dimension","page":"Traditional Optimal Embedding","title":"Optimal embedding dimension","text":"","category":"section"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"optimal_traditional_de\ndelay_afnn\ndelay_ifnn\ndelay_fnn\ndelay_f1nn\nDelayEmbeddings.stochastic_indicator","category":"page"},{"location":"embedding/traditional/#DelayEmbeddings.optimal_traditional_de","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.optimal_traditional_de","text":"optimal_traditional_de(s, method = \"afnn\", dmethod = \"mi_min\"; kwargs...) → 𝒟, τ, E\n\nProduce an optimal delay embedding 𝒟 of the given timeseries s by using the traditional approach of first finding an optimal (and constant) delay time using estimate_delay with the given dmethod, and then an optimal embedding dimension, by calculating an appropriate statistic for each dimension d ∈ 1:dmax. Return the embedding 𝒟, the optimal delay time τ (the optimal embedding dimension d is just size(𝒟, 2)) and the actual statistic E used to estimate optimal d.\n\nNotice that E is a function of the embedding dimension, which ranges from 1 to dmax.\n\nFor calculating E to estimate the dimension we use the given method which can be:\n\n\"afnn\" (default) is Cao's \"Averaged False Nearest Neighbors\" method[Cao1997],   which gives a ratio of distances between nearest neighbors.\n\"ifnn\" is the \"Improved False Nearest Neighbors\" from Hegger & Kantz[Hegger1999],   which gives the fraction of false nearest neighbors.\n\"fnn\" is Kennel's \"False Nearest Neighbors\" method[Kennel1992], which gives   the number of points that cease to be \"nearest neighbors\" when the dimension   increases.\n\"f1nn\" is Krakovská's \"False First Nearest Neighbors\" method[Krakovská2015],   which gives the ratio of pairs of points that cease to be \"nearest neighbors\"   when the dimension increases.\n\nFor more details, see individual methods: delay_afnn, delay_ifnn, delay_fnn, delay_f1nn. The special keywords `` denote for which possible embedding dimensions should the statistics be computed for.\n\nwarn: Careful in automated methods\nWhile this method is automated if you want to be really sure of the results, you should directly calculate the statistic and plot its values versus the dimensions.\n\nKeywords\n\nThe keywords\n\nτs = 1:100, dmax = 10\n\ndenote which delay times and embedding dimensions ds ∈ 1:dmax to consider when calculating optimal embedding. All remaining keywords are propagated to the low level functions:\n\nfnn_thres::Real = 0.05, slope_thres::Real= 0.2, w::Int=1,\nrtol=10.0, atol=2.0, τs = 1:100, metric = Euclidean(), r::Real=2.0,\nstoch_thres = 0.1\n\nDescription\n\nWe estimate the optimal embedding dimension based on the given delay time gained from dmethod as follows: For Cao's method the optimal dimension is reached, when the slope of the E₁-statistic (output from \"afnn\") falls below the threshold slope_thres (default is .05) and the according stochastic test turns out to be false, i.e. if the E₂-statistic's first value is < 1 - stoch_thres.\n\nFor all the other methods we return the optimal embedding dimension when the corresponding FNN-statistic (output from \"ifnn\", \"fnn\" or \"f1nn\") falls below the fnn-threshold fnn_thres (Default is .05) AND the slope of the statistic falls below the threshold slope_thres. Note that with noise contaminated time series, one might need to adjust fnn_thres according to the noise level.\n\nSee also the file test/compare_different_dimension_estimations.jl for a comparison.\n\n[Cao1997]: Liangyue Cao, Physica D, pp. 43-50 (1997)\n\n[Kennel1992]: M. Kennel et al., Phys. Review A 45(6), (1992).\n\n[Krakovská2015]: Anna Krakovská et al., J. Complex Sys. 932750 (2015)\n\n[Hegger1999]: Hegger & Kantz, Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.delay_afnn","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.delay_afnn","text":"delay_afnn(s::AbstractVector, τ:Int, ds = 2:6; metric=Euclidean(), w = 0) → E₁\n\nCompute the parameter E₁ of Cao's \"averaged false nearest neighbors\" method for determining the minimum embedding dimension of the time series s, with a sequence of τ-delayed temporal neighbors.\n\nDescription\n\nGiven the scalar timeseries s and the embedding delay τ compute the values of E₁ for each embedding dimension d ∈ ds, according to Cao's Method (eq. 3 of[Cao1997]).\n\nThis quantity is a ratio of the averaged distances between the nearest neighbors of the reconstructed time series, which quantifies the increment of those distances when the embedding dimension changes from d to d+1.\n\nReturn the vector of all computed E₁s. To estimate a good value for d from this, find d for which the value E₁ saturates at some value around 1.\n\nNote: This method does not work for datasets with perfectly periodic signals.\n\nw is the Theiler window.\n\nSee also: optimal_traditional_de and stochastic_indicator.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.delay_ifnn","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.delay_ifnn","text":"delay_ifnn(s::Vector, τ::Int, ds = 1:10; kwargs...) → `FNNs`\n\nCompute and return the FNNs-statistic for the time series s and a uniform time delay τ and embedding dimensions ds after [Hegger1999]. In this notation γ ∈ γs = d-1, if d is the embedding dimension. This fraction tends to 0 when the optimal embedding dimension with an appropriate lag is reached.\n\nKeywords\n\n*r = 2: Obligatory threshold, which determines the maximum tolerable spreading     of trajectories in the reconstruction space. *metric = Euclidean: The norm used for distance computations. *w = 1 = The Theiler window.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.delay_fnn","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.delay_fnn","text":"delay_fnn(s::AbstractVector, τ:Int, ds = 2:6; rtol=10.0, atol=2.0) → FNNs\n\nCalculate the number of \"false nearest neighbors\" (FNNs) of the datasets created from s with embed(s, d, τ) for d ∈ ds.\n\nDescription\n\nGiven a dataset made by embed(s, d, τ) the \"false nearest neighbors\" (FNN) are the pairs of points that are nearest to each other at dimension d, but are separated at dimension d+1. Kennel's criteria for detecting FNN are based on a threshold for the relative increment of the distance between the nearest neighbors (rtol, eq. 4 in[Kennel1992]), and another threshold for the ratio between the increased distance and the \"size of the attractor\" (atol, eq. 5 in[Kennel1992]). These thresholds are given as keyword arguments.\n\nThe returned value is a vector with the number of FNN for each γ ∈ γs. The optimal value for γ is found at the point where the number of FNN approaches zero.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.delay_f1nn","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.delay_f1nn","text":"delay_f1nn(s::AbstractVector, τ::Int, ds = 2:6, metric = Euclidean())\n\nCalculate the ratio of \"false first nearest neighbors\" (FFNN) of the datasets created from s with embed(s, d, τ) for d ∈ ds.\n\nDescription\n\nGiven a dataset made by embed(s, d, τ) the \"false first nearest neighbors\" (FFNN) are the pairs of points that are nearest to each other at dimension d that cease to be nearest neighbors at dimension d+1.\n\nThe returned value is a vector with the ratio between the number of FFNN and the number of points in the dataset for each d ∈ ds. The optimal value for d is found at the point where this ratio approaches zero.\n\nSee also: optimal_traditional_de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#DelayEmbeddings.stochastic_indicator","page":"Traditional Optimal Embedding","title":"DelayEmbeddings.stochastic_indicator","text":"stochastic_indicator(s::AbstractVector, τ:Int, ds = 2:5) -> E₂s\n\nCompute an estimator for apparent randomness in a delay embedding with ds dimensions.\n\nDescription\n\nGiven the scalar timeseries s and the embedding delay τ compute the values of E₂ for each d ∈ ds, according to Cao's Method (eq. 5 of [Cao1997]).\n\nUse this function to confirm that the input signal is not random and validate the results of estimate_dimension. In the case of random signals, it should be E₂ ≈ 1 ∀ d.\n\n\n\n\n\n","category":"function"},{"location":"embedding/traditional/#Example","page":"Traditional Optimal Embedding","title":"Example","text":"","category":"section"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"using DynamicalSystems, PyPlot\n\nds = Systems.roessler()\n# This trajectory is a chaotic attractor with fractal dim ≈ 2\n# therefore the set needs at least embedding dimension of 3\ntr = trajectory(ds, 1000.0; dt = 0.05)\nx = tr[:, 1]\n\ndmax = 7\nfig = figure()\nfor (i, method) in enumerate([\"afnn\", \"fnn\", \"f1nn\", \"ifnn\"])\n    # Plot statistic used to estimate optimal embedding\n    # as well as the automated output embedding\n    𝒟, τ, E = optimal_traditional_de(x, method; dmax)\n    plot(1:dmax, E; label = method, marker = \"o\", ms = 5, color = \"C$(i-1)\")\n    optimal_d = size(𝒟, 2)\n    scatter(optimal_d, E[optimal_d]; marker = \"s\", s = 100, color = \"C$(i-1)\")\nend\nlegend(); xlabel(\"embedding dimension\")\nylabel(\"estimator\")\ntight_layout()\nfig.savefig(\"estimateD.png\"); nothing # hide","category":"page"},{"location":"embedding/traditional/","page":"Traditional Optimal Embedding","title":"Traditional Optimal Embedding","text":"(Image: )","category":"page"},{"location":"ds/general/#Dynamical-System-Definition","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In DynamicalSystems.jl a Dynamical System can be either in continuous time","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"fracdvecudt = vecf(vecu p t)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"or discrete time","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"vecu_n+1 = vecf(vecu_n p n)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"where u is the state of the system and p contains the parameters of the system. The function f is called the dynamic rule of the system, also known as equations of motion.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In addition to f, information about the Jacobian of the system J_f is also used throughout the library.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"warning: Warning\nKeep in mind that almost all functions of DynamicalSystems.jl that use a DynamicalSystem assume that f is differentiable!","category":"page"},{"location":"ds/general/#Creating-a-Dynamical-System","page":"Dynamical System Definition","title":"Creating a Dynamical System","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DynamicalSystem","category":"page"},{"location":"ds/general/#DynamicalSystemsBase.DynamicalSystem","page":"Dynamical System Definition","title":"DynamicalSystemsBase.DynamicalSystem","text":"DynamicalSystem\n\nThe central structure of DynamicalSystems.jl. All functions of the suite that can use known dynamic rule f (equations of motion) expect an instance of this type.\n\nConstructing a DynamicalSystem\n\nDiscreteDynamicalSystem(f, state, p [, jacobian [, J0]]; t0::Int = 0)\nContinuousDynamicalSystem(f, state, p [, jacobian [, J0]]; t0 = 0.0)\n\nwith f a Julia function (see below). p is a parameter container, which we highly suggest to be a mutable, concretely typed container. Pass nothing as p if your system does not have parameters.\n\nt0, J0 allow you to choose the initial time and provide an initialized Jacobian matrix. See CDS_KWARGS for the default options used to evolve continuous systems (through OrdinaryDiffEq).\n\nDynamic rule f\n\nThe are two \"versions\" for DynamicalSystem, depending on whether f is in-place (iip) or out-of-place (oop). Here is how to define them (1D systems are treated differently, see below):\n\noop : f must be in the form f(x, p, t) -> SVector which means that given a state x::SVector and some parameter container p it returns an SVector (from the StaticArrays module) containing the next state/rate-of-change.\niip : f must be in the form f!(xnew, x, p, t) which means that given a state x::Vector and some parameter container p, it writes in-place the new state/rate-of-change in xnew.\n\nt stands for time (integer for discrete systems). iip is suggested for big systems, whereas oop is suggested for small systems. The break-even point at around 10 dimensions.\n\nThe constructor deduces automatically whether f is iip or oop. It is not possible however to deduce whether the system is continuous or discrete just from f, hence the 2 constructors.\n\nJacobian\n\nThe optional argument jacobian for the constructors is a function and (if given) must also be of the same form as f, jacobian(x, p, n) -> SMatrix for the out-of-place version and jacobian!(J, x, p, n) for the in-place version.\n\nThe constructors also allow you to pass an initialized Jacobian matrix J0. This is useful for large oop systems where only a few components of the Jacobian change during the time evolution.\n\nIf jacobian is not given, it is constructed automatically using the module ForwardDiff. Even though ForwardDiff is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so we recommend it.\n\nComment on 1-D\n\nOne dimensional discrete systems expect the state always as a pure number, 0.8 instead of SVector(0.8). For continuous systems, the state can be in-place/out-of-place as in higher dimensions, however the derivative function must be always explicitly given.\n\nInterface to DifferentialEquations.jl\n\nContinuous systems are solved using DifferentialEquations.jl, by default using the keyword arguments contained in the constant CDS_KWARGS.\n\nThe following two interfaces are provided:\n\nContinuousDynamicalSystem(prob::ODEProblem [, jacobian [, J0]])\nODEProblem(continuous_dynamical_system, tspan, args...)\n\nwhere in the second case args stands for the standard extra arguments of ODEProblem: callback, mass_matrix.\n\nIf you want to use callbacks with tangent_integrator or parallel_integrator, then invoke them with extra arguments as shown in the Advanced Documentation.\n\nRelevant Functions\n\ntrajectory, set_parameter!.\n\n\n\n\n\n","category":"type"},{"location":"ds/general/#Definition-Table","page":"Dynamical System Definition","title":"Definition Table","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Here is a handy table that summarizes in what form should be the functions required for the equations of motion and the Jacobian, for each system type:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"System Type equations of motion Jacobian\nin-place (big systems) eom!(du, u, p, t) jacobian!(J, u, p, t)\nout-of-place (small systems) eom(u, p, t) -> SVector jacobian(u, p, t) -> SMatrix","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tip: Use mutable containers for the parameters\nIt is highly suggested to use a subtype of Array,  LMArray or a dictionary for the container of the model's parameters. Some functions offered by DynamicalSystems.jl, like e.g. orbitdiagram, assume that the parameters can be first accessed by p[x] with x some qualifier as well as that this value can be set by p[x] = newvalue.The Labelled Arrays package offers Array implementations that can be accessed both by index as well as by some name.","category":"page"},{"location":"ds/general/#Convenience-functions","page":"Dynamical System Definition","title":"Convenience functions","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The following functions are defined for convenience for any dynamical system:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"dimension\njacobian\nset_parameter!","category":"page"},{"location":"ds/general/#DelayEmbeddings.dimension","page":"Dynamical System Definition","title":"DelayEmbeddings.dimension","text":"dimension(thing) -> D\n\nReturn the dimension of the thing, in the sense of state-space dimensionality.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#DynamicalSystemsBase.jacobian","page":"Dynamical System Definition","title":"DynamicalSystemsBase.jacobian","text":"jacobian(ds::DynamicalSystem, u = ds.u0, t = ds.t0)\n\nReturn the jacobian of the system at u, at t.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#DynamicalSystemsBase.set_parameter!","page":"Dynamical System Definition","title":"DynamicalSystemsBase.set_parameter!","text":"set_parameter!(ds::DynamicalSystem, index, value)\nset_parameter!(ds::DynamicalSystem, values)\n\nChange one or many parameters of the system by setting p[index] = value in the first case and p .= values in the second.\n\nThe same function also works for any integrator.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/#Example:-continuous,-out-of-place","page":"Dynamical System Definition","title":"Example: continuous, out-of-place","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Let's see an example for a small system, which is a case where out-of-place equations of motion are preferred.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"using DynamicalSystems # also exports relevant StaticArrays names\n# Lorenz system\n# Equations of motion:\n@inline @inbounds function loop(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n# Jacobian:\n@inline @inbounds function loop_jac(u, p, t)\n    σ, ρ, β = p\n    J = @SMatrix [-σ  σ  0;\n    ρ - u[3]  (-1)  (-u[1]);\n    u[2]   u[1]  -β]\n    return J\nend\n\nds = ContinuousDynamicalSystem(loop, rand(3), [10.0, 28.0, 8/3], loop_jac)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"3-dimensional continuous dynamical system\n state:     [0.068248, 0.828095, 0.0743729]\n e.o.m.:    loop\n in-place?  false\n jacobian:  loop_jac","category":"page"},{"location":"ds/general/#Example:-discrete,-in-place","page":"Dynamical System Definition","title":"Example: discrete, in-place","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The following example is only 2-dimensional, and thus once again it is \"correct\" to use out-of-place version with SVector. For the sake of example though, we use the in-place version.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"# Henon map.\n# equations of motion:\nfunction hiip(dx, x, p, n)\n    dx[1] = 1.0 - p[1]*x[1]^2 + x[2]\n    dx[2] = p[2]*x[1]\n    return\nend\n# Jacobian:\nfunction hiip_jac(J, x, p, n)\n    J[1,1] = -2*p[1]*x[1]\n    J[1,2] = 1.0\n    J[2,1] = p[2]\n    J[2,2] = 0.0\n    return\nend\nds = DiscreteDynamicalSystem(hiip, zeros(2), [1.4, 0.3], hiip_jac)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"2-dimensional discrete dynamical system\n state:     [0.0, 0.0]\n e.o.m.:    hiip\n in-place?  true\n jacobian:  hiip_jac","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Or, if you don't want to write a Jacobian and want to use the auto-differentiation capabilities of DynamicalSystems.jl, which use the module ForwardDiff:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = DiscreteDynamicalSystem(hiip, zeros(2), [1.4, 0.3])","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"2-dimensional discrete dynamical system\n state:     [0.0, 0.0]\n e.o.m.:    hiip\n in-place?  true\n jacobian:  ForwardDiff","category":"page"},{"location":"ds/general/#Complex-Example","page":"Dynamical System Definition","title":"Complex Example","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"In this example we will go through the implementation of the coupled standard maps from our Predefined Dynamical Systems. It is the most complex implementation and takes full advantage of the flexibility of the constructors. The example will use a function-like-object as equations of motion, as well as a sparse matrix for the Jacobian.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Coupled standard maps is a big mapping that can have arbitrary number of equations of motion, since you can couple N standard maps which are 2D maps, like:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"theta_i = theta_i + p_i \np_i = p_i + k_isin(theta_i) - Gamma leftsin(theta_i+1 - theta_i) + sin(theta_i-1 - theta_i) right","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"To model this, we will make a dedicated struct, which is parameterized on the number of coupled maps:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"struct CoupledStandardMaps{N}\n    idxs::SVector{N, Int}\n    idxsm1::SVector{N, Int}\n    idxsp1::SVector{N, Int}\nend","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"(what these fields are will become apparent later)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We initialize the struct with the amount of standard maps we want to couple, and we also define appropriate parameters:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"M = 5  # couple number\nu0 = 0.001rand(2M) #initial state\nks = 0.9ones(M) # nonlinearity parameters\nΓ = 1.0 # coupling strength\np = (ks, Γ) # parameter container\n\n# Create struct:\nSV = SVector{M, Int}\nidxs = SV(1:M...) # indexes of thetas\nidxsm1 = SV(circshift(idxs, +1)...)  #indexes of thetas - 1\nidxsp1 = SV(circshift(idxs, -1)...)  #indexes of thetas + 1\n# So that:\n# x[i] ≡ θᵢ\n# x[[idxsp1[i]]] ≡ θᵢ+₁\n# x[[idxsm1[i]]] ≡ θᵢ-₁\ncsm = CoupledStandardMaps{M}(idxs, idxsm1, idxsp1);","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We will now use this struct to define a function-like-object, a Type that also acts as a function.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function (f::CoupledStandardMaps{N})(xnew::AbstractVector, x, p, n) where {N}\n    ks, Γ = p\n    @inbounds for i in f.idxs\n\n        xnew[i+N] = mod2pi(\n            x[i+N] + ks[i]*sin(x[i]) -\n            Γ*(sin(x[f.idxsp1[i]] - x[i]) + sin(x[f.idxsm1[i]] - x[i]))\n        )\n\n        xnew[i] = mod2pi(x[i] + xnew[i+N])\n    end\n    return nothing\nend","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"We will use the same struct to create a function for the Jacobian:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function (f::CoupledStandardMaps{M})(\n    J::AbstractMatrix, x, p, n) where {M}\n\n    ks, Γ = p\n    # x[i] ≡ θᵢ\n    # x[[idxsp1[i]]] ≡ θᵢ+₁\n    # x[[idxsm1[i]]] ≡ θᵢ-₁\n    @inbounds for i in f.idxs\n        cosθ = cos(x[i])\n        cosθp= cos(x[f.idxsp1[i]] - x[i])\n        cosθm= cos(x[f.idxsm1[i]] - x[i])\n        J[i+M, i] = ks[i]*cosθ + Γ*(cosθp + cosθm)\n        J[i+M, f.idxsm1[i]] = - Γ*cosθm\n        J[i+M, f.idxsp1[i]] = - Γ*cosθp\n        J[i, i] = 1 + J[i+M, i]\n        J[i, f.idxsm1[i]] = J[i+M, f.idxsm1[i]]\n        J[i, f.idxsp1[i]] = J[i+M, f.idxsp1[i]]\n    end\n    return nothing\nend","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"The only reason that this is possible, is because the eom always takes a AbstractVector as first argument, while the Jacobian always takes an AbstractMatrix. Therefore we can take advantage of multiple dispatch!","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice in addition, that the Jacobian function accesses only half the elements of the matrix. This is intentional, and takes advantage of the fact that the other half is constant. We can leverage this further, by making the Jacobian a sparse matrix. Because the DynamicalSystem constructors allow us to give in a pre-initialized Jacobian matrix, we take advantage of that and create:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"J = zeros(eltype(u0), 2M, 2M)\n# Set ∂/∂p entries (they are eye(M,M))\n# And they dont change they are constants\nfor i in idxs\n    J[i, i+M] = 1\n    J[i+M, i+M] = 1\nend\nsparseJ = sparse(J)\n\ncsm(sparseJ, u0, p, 0) # apply Jacobian to initial state","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"And finally, we are ready to create our dynamical system:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = DiscreteDynamicalSystem(csm, u0, p, csm, sparseJ)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"10-dimensional discrete dynamical system\n state:       [0.000803001, 0.00092095, 0.000313022, …, 3.07769e-5, 0.000670152]\n e.o.m.:      CoupledStandardMaps\n in-place?    true\n jacobian:    CoupledStandardMaps\n parameters:  Tuple","category":"page"},{"location":"ds/general/#Automatic-Jacobians","page":"Dynamical System Definition","title":"Automatic Jacobians","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice that if you are using automatic differentiation for the Jacobian, you should take care to NOT define your equations of motion so that they explicitly use, or return, Float64 numbers. This is because ForwardDiff uses DualNumbers for differentiation. For example, if you did","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"function lorenz(u,p,t)\n    σ, ρ, β = p\n    dx = zeros(3)\n    du1 = σ*(u[2] - u[1]) +\n    du2 = u[1]*(ρ - u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{Float64, 3}(du1, du2, du3)\nend","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"this function could not be used to auto-differentiate, as you would get an error when adding dual numbers to SVector{Float64}. Instead, leave the number type untyped, or use eltype(u) as the number type.","category":"page"},{"location":"ds/general/#Time-Evolution-of-Systems","page":"Dynamical System Definition","title":"Time Evolution of Systems","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DynamicalSystems.jl provides a convenient function for getting a trajectory of a system at equally spaced time points:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"trajectory","category":"page"},{"location":"ds/general/#DynamicalSystemsBase.trajectory","page":"Dynamical System Definition","title":"DynamicalSystemsBase.trajectory","text":"trajectory(ds::DynamicalSystem, T [, u]; kwargs...) -> dataset\n\nReturn a dataset that will contain the trajectory of the system, after evolving it for total time T, optionally starting from state u. See Dataset for info on how to use this object.\n\nA W×D dataset is returned, with W = length(t0:dt:T) with t0:dt:T representing the time vector (not returned) and D the system dimension.\n\nKeyword Arguments\n\ndt :  Time step of value output. For discrete systems it must be an integer. Defaults to 0.01 for continuous and 1 for discrete.\nTtr : Transient time to evolve the initial state before starting saving states.\nsave_idxs: Which variables to output in the dataset. By default all.\ndiffeq... : Remaining keyword arguments are propagated to the solvers of DifferentialEquations.jl. For example abstol = 1e-9.  Only valid for continuous systems. If you want to specify a solver, do so by using the name alg, e.g.: alg = Tsit5(), maxiters = 100000. This requires you to have been first using OrdinaryDiffEq to access the solvers. See DynamicalSystemsBase.CDS_KWARGS for default values. These keywords can also include callback for event handling.\n\n\n\n\n\n","category":"function"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Notice that if you want to do repeated evolutions of different states of a continuous system, you should use the integrator interface instead.","category":"page"},{"location":"ds/general/#Example","page":"Dynamical System Definition","title":"Example","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"using DynamicalSystems\nds = Systems.towel()","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 100)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"To get every 3-rd point of the trajectory, do","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 100; dt = 3)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"Identical syntax is used for continuous systems","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = Systems.lorenz()","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"tr = trajectory(ds, 10.0; dt = 0.01)","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"And a final example controlling the integrator accuracy:","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"ds = Systems.lorenz()\ntr = trajectory(ds, 10.0; dt = 0.1, abstol = 1e-9, reltol = 1e-9)","category":"page"},{"location":"ds/general/#Solution-precision-for-continuous-systems","page":"Dynamical System Definition","title":"Solution precision for continuous systems","text":"","category":"section"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"A numerical solution of an ODE is not the \"true\" solution, uniquely defined by a (well-defined) ODE and an initial condition. Especially for chaotic systems, where deviations are amplified exponentially, one is left worried if the numerical solutions truly are part of the system and can truly give insight in understanding the system.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"DifferentialEquations.jl offers a tool, called Uncertainty Quantification, which allows users to asses up to what time-scales the numerical solution is close to the \"true\" solution. For example, using the default solving parameters of DynamicalSystems.jl, the Lorenz system is accurate up to time t = 50.0.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"However, fortunately for us, there is not too much worry about the numerical solution diverging from the true solution. That is because of the shadowing theorem (or shadowing lemma):","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"quote: Shadowing Theorem\nAlthough a numerically computed chaotic trajectory diverges exponentially from the true trajectory with the same initial coordinates, there exists an errorless trajectory with a slightly different initial condition that stays near (\"shadows\") the numerically computed one.","category":"page"},{"location":"ds/general/","page":"Dynamical System Definition","title":"Dynamical System Definition","text":"This simply means that one can always numerically study chaos not only qualitatively but also quantitatively. For more information, see the book Chaos in Dynamical Systems by E. Ott, or the scholarpedia entry.","category":"page"},{"location":"embedding/reconstruction/#Delay-Coordinates-Embedding","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"","category":"section"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"A timeseries recorded in some manner from a dynamical system can be used to gain information about the dynamics of the entire state space of the system. This can be done by constructing a new state space from the timeseries. One method that can do this is what is known as delay coordinates embedding or delay coordinates reconstruction.","category":"page"},{"location":"embedding/reconstruction/#Timeseries-embedding","page":"Delay Coordinates Embedding","title":"Timeseries embedding","text":"","category":"section"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"Delay embeddings are done through embed:","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"embed","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.embed","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.embed","text":"embed(s, d, τ [, h])\n\nEmbed s using delay coordinates with embedding dimension d and delay time τ and return the result as a Dataset. Optionally use weight h, see below.\n\nHere τ > 0, use genembed for a generalized version.\n\nDescription\n\nIf τ is an integer, then the n-th entry of the embedded space is\n\n(s(n) s(n+tau) s(n+2tau) dots s(n+(d-1)tau))\n\nIf instead τ is a vector of integers, so that length(τ) == d-1, then the n-th entry is\n\n(s(n) s(n+tau1) s(n+tau2) dots s(n+taud-1))\n\nThe resulting set can have same invariant quantities (like e.g. lyapunov exponents) with the original system that the timeseries were recorded from, for proper d and τ. This is known as the Takens embedding theorem [Takens1981] [Sauer1991]. The case of different delay times allows embedding systems with many time scales, see[Judd1998].\n\nIf provided, h can be weights to multiply the entries of the embedded space. If h isa Real then the embedding is\n\n(s(n) h cdot s(n+tau) w^2 cdot s(n+2tau) dotsw^d-1 cdot s(n+γtau))\n\nOtherwise h can be a vector of length d-1, which the decides the weights of each entry directly.\n\nReferences\n\n[Takens1981] : F. Takens, Detecting Strange Attractors in Turbulence — Dynamical Systems and Turbulence, Lecture Notes in Mathematics 366, Springer (1981)\n\n[Sauer1991] : T. Sauer et al., J. Stat. Phys. 65, pp 579 (1991)\n\n[Judd1998]: K. Judd & A. Mees, Physica D 120, pp 273 (1998)\n\n[Farmer1988]: Farmer & Sidorowich, Exploiting Chaos to Predict the Future and Reduce Noise\"\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"Here are some examples of embedding a 3D continuous chaotic system:","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"using DynamicalSystems, PyPlot\n\nds = Systems.gissinger(ones(3))\ndata = trajectory(ds, 1000.0, dt = 0.05)\n\nxyz = columns(data)\n\nfigure(figsize = (12,10))\nk = 1\nfor i in 1:3\n    for τ in [5, 30, 100]\n        R = embed(xyz[i], 2, τ)\n        ax = subplot(3,3,k)\n        plot(R[:, 1], R[:, 2], color = \"C$(k-1)\", lw = 0.8)\n        title(\"var = $i, τ = $τ\")\n        global k+=1\n    end\nend\n\ntight_layout()\nsuptitle(\"2D reconstructed space\")\nsubplots_adjust(top=0.9)\nsavefig(\"simple_reconstruction.png\"); nothing # hide","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"(Image: )","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"note: `τ` and `dt`\nKeep in mind that whether a value of τ is \"reasonable\" for continuous systems depends on dt. In the above example the value τ=30 is good, only for the case of using dt = 0.05. For shorter/longer dt one has to adjust properly τ so that their product τ*dt is the same.","category":"page"},{"location":"embedding/reconstruction/#Embedding-Functors","page":"Delay Coordinates Embedding","title":"Embedding Functors","text":"","category":"section"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"The high level function embed utilize a low-level interface for creating embedded vectors on-the-fly. The high level interface simply loops over the low level interface. The low level interface is composed of the following two structures:","category":"page"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"DelayEmbedding\nτrange","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.DelayEmbedding","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.DelayEmbedding","text":"DelayEmbedding(γ, τ, h = nothing) → `embedding`\n\nReturn a delay coordinates embedding structure to be used as a function-like-object, given a timeseries and some index. Calling\n\nembedding(s, n)\n\nwill create the n-th delay vector of the embedded space, which has γ temporal neighbors with delay(s) τ. γ is the embedding dimension minus 1, τ is the delay time(s) while h are extra weights, as in embed for more.\n\nBe very careful when choosing n, because @inbounds is used internally. Use τrange!\n\n\n\n\n\n","category":"type"},{"location":"embedding/reconstruction/#DelayEmbeddings.τrange","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.τrange","text":"τrange(s, de::AbstractEmbedding)\n\nReturn the range r of valid indices n to create delay vectors out of s using de.\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/#Generalized-embeddings","page":"Delay Coordinates Embedding","title":"Generalized embeddings","text":"","category":"section"},{"location":"embedding/reconstruction/","page":"Delay Coordinates Embedding","title":"Delay Coordinates Embedding","text":"genembed\nGeneralizedEmbedding","category":"page"},{"location":"embedding/reconstruction/#DelayEmbeddings.genembed","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.genembed","text":"genembed(s, τs, js = ones(...); ws = nothing) → dataset\n\nCreate a generalized embedding of s which can be a timeseries or arbitrary Dataset, and return the result as a new Dataset.\n\nThe generalized embedding works as follows:\n\nτs denotes what delay times will be used for each of the entries of the delay vector. It is recommended that τs[1] = 0. τs is allowed to have negative entries as well.\njs denotes which of the timeseries contained in s will be used for the entries of the delay vector. js can contain duplicate indices.\nws are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by ws[i]). If provided, it is recommended that ws[1] = 1\n\nτs, js, ws are tuples (or vectors) of length D, which also coincides with the embedding dimension. For example, imagine input trajectory s = x y z where x y z are timeseries (the columns of the Dataset). If js = (1, 3, 2) and τs = (0, 2, -7) the created delay vector at each step n will be\n\n(x(n) z(n+2) y(n-7))\n\nUsing ws = (1, 0.5, 0.25) as well would create\n\n(x(n) frac12 z(n+2) frac14 y(n-7))\n\njs can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if s is a timeseries instead of a Dataset.\n\nSee also embed. Internally uses GeneralizedEmbedding.\n\n\n\n\n\n","category":"function"},{"location":"embedding/reconstruction/#DelayEmbeddings.GeneralizedEmbedding","page":"Delay Coordinates Embedding","title":"DelayEmbeddings.GeneralizedEmbedding","text":"GeneralizedEmbedding(τs, js = ones(length(τs)), ws = nothing) -> `embedding`\n\nReturn a delay coordinates embedding structure to be used as a functor. Given a timeseries or trajectory (i.e. Dataset) s and calling\n\nembedding(s, n)\n\nwill create the delay vector of the n-th point of s in the embedded space using generalized embedding (see genembed).\n\njs is ignored for timeseries input s (since all entries of js must be 1 in this case) and in addition js defaults to (1, ..., 1) for all τ.\n\nBe very careful when choosing n, because @inbounds is used internally. Use τrange!\n\n\n\n\n\n","category":"type"},{"location":"rqa/windowed/#Windowed-RQA","page":"Windowed RQA","title":"Windowed RQA","text":"","category":"section"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"In some cases, specially with very long time series, it may be suitable to perform the analysis at different points, considering only a limited window of data around each observation. The macro @windowed modifies the behaviour of the basic functions to calculate RQA parameters in that fashion. For instance, if rmat is a 10<sup>4</sup>&times;10<sup>4</sup> recurrence matrix, then","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed determinism(rmat, theiler=2, lmin=3) width=1000 step=100","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"will return a 91-element vector, such that each value is the determinism associated to a 1000-point fragment, starting at every 100 points (i.e. at 1, 101, &hellip; 9001).","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"The general syntax of that macro is:","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed expr w                 #1\n@windowed expr width=w step=s    #2","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"where:","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"expr is an expression used to calculate RQA parameters\nw is the width of the window for relevant data around each point.\ns is the step or distance between points where the calculations are done (starting in the first point).","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"To prevent syntax failures in the expansion of the macro, identify the RQA function (rqa, recurrencerate, determinism,...) directly by its name (avoid aliases), and use simple variable names (not complex expressions) for the arguments. On the other hand, the windowing options width and step can be given in any order. If step is ommitted, the calculations are done at every point, and the keyword width may be ommitted. (However, using step=1 may be computationally very expensive, and that will provide just overly redundant results around each point, so it is advisable to set step a relatively big fraction of the window width.)","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"The value returned by the macro will normally be a vector with the same type of numbers as expected by expr. In the case of @windowed rqa(...) ..., it will return a NamedTuple with a similar structure as in the default rqa function, but replacing scalar values by vectors.","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"The macro @windowed can also be applied to the functions that calculate recurrence matrices (RecurrenceMatrix, CrossRecurrenceMatrix, JointRecurrenceMatrix). That creates a sparse matrix with the same size as if the macro was not used, but only containing valid values for pairs of points that belong to the w first main diagonals (i.e. the separation in time from one point to the other is w or smaller). The &lsquo;step&rsquo; parameter s has no effect on those functions. Such &lsquo;windowed&rsquo; matrices can be used as the input arguments to calculate windowed RQA parameters, obtaining the same results as if the complete matrix was used (under certain conditions, see below). For instance, the following calculations are equivalent:","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"# Using complete matrix\nrmat = RecurrenceMatrix(x, 1.5)\nd = @windowed determinism(rmat) width=1000 step=250\n\n# Using windowed matrix\nrmatw = @windowed RecurrenceMatrix(x, 1.5) 1000\nd = @windowed determinism(rmatw) width=1000 step=250","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"The main difference between the two alternatives is that the second one will be faster and consume less memory. To ensure the equivalence between both approaches, the window width used to create the matrix must be greater than the one used to calculate the RQA parameters. Otherwise, the computation of RQA parameters might involve data points whose value is not well defined. Besides, the threshold to identify recurrences should be referred to a fixed scale. For instance:","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"rmat  =           RecurrenceMatrix(x, 0.1, scale=maximum)\nrmatw = @windowed RecurrenceMatrix(x, 0.1, scale=maximum) 1000\nrmat[1:1000,1:1000] == rmatw[1:1000,1:1000] # FALSE!!!","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"In this example, the 1000×1000 blocks of both matrices differ, because the threshold 0.1 is scaled with respect to the maximum distance between all points of x in rmat, but in the case of rmatw the scale changes between subsets of points. Something similar may happen if the recurrence matrix is calculated for a fixed recurrence rate (with the option fixedrate=true).","category":"page"},{"location":"rqa/windowed/#Docstring","page":"Windowed RQA","title":"Docstring","text":"","category":"section"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"@windowed","category":"page"},{"location":"rqa/windowed/#RecurrenceAnalysis.@windowed","page":"Windowed RQA","title":"RecurrenceAnalysis.@windowed","text":"@windowed(f(x,...), width)\n@windowed(f(x,...); width, step=1)\n\nCalculate windowed RQA parameters with a given window width.\n\nf(x,...) may be any call to RQA functions (e.g. recurrencerate, determinism, etc.), with x being a named variable that designates the recurrence matrix (do not use in-place calculations of the recurrence matrix). The results are returned in a vector with one value for each position of the window. By default the window moves at one-point intervals, but a longer step length may be specified, together with the window width, by declaring those options as keyword arguments.\n\nThis macro may be also used with recurrence matrix constructors (RecurrenceMatrix, CrossRecurrenceMatrix, JointRecurrenceMatrix), to create 'incomplete' matrices that are suitable for such windowed RQA. The values of the resulting matrix in the diagonals within the window width will be equal to those obtained without the @windowed macro, if the distances are not scaled (using the option scale=1, see RecurrenceMatrix). Outside the window width, the values of the recurrence matrix will be undefined (mostly zero).\n\n\n\n\n\n","category":"macro"},{"location":"rqa/windowed/#Alternative-syntax-for-@windowed","page":"Windowed RQA","title":"Alternative syntax for @windowed","text":"","category":"section"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"The following ways of using the macro @windowed are equivalent:","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"y = @windowed f(x,...) w\n@windowed y=f(x,...) w\ny = @windowed(f(x,...), w)\n@windowed(y=f(x,...), w)","category":"page"},{"location":"rqa/windowed/","page":"Windowed RQA","title":"Windowed RQA","text":"In all four cases, the width parameter w might have been qualified with a keyword as width=w. If the step parameter is added, the keyword qualification is mandatory.","category":"page"},{"location":"chaos/lyapunovs/#Lyapunov-Exponents","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"section"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Lyapunov exponents measure exponential rates of separation of nearby trajectories in the flow of a dynamical system. The Wikipedia and the Scholarpedia entries have a lot of valuable information about the history and usage of these quantities.","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"This page treats systems where the equations of motion are known. If instead you have numerical data, see numericallyapunov.","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"info: Performance depends on the solver\nNotice that the performance of functions that use ContinuousDynamicalSystems depend crucially on the chosen solver. Please see the documentation page on Choosing a solver for an in-depth discussion.","category":"page"},{"location":"chaos/lyapunovs/#Concept-of-the-Lyapunov-exponent","page":"Lyapunov Exponents","title":"Concept of the Lyapunov exponent","text":"","category":"section"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Before providing the documentation of the offered functionality, it is good to demonstrate exactly what are the Lyapunov exponents.","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For chaotic systems, nearby trajectories separate in time exponentially fast (while for stable systems they come close exponentially fast). This happens at least for small separations, and is demonstrated in the following sketch:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: ).","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"In this sketch lambda is the maximum Lyapunov exponent (and in general a system has as many exponents as its dimensionality).","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Let's demonstrate these concepts using a real system, the Henon map:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"beginaligned\nx_n+1 = 1 - ax_n^2 + y_n \ny_n+1 = bx_n\nendaligned","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Let's get a trajectory","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\nhenon = Systems.henon()\ntr1 = trajectory(henon, 100)\nsummary(tr1)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"and create one more trajectory that starts very close to the first one","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"u2 = get_state(henon) + (1e-9 * ones(dimension(henon)))\ntr2 = trajectory(henon, 100, u2)\nsummary(tr2)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"We now want to demonstrate how the distance between these two trajectories increases with time:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using LinearAlgebra: norm\n\nfigure(figsize=(8,5))\n\n# Plot the x-coordinate of the two trajectories:\nax1 = subplot(2,1,1)\nplot(tr1[:, 1], alpha = 0.5)\nplot(tr2[:, 1], alpha = 0.5)\nylabel(\"x\")\n\n# Plot their distance in a semilog plot:\nax2 = subplot(2,1,2, sharex = ax1)\nd = [norm(tr1[i] - tr2[i]) for i in 1:length(tr2)]\nylabel(\"d\"); xlabel(\"n\"); semilogy(d);\ntight_layout() # hide\nsavefig(\"demonstration.png\"); nothing # hide","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: )","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The initial slope of the d vs n plot (before the curve saturates) is approximately the maximum Lyapunov exponent!","category":"page"},{"location":"chaos/lyapunovs/#Lyapunov-Spectrum","page":"Lyapunov Exponents","title":"Lyapunov Spectrum","text":"","category":"section"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The function lyapunovspectrum calculates the entire spectrum of the Lyapunov exponents of a system:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovspectrum","category":"page"},{"location":"chaos/lyapunovs/#ChaosTools.lyapunovspectrum","page":"Lyapunov Exponents","title":"ChaosTools.lyapunovspectrum","text":"lyapunovspectrum(ds::DynamicalSystem, N [, k::Int | Q0]; kwargs...) -> λs\n\nCalculate the spectrum of Lyapunov exponents [Lyapunov1992] of ds by applying a QR-decomposition on the parallelepiped matrix N times. Return the spectrum sorted from maximum to minimum.\n\nThe third argument k is optional, and dictates how many lyapunov exponents to calculate (defaults to dimension(ds)). Instead of passing an integer k you can pass a pre-initialized matrix Q0 whose columns are initial deviation vectors (then k = size(Q0)[2]).\n\nKeyword Arguments\n\nu0 = get_state(ds) : State to start from.\nTtr = 0 : Extra \"transient\" time to evolve the system before application of the algorithm. Should be Int for discrete systems. Both the system and the deviation vectors are evolved for this time.\ndt = 1 : Time of individual evolutions between successive orthonormalization steps. For continuous systems this is approximate.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nThe method we employ is \"H2\" of [Geist1990], originally stated in [Benettin1980]. The deviation vectors defining a D-dimensional parallepiped in tangent space are evolved using the tangent dynamics of the system. A QR-decomposition at each step yields the local growth rate for each dimension of the parallepiped. The growth rates are then averaged over N successive steps, yielding the lyapunov exponent spectrum (at each step the parallepiped is re-normalized).\n\nPerformance Notes\n\nThis function uses a tangent_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nlyapunovspectrum(tinteg, N, dt::Real, Ttr::Real)\n\nIf you want to obtain the convergence timeseries of the Lyapunov spectrum, use the method\n\nChaosTools.lyapunovspectrum_convergence(tinteg, N, dt, Ttr)\n\n(not exported).\n\n[Lyapunov1992]: A. M. Lyapunov, The General Problem of the Stability of Motion, Taylor & Francis (1992)\n\n[Geist1990]: K. Geist et al., Progr. Theor. Phys. 83, pp 875 (1990)\n\n[Benettin1980]: G. Benettin et al., Meccanica 15, pp 9-20 & 21-30 (1980)\n\n\n\n\n\n","category":"function"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"As you can see, the documentation string is detailed and self-contained. For example, the Lyapunov spectrum of the folded towel map is calculated as:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems\n\nds = Systems.towel()\nλλ = lyapunovspectrum(ds, 10000)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Similarly, for a continuous system, e.g. the Lorenz system, you would do:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lor = Systems.lorenz(ρ = 32.0) #this is not the original parameter!\nλλ = lyapunovspectrum(lor, 10000, dt = 0.1)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovspectrum is also very fast:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using BenchmarkTools\nds = Systems.towel()\n@btime lyapunovspectrum($ds, 2000);","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"  237.226 μs (45 allocations: 4.27 KiB)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Here is an example of plotting the exponents of the Henon map for various parameters:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\n\nhe = Systems.henon()\nas = 0.8:0.005:1.225; λs = zeros(length(as), 2)\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    λs[i, :] .= lyapunovspectrum(he, 10000; Ttr = 500)\nend\n\nfigure()\nplot(as, λs); xlabel(\"\\$a\\$\"); ylabel(\"\\$\\\\lambda\\$\")\ntight_layout() # hide\nsavefig(\"heλ.png\"); nothing # hide","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"(Image: )","category":"page"},{"location":"chaos/lyapunovs/#Maximum-Lyapunov-Exponent","page":"Lyapunov Exponents","title":"Maximum Lyapunov Exponent","text":"","category":"section"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"It is possible to get only the maximum Lyapunov exponent simply by giving 1 as the third argument of lyapunovspectrum. However, there is a second algorithm that allows you to do the same thing, which is offered by the function lyapunov:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunov","category":"page"},{"location":"chaos/lyapunovs/#ChaosTools.lyapunov","page":"Lyapunov Exponents","title":"ChaosTools.lyapunov","text":"lyapunov(ds::DynamicalSystem, Τ; kwargs...) -> λ\n\nCalculate the maximum Lyapunov exponent λ using a method due to Benettin [Benettin1976], which simply evolves two neighboring trajectories (one called \"given\" and one called \"test\") while constantly rescaling the test one. T  denotes the total time of evolution (should be Int for discrete systems).\n\nKeyword Arguments\n\nu0 = get_state(ds) : Initial condition.\nTtr = 0 : Extra \"transient\" time to evolve the trajectories before starting to measure the expontent. Should be Int for discrete systems.\nd0 = 1e-9 : Initial & rescaling distance between the two neighboring trajectories.\nupper_threshold = 1e-6 : Upper distance threshold for rescaling.\nlower_threshold = 1e-12 : Lower distance threshold for rescaling (in order to  be able to detect negative exponents).\ndt = 1 : Time of evolution between each check of distance exceeding the thresholds. For continuous systems this is approximate.\ninittest = (u1, d0) -> u1 .+ d0/sqrt(D) : A function that given (u1, d0) initializes the test state with distance d0 from the given state u1 (D is the dimension of the system). This function can be used when you want to avoid the test state appearing in a region of the phase-space where it would have e.g. different energy or escape to infinity.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nTwo neighboring trajectories with initial distance d0 are evolved in time. At time t_i their distance d(t_i) either exceeds the upper_threshold, or is lower than lower_threshold, which initializes a rescaling of the test trajectory back to having distance d0 from the given one, while the rescaling keeps the difference vector along the maximal expansion/contraction direction: u_2 to u_1+(u_2u_1)(d(t_i)d_0).\n\nThe maximum Lyapunov exponent is the average of the time-local Lyapunov exponents\n\nlambda = frac1t_n - t_0sum_i=1^n\nlnleft( a_i right)quad a_i = fracd(t_i)d_0\n\nPerformance Notes\n\nThis function uses a parallel_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nlyapunov(pinteg, T, Ttr, dt, d0, ut, lt)\n\n[Benettin1976]: G. Benettin et al., Phys. Rev. A 14, pp 2338 (1976)\n\n\n\n\n\n","category":"function"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For example:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DynamicalSystems, PyPlot\nhenon = Systems.henon()\nλ = lyapunov(henon, 10000, d0 = 1e-7, upper_threshold = 1e-4, Ttr = 100)","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The same is done for continuous systems:","category":"page"},{"location":"chaos/lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lor = Systems.lorenz(ρ = 32)\nλ = lyapunov(lor, 10000.0, dt = 10.0, Ttr = 100.0)","category":"page"},{"location":"chaos/choosing/#Choosing-a-solver","page":"Choosing a solver","title":"Choosing a solver","text":"","category":"section"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"ContinuousDynamicalSystems are evolved using solvers from DifferentialEquations.jl. In this page we discuss the importance of which solver to choose.","category":"page"},{"location":"chaos/choosing/#Default-Solver","page":"Choosing a solver","title":"Default Solver","text":"","category":"section"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"The default solver is:","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"using DynamicalSystems\nDynamicalSystemsBase.DEFAULT_SOLVER","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"which is a Runge-Kutta-like solver. The number in the solver's name is the \"order\" of the solver.","category":"page"},{"location":"chaos/choosing/#Speed-of-a-solver","page":"Choosing a solver","title":"Speed of a solver","text":"","category":"section"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"Estimating a given solver's performance for a particular problem is not trivial. The following are general rules of thumb:","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"Higher order solvers call the equations of motion function more times per step.\nHigher order solvers can cover larger timespans per step.\nHigher order solvers do better at small tolerances.","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"This means that there is a delicate balance between how expensive is your function and how large of a step a solver can take while it is still efficient. In general you want to strike a point of taking large steps but also not calling the function exceedingly often.","category":"page"},{"location":"chaos/choosing/#How-do-I-pick?","page":"Choosing a solver","title":"How do I pick?","text":"","category":"section"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"The answer to this question is easy: benchmarks!","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"Here is a simple case: let's compute the Lyapunov spectrum of the Lorenz system using lyapunovspectrum:","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"ds = Systems.lorenz()\ntols = (abstol = 1e-6, reltol = 1e-6)\nlyapunovspectrum(ds, 2000; Ttr = 100.0, tols...)","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"The above uses the default solver. Let's now benchmark using two different solvers, SimpleATsit5 and Vern9. Since the SimpleATsit5 case is of lower order, naively one might think it is faster because it makes less function calls. This argument is not necessarily true though.","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"It is important to understand that when calling lyapunovspectrum(ds, 2000) you want the system (and the tangent space) to be evolved so that it reaches a total time of 2000*dt, which by default is 2000.0 units of time. Even though SimpleATsit5 requires less function calls per step, Vern9 can cover larger timespans per step.","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"Here are the numbers:","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"using BenchmarkTools, OrdinaryDiffEq, SimpleDiffEq, Statistics\nb1 = @benchmark lyapunovspectrum(ds, 2000; alg = SimpleATsit5(), Ttr = 100.0, tols...);\nb2 = @benchmark lyapunovspectrum(ds, 2000; alg = Vern9(),        Ttr = 100.0, tols...);\nprintln(\"Timing for SimpleATsit5:\")\nprintln(mean(b1))\nprintln(\"Timing for Vern9:\")\nprintln(mean(b2))","category":"page"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"As you can see Vern9 is faster in doing the entire computation! Of course this does not have to be universally true. It is true for the Lorenz system, but for your specific system you should do dedicated benchmarks!","category":"page"},{"location":"chaos/choosing/#DifferentialEquations.jl","page":"Choosing a solver","title":"DifferentialEquations.jl","text":"","category":"section"},{"location":"chaos/choosing/","page":"Choosing a solver","title":"Choosing a solver","text":"For more info about the possible solvers be sure to head over to the documentation of DifferentialEquations.jl!","category":"page"},{"location":"chaos/fractaldim/#Fractal-Dimension","page":"Fractal Dimension","title":"Fractal Dimension","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"There are numerous methods that one can use to calculate a so-called \"dimension\" of a dataset which in the context of dynamical systems is called the Fractal dimension. Several variants of a computationally feasible fractal dimension exist and a simple usage example is shown in the Fractal dimension example subsection.","category":"page"},{"location":"chaos/fractaldim/#Generalized-dimension","page":"Fractal Dimension","title":"Generalized dimension","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"Based on the definition of the Generalized entropy, one can calculate an appropriate dimension, called generalized dimension:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"generalized_dim","category":"page"},{"location":"chaos/fractaldim/#ChaosTools.generalized_dim","page":"Fractal Dimension","title":"ChaosTools.generalized_dim","text":"generalized_dim(dataset [, sizes]; q = 1, base = MathConstants.e) -> D_α\n\nReturn the α order generalized dimension of the dataset, by calculating the genentropy for each ε ∈ sizes.\n\nThe case of α=0 is often called \"capacity\" or \"box-counting\" dimension.\n\nDescription\n\nThe returned dimension is approximated by the (inverse) power law exponent of the scaling of the genentropy versus the box size ε, where ε ∈ sizes.\n\nCalling this function performs a lot of automated steps:\n\nA vector of box sizes is decided by calling sizes = estimate_boxsizes(dataset), if sizes is not given.\nFor each element of sizes the appropriate entropy is calculated, through h = genentropy.(Ref(dataset), sizes; α, base). Let x = -log.(sizes).\nThe curve h(x) is decomposed into linear regions, using linear_regions(x, h).\nThe biggest linear region is chosen, and a fit for the slope of that region is performed using the function linear_region, which does a simple linear regression fit using linreg. This slope is the return value of generalized_dim.\n\nBy doing these steps one by one yourself, you can adjust the keyword arguments given to each of these function calls, refining the accuracy of the result.\n\nThe following aliases are provided:\n\nα = 0 : boxcounting_dim, capacity_dim\nα = 1 : information_dim\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"danger: Be wary when using `generalized_dim`\nAs stated clearly by the documentation string, calling generalized_dim performs a lot of automated steps by calling other functions (see below) with default arguments. It is actually more like a convenient bundle than an actual function and therefore you should be careful when considering the validity of the returned number.","category":"page"},{"location":"chaos/fractaldim/#Fractal-dimension-example","page":"Fractal Dimension","title":"Fractal dimension example","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"For an example of using entropies to compute the dimension of an attractor let's use everyone's favorite system:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"using DynamicalSystems, PyPlot\nlor = Systems.lorenz()","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"Our goal is to compute entropies for many different partition sizes ε, so let's get down to it:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"tr = trajectory(lor, 100.0; Ttr = 10.0)\n\nες = ℯ .^ (-3.5:0.5:3.5) # semi-random guess\nHs = genentropy.(Ref(tr), ες; q = 1)","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"xs = @. -log(ες)\nfigure()\nplot(xs, Hs)\nylabel(\"\\$H_1\\$\")\nxlabel(\"\\$-\\\\log (\\\\epsilon)\\$\");\ntight_layout(pad=0.3) # hide\nsavefig(\"genentropy1.png\"); nothing # hide","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"(Image: )","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"The slope of the linear scaling region of the above plot is the generalized dimension (of order q = 1) for the attractor of the Lorenz system.","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"Given that we see the plot, we can estimate where the linear scaling region starts and ends. However, we can use the function linear_region to get an estimate of the result as well. First let's visualize what it does:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"lrs, slopes = linear_regions(xs, Hs, tol = 0.25)\n\nfigure()\nfor i in 1:length(lrs)-1\n    plot(xs[lrs[i]:lrs[i+1]], Hs[lrs[i]:lrs[i+1]], marker = \"o\")\nend\nylabel(\"\\$H_1\\$\")\nxlabel(\"\\$-\\\\log (\\\\epsilon)\\$\");\nsavefig(\"genentropy2.png\"); nothing # hide","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"(Image: )","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"The linear_region function  computes the slope of the largest region:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"Δ = linear_region(xs, Hs)[2]","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"This result is an approximation of the information dimension (because we used q = 1) of the Lorenz attractor.","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"The above pipeline is bundled in generalized_dim. For example, the dimension of the strange attractor of the Systems.henon map, following the above approach but taking automated steps, is:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"using DynamicalSystems\nhen = Systems.henon()\ntr = trajectory(hen, 200000)\nD_hen = generalized_dim(tr; q = 1)","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"As a side note, be sure that you have enough data points, otherwise the values you will get will never be correct, as is demonstrated by J.-P. Eckmann and D. Ruelle (see Physica D 56, pp 185-187 (1992)).","category":"page"},{"location":"chaos/fractaldim/#Linear-scaling-regions","page":"Fractal Dimension","title":"Linear scaling regions","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"And other utilities, especially linreg, used in both [generalized_dim] and grassberger.","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"linear_regions\nlinear_region\nlinreg\nestimate_boxsizes","category":"page"},{"location":"chaos/fractaldim/#ChaosTools.linear_regions","page":"Fractal Dimension","title":"ChaosTools.linear_regions","text":"linear_regions(x, y; dxi::Int = 1, tol = 0.25) -> (lrs, tangents)\n\nIdentify regions where the curve y(x) is linear, by scanning the x-axis every dxi indices sequentially (e.g. at x[1] to x[5], x[5] to x[10], x[10] to x[15] and so on if dxi=5).\n\nIf the slope (calculated via linear regression) of a region of width dxi is approximatelly equal to that of the previous region, within tolerance tol, then these two regions belong to the same linear region.\n\nReturn the indices of x that correspond to linear regions, lrs, and the correct tangents at each region (obtained via a second linear regression at each accumulated region).\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.linear_region","page":"Fractal Dimension","title":"ChaosTools.linear_region","text":"linear_region(x, y; kwargs...) -> ((ind1, ind2), slope)\n\nCall linear_regions and identify and return the largest linear region and its slope. The region starts and stops at x[ind1:ind2].\n\nThe keywords dxi, tol are propagated as-is to linear_regions. The keyword ignore_saturation = true ignores saturation that (typically) happens at the final points of the curve y(x), where the curve flattens out.\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.linreg","page":"Fractal Dimension","title":"ChaosTools.linreg","text":"linreg(x, y) -> a, b\n\nPerform a linear regression to find the best coefficients so that the curve: z = a + b*x has the least squared error with y.\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.estimate_boxsizes","page":"Fractal Dimension","title":"ChaosTools.estimate_boxsizes","text":"estimate_boxsizes(A::Dataset; kwargs...)\n\nReturn k exponentially spaced values: base .^ range(lower + w, upper + z; length = k), that are a good estimate for sizes ε that are used in calculating a Fractal Dimension.\n\nLet d₋ be the minimum pair-wise distance in A and d₊ the length of the diagonal of the hypercube that contains A. Then lower = log(base, d₋) and upper = log(base, d₊). Because by default w=1, z=-1, the returned sizes are an order of mangitude larger than the minimum distance, and an order of magnitude smaller than the maximum distance.\n\nKeywords\n\nw = 1, z = -1, k = 20 : as explained above.\nbase = MathConstants.e : the base used in the log function.\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#Correlation-dimension","page":"Fractal Dimension","title":"Correlation dimension","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"correlationsum\ngrassberger\ntakens_best_estimate\nkernelprob","category":"page"},{"location":"chaos/fractaldim/#ChaosTools.correlationsum","page":"Fractal Dimension","title":"ChaosTools.correlationsum","text":"correlationsum(X, ε::Real; w = 0, norm = Euclidean(), q = 2) → C_q(ε)\n\nCalculate the q-order correlation sum of X (Dataset or timeseries) for a given radius ε and norm, using the formula:\n\nC_2(epsilon) = frac2(N-w)(N-w-1)sum_i=1^Nsum_j=1+w+i^N B(X_i - X_j  epsilon)\n\nfor q=2 and\n\nC_q(epsilon) = leftfrac1alpha sum_i=w+1^N-wleftsum_ji-j  w B(X_i - X_j  epsilon)right^q-1right^1(q-1)\n\nwhere\n\nalpha = (N-2w)(N-2w-1)^(q-1)\n\nfor q≠2, where N is its length and B gives 1 if the argument is true. w is the Theiler window. If ε is a vector its values have to be ordered. See the article of Grassberger for the general definition [Grassberger2007] and the book \"Nonlinear Time Series Analysis\" [Kantz2003], Ch. 6, for a discussion around w and choosing best values and Ch. 11.3 for the explicit definition of the q-order correlationsum.\n\ncorrelationsum(X, εs::AbstractVector; w, norm, q) → C_q(ε)\n\nIf εs is a vector, C_q is calculated for each ε ∈ εs. If also q=2, some strong optimizations are done, but this requires the allocation a matrix of size N×N. If this is larger than your available memory please use instead:\n\n[correlationsum(..., ε) for ε in εs]\n\nSee grassberger for more. See also takens_best_estimate.\n\n[Grassberger]: Peter Grassberger (2007) Grassberger-Procaccia algorithm. Scholarpedia, 2(5):3043.\n\n[Kantz]: Kantz, H., & Schreiber, T. (2003). More about invariant quantities. In Nonlinear Time Series Analysis (pp. 197-233). Cambridge: Cambridge University Press.\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.grassberger","page":"Fractal Dimension","title":"ChaosTools.grassberger","text":"grassberger(data, εs = estimate_boxsizes(data); kwargs...) → D_C\n\nUse the method of Grassberger and Proccacia[Grassberger1983], and the correction by Theiler[Theiler1986], to estimate the correlation dimension D_C of the given data.\n\nThis function does something extrely simple:\n\ncm = correlationsum(data, εs; kwargs...)\nreturn linear_region(log.(sizes), log(cm))[2]\n\ni.e. it calculates correlationsum for various radii and then tries to find a linear region in the plot of the log of the correlation sum versus log(ε). See generalized_dim for a more thorough explanation.\n\nSee also takens_best_estimate.\n\n[Grassberger1983]: Grassberger and Proccacia, Characterization of strange attractors, PRL 50 (1983)\n\n[Theiler1986]: Theiler, Spurious dimension from correlation algorithms applied to limited time-series data. Physical Review A, 34\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.takens_best_estimate","page":"Fractal Dimension","title":"ChaosTools.takens_best_estimate","text":"takens_best_estimate(X, εmax, metric = Chebyshev(),εmin = 0) → D_C, D_C_95u, D_C_95l\n\nUse the so-called \"Takens' best estimate\" [Takens1985][Theiler1988] method for estimating the correlation dimension D_C and the upper (D_C_95u) and lower (D_C_95l) confidence limit for the given dataset X.\n\nThe original formula is\n\nD_C approx fracC(epsilon_textmax)int_0^epsilon_textmax(C(epsilon)  epsilon)  depsilon\n\nwhere C is the correlationsum and epsilon_textmax is an upper cutoff. Here we use the later expression\n\nD_C approx - frac1etaquad eta = frac1(N-1)^*sum_i j^*log(X_i - X_j  epsilon_textmax)\n\nwhere the sum happens for all i j so that i  j and X_i - X_j  epsilon_textmax. In the above expression, the bias in the original paper has already been corrected, as suggested in [Borovkova1999].\n\nThe confidence limits are estimated from the log-likelihood function by finding the values of D_C where the function has fallen by 2 from its maximum, see e.g. [Barlow] chapter 5.3 Because the CLT does not apply (no independent measurements), the limits are not neccesarily symmetric.\n\nAccording to [Borovkova1999], introducing a lower cutoff εmin can make the algorithm more stable (no divergence), this option is given but defaults to zero.\n\nIf X comes from a delay coordinates embedding of a timseries x, a recommended value for epsilon_textmax is std(x)/4.\n\n[Takens1985]: Takens, On the numerical determination of the dimension of an attractor, in: B.H.W. Braaksma, B.L.J.F. Takens (Eds.), Dynamical Systems and Bifurcations, in: Lecture Notes in Mathematics, Springer, Berlin, 1985, pp. 99–106.\n\n[Theiler1988]: Theiler, Lacunarity in a best estimator of fractal dimension. Physics Letters A, 133(4–5)\n\n[Borovkova1999]: Borovkova et al., Consistency of the Takens estimator for the correlation dimension. The Annals of Applied Probability, 9, 05 1999.\n\n[Barlow]: Barlow, R., Statistics - A Guide to the Use of Statistical Methods in the Physical Sciences. Vol 29. John Wiley & Sons, 1993\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#ChaosTools.kernelprob","page":"Fractal Dimension","title":"ChaosTools.kernelprob","text":"kernelprob(X, ε; norm = Euclidean()) → p::Probabilities\n\nAssociate each point in X (Dataset or timesries) with a probability p using the \"kernel estimation\" (also called \"nearest neighbor kernel estimation\" and other names):\n\np_j = frac1Nsum_i=1^N B(X_i - X_j  epsilon)\n\nwhere N is its length and B gives 1 if the argument is true.\n\nSee also genentropy and correlationsum. kernelprob is equivalent with probabilities(X, NaiveKernel(ε, TreeDistance(norm))).\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/#Kaplan-Yorke-Dimension","page":"Fractal Dimension","title":"Kaplan-Yorke Dimension","text":"","category":"section"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"kaplanyorke_dim","category":"page"},{"location":"chaos/fractaldim/#ChaosTools.kaplanyorke_dim","page":"Fractal Dimension","title":"ChaosTools.kaplanyorke_dim","text":"kaplanyorke_dim(λs::AbstractVector)\n\nCalculate the Kaplan-Yorke dimension, a.k.a. Lyapunov dimension[Kaplan1970].\n\nDescription\n\nThe Kaplan-Yorke dimension is simply the point where cumsum(λs) becomes zero (interpolated):\n\n D_KY = k + fracsum_i=1^k lambda_ilambda_k+1quad k = max_j left sum_i=1^j lambda_i  0 right\n\nIf the sum of the exponents never becomes negative the function will return the length of the input vector.\n\nUseful in combination with lyapunovspectrum.\n\n[Kaplan1970]: J. Kaplan & J. Yorke, Chaotic behavior of multidimensional difference equations, Lecture Notes in Mathematics vol. 730, Springer (1979)\n\n\n\n\n\n","category":"function"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"Notice that calling this function requires you to pass the Lyapunov exponents in an ordered vector form (largest to smallest). Example:","category":"page"},{"location":"chaos/fractaldim/","page":"Fractal Dimension","title":"Fractal Dimension","text":"using DynamicalSystems\nhen = Systems.henon()\nD_kp = kaplanyorke_dim(lyapunovspectrum(hen, 100000))","category":"page"},{"location":"rqa/quantification/#Recurrence-Quantification-Analysis","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"A RecurrenceMatrix can be analyzed in several ways to yield information about the dynamics of the trajectory. All these various measures and functions are collectively called \"Recurrence Quantification Analysis\" (RQA).","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"To understand how each measure can be useful, we suggest to see the review articles listed in our documentation strings, namely:","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007).\nN. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"You can also check the wikipedia page for Recurrence quantification analysis.","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"The functions described in this page all accept a recurrence matrix (x), see RecurrenceMatrix.","category":"page"},{"location":"rqa/quantification/#RQA-Measures","page":"Recurrence Quantification Analysis","title":"RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/#All-in-one-Bundle","page":"Recurrence Quantification Analysis","title":"All-in-one Bundle","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"In case you need all of the RQA-related functions (see below) and you don't want to write 10 lines of code to compute them all (since they are so many) we provide an all-in-one function that computes all of them and returns a dictionary with the results!","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"rqa","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.rqa","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rqa","text":"rqa(R; kwargs...)\n\nCalculate all RQA parameters of a recurrence matrix R. See the functions referred to below for the definition of the different parameters and the default values of the arguments. Using this function is much more efficient than calling all individual functions one by one.\n\nReturn\n\nThe returned value contains the following entries, which can be retrieved as from a dictionary (e.g. results[:RR], etc.):\n\n:RR: recurrence rate (see recurrencerate)\n:DET: determinsm (see determinism)\n:L: average length of diagonal structures (see dl_average)\n:Lmax: maximum length of diagonal structures (see dl_max)\n:DIV: divergence (see divergence)\n:ENTR: entropy of diagonal structures (see dl_entropy)\n:TREND: trend of recurrences (see trend)\n:LAM: laminarity (see laminarity)\n:TT: trapping time (see trappingtime)\n:Vmax: maximum length of vertical structures (see vl_max)\n:VENTR: entropy of vertical structures (see vl_entropy)\n:MRT: mean recurrence time (see meanrecurrencetime)\n:RTE recurrence time entropy (see rt_entropy)\n:NMPRT: number of the most probable recurrence time (see nmprt)\n\nAll the parameters returned by rqa are Float64 numbers, even for parameters like :Lmax, :Vmax or :NMPRT which are integer values. In the case of empty histograms (e.g. no existing vertical lines less than the keyword lminvert) the average and maximum values (:L, :Lmax, :TT, :Vmax, :MRT) are returned as 0.0 but their respective entropies (:ENTR, :VENTR, :RTE) are returned as NaN.\n\nKeyword Arguments\n\nStandard keyword arguments are the ones accepted by the functions listed below, i.e. theiler, lmin, and border:\n\ntheiler is used to define a \"Theiler window\" around the central diagonal or \"line of identity\" (LOI): a region of points that are excluded in the calculation of RQA parameters, in order to rule out self-recurrences and apparent recurrences for smooth or high resolution data. The LOI is excluded by default for matrices of the types RecurrenceMatrix or JointRecurrenceMatrix, but it is included for matrices of the type CrossRecurrenceMatrix. theiler=0 means that the whole matrix is scanned for lines. theiler=1 means that the LOI is excluded. In general, theiler=n means that the n central diagonals are excluded (at both sides of the LOI, i.e. actually 2n-1 diagonals are excluded).\nlmin is used to define the minimum line length in the parameters that describe the distributions of diagonal or vertical lines (it is set as 2 by default).\nborder is used to avoid border effects in the calculation of :TREND (cf. trend).\n\nIn addition theilerdiag, lmindiag may be used to declare specific values that override the values of theiler and lmin in the calculation of parameters related to diagonal structures. Likewise, theilervert and lminvert can be used for the calculation of parameters related to vertical structures.\n\nThe keyword argument onlydiagonal (false by default) can be set to true in order to restrict the analysis to the recurrence rate and the parameters related to diagonal structures (:RR, :DET, :L, :Lmax, :DIV and :ENTR), which makes this function slightly faster.\n\nTransitional note on the returned type\n\nIn older versions, the rqa function returned a NamedTuple, and in future versions it is planned to return a Dict instead. In both cases, the results can be indexed with square brackets and Symbol keys, as result[:RR], result[:DET], etc. However, named tuples can also be indexed with \"dot syntax\", e.g. result.RR, whereas this will not be possible with dictionaries, and there are other differences in the indexing and iteration of those two types.\n\nIn order to facilitate the transition between versions, this function currently returns a RQA object that essentially works as a dictionary, but can also be indexed with the dot syntax (logging a deprecation warning). The returned type can also be specified as a first argument of rqa in order to replicate the output of different versions:\n\nrqa(NamedTuple, R...) to obtain the output of the older version (as in 1.3).\nrqa(Dict, R...) to obtain the output of the planned future version.\nrqa(RQA, R...) to obtain the default current output (same as rqa(R...))\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"note: Return values for empty histograms\nIt may be the case that for a given recurrence matrix some structures do not exist at all. For example there are recurrence matrices that have no vertical lengths (or no vertical lengths with length less than lmin). In such cases the behavior of our RQA pipeline is the following:Quantities that represent maximum or average values are 0.0.\nQuantities that represent entropies are NaN.","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"See also the @windowed macro for a windowed version of rqa.","category":"page"},{"location":"rqa/quantification/#Classical-RQA-Measures","page":"Recurrence Quantification Analysis","title":"Classical RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"recurrencerate\ndeterminism\ndl_average\ndl_max\ndl_entropy\ndivergence\ntrend","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.recurrencerate","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencerate","text":"recurrencerate(R[; theiler])\n\nCalculate the recurrence rate of the recurrence matrix R.\n\nDescription\n\nThe recurrence rate is calculated as:\n\nRR = frac1S sum R\n\nwhere S is the size of R or the region of R with potential recurrent points. There is not a unique definition of that denominator, which is defined as the full size of the matrix in many sources (e.g. [1]), whereas in others it is adjusted to remove the points of the LOI when they are excluded from the count [2,3].\n\nFor matrices of type RecurrenceMatrix or JointRecurrenceMatrix, where the points around the central diagonal are usually excluded, the denominator is adjusted to the size of the matrix outside the Theiler window (by default equal to the LOI, and adjustable with the keyword argument theiler; see rqa for details). For matrices of type CrossRecurrenceMatrix, where normally all points are analyzed, the denominator is always the full size of the matrix, regardless of the Theiler window that might be defined (none by default).\n\nHint: to reproduce the calculations done following the formulas that use the full size of the matrix in the denominator, use CrossRecurrenceMatrix(s,s,ε) to define the recurrence matrix, instead of RecurrenceMatrix(s,ε), setting theiler=1 (or theiler=n in general) to explicitly exclude the LOI or other diagonals around it.\n\nReferences\n\n[1] : N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007). DOI:10.1016/j.physrep.2006.11.001\n\n[2] : C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 26-94 (2005). URL: https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf\n\n[3] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.determinism","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.determinism","text":"determinism(R[; lmin=2, theiler])\n\nCalculate the determinism of the recurrence matrix R:\n\nDescription\n\nThe determinism is calculated as:\n\nDET = fracsum_l=lminl P(l)sum_l=1l P(l) =\nfracsum_l=lminl P(l)sum R\n\nwhere l stands for the lengths of diagonal lines in the matrix, and P(l) is the number of lines of length equal to l.\n\nlmin is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_average","text":"dl_average(R[; lmin=2, theiler])\n\nCalculate the average of the diagonal lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_max","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_max","text":"dl_max(R[; lmin=2, theiler])\n\nCalculate the longest diagonal line contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.dl_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_entropy","text":"dl_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the diagonal lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.divergence","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.divergence","text":"divergence(R[; theiler])\n\nCalculate the divergence of the recurrence matrix R (actually the inverse of dl_max).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.trend","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trend","text":"trend(R[; border=10, theiler])\n\nCalculate the trend of recurrences in the recurrence matrix R.\n\nDescription\n\nThe trend is the slope of the linear regression that relates the density of recurrent points in the diagonals parallel to the LOI and the distance between those diagonals and the LOI. It quantifies the degree of system stationarity, such that in recurrence plots where points \"fade away\" from the central diagonal, the trend will have a negative value.\n\nIt is calculated as:\n\nTREND = 10^3fracsum_d=tau^tildeNdeltadleft(RRd-langle RRdrangleright)sum_d=tau^tildeNdeltad^2\n\nwhere RRd is the local recurrence rate of the diagonal d, deltad is a balanced measure of the distance between that diagonal and the LOI, tau is the Theiler window (number of central diagonals that are excluded), and tildeN is the number of the outmost diagonal that is included.\n\nThis parameter is expressed in units of variation recurrence rate every 1000 data points, hence the factor 10^3 in the formula [1].\n\nThe 10 outermost diagonals (counting from the corners of the matrix) are excluded by default to avoid \"border effects\". Use the keyword argument border to define a different number of excluded lines, and theiler to define the size of the Theiler window (see rqa for details).\n\nNote: In rectangular cross-recurrence plots (i.e. when the time series that originate them are not of the same length), the limits of the formula for TREND are not clearly defined. For the sake of consistency, this function limits the calculations to the biggest square matrix that contains the LOI.\n\nReferences\n\n[1] C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 2005, 26-94. https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Extended-RQA-Measures","page":"Recurrence Quantification Analysis","title":"Extended RQA Measures","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"laminarity\ntrappingtime\nvl_average\nvl_max\nvl_entropy","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.laminarity","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.laminarity","text":"laminarity(R[; lmin=2, theiler])\n\nCalculate the laminarity of the recurrence matrix R.\n\nDescription\n\nThe laminarity is calculated as:\n\nLAM = fracsum_v=lminv P(l)sum_v=1v P(v) =\nfracsum_v=lminv P(l)sum R\n\nwhere v stands for the lengths of vertical lines in the matrix, and P(v) is the number of lines of length equal to v.\n\nlmin is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.trappingtime","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trappingtime","text":"trappingtime(R[; lmin=2, theiler])\n\nCalculate the trapping time of the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nThe trapping time is the average of the vertical line structures and thus equal to vl_average.\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_average","text":"vl_average(R[; lmin=2, theiler])\n\nCalculate the average of the vertical lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_max","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_max","text":"vl_max(R[; lmin=2, theiler])\n\nCalculate the longest vertical line contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.vl_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_entropy","text":"vl_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the vertical lines contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Recurrence-Time-Measures","page":"Recurrence Quantification Analysis","title":"Recurrence Time Measures","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"meanrecurrencetime\nnmprt\nrt_entropy\nrt_average","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.meanrecurrencetime","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.meanrecurrencetime","text":"meanrecurrencetime(R[; lmin=2, theiler])\n\nCalculate the mean recurrence time of the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nEquivalent to rt_average.\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.nmprt","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.nmprt","text":"nmprt(R[; lmin=2, theiler])\n\nCalculate the number of the most probable recurrence time (NMPRT), ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\nThis number indicates how many times the system has recurred using the recurrence time that appears most frequently, i.e it is the maximum value of the histogram of recurrence times [1].\n\nReferences\n\n[1] : E.J. Ngamga et al. \"Recurrence analysis of strange nonchaotic dynamics\", Physical Review E, 75(3), 036222(1-8) (2007) DOI:10.1103/physreve.75.036222\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.rt_entropy","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_entropy","text":"rt_entropy(R[; lmin=2, theiler])\n\nCalculate the Shannon entropy of the recurrence times contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#RecurrenceAnalysis.rt_average","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_average","text":"rt_average(R[; lmin=2, theiler])\n\nCalculate the average of the recurrence times contained in the recurrence matrix R, ruling out the lines shorter than lmin (2 by default) and all the points inside the Theiler window (see rqa for the default values and usage of the keyword argument theiler).\n\n\n\n\n\n","category":"function"},{"location":"rqa/quantification/#Keyword-table","page":"Recurrence Quantification Analysis","title":"Keyword table","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"Since most of the above functions can be fined tuned with keyword arguments, here is a table summarizing them that could be of use:","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"Argument Default Functions Description\ntheiler 0 for CrossRecurrenceMatrix, 1 otherwise. recurrencerate<br/>determinism<br/>*_average<br/>*_max<br/>*_entropy<br/>divergence<br/>trend<br/>laminarity<br/>trappingtime<br/> meanrecurrencetime<br/>nmprt Theiler window: number of diagonals around the LOI excluded from the analysis. The value 0 means that the LOI is included in the analysis. Use 1 to exclude the LOI.\nlmin 2 determinism<br/>*_average<br/>*_max<br/>*_entropy<br/>divergence<br/>laminarity<br/>trappingtime<br/> meanrecurrencetime<br/>nmprt Minimum length of the recurrent structures (diagonal or vertical) considered in the analysis.\nborder 10 trend Number of diagonals excluded from the analysis near the border of the matrix.","category":"page"},{"location":"rqa/quantification/#Recurrence-Structures-Histograms","page":"Recurrence Quantification Analysis","title":"Recurrence Structures Histograms","text":"","category":"section"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"The functions that we list in this page internally compute histograms of some recurrence structures, like e.g. the vertical lengths. You can access these values directly with the following function:","category":"page"},{"location":"rqa/quantification/","page":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","text":"recurrencestructures","category":"page"},{"location":"rqa/quantification/#RecurrenceAnalysis.recurrencestructures","page":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencestructures","text":"recurrencestructures(x::AbstractRecurrenceMatrix;\n                         diagonal=true,\n                         vertical=true,\n                         recurrencetimes=true,\n                         kwargs...)\n\nReturn a dictionary with the histograms of the recurrence structures contained in the recurrence matrix x, with the keys \"diagonal\", \"vertical\" or \"recurrencetimes\", depending on what keyword arguments are given as true.\n\nDescription\n\nEach item of the dictionary is a vector of integers, such that the i-th element of the vector is the number of lines of length i contained in x.\n\n\"diagonal\" counts the diagonal lines, i.e. the recurrent trajectories.\n\"vertical\" counts the vertical lines, i.e. the laminar states.\n\"recurrencetimes\" counts the vertical distances between recurrent states,   i.e. the recurrence times.\n\nAll the points of the matrix are counted by default. The keyword argument theiler can be passed to rule out the lines around the main diagonal. See the arguments of the function rqa for further details.\n\n\"Empty\" histograms are represented always as [0].\n\nNotice: There is not a unique operational definition of \"recurrence times\". In the analysis of recurrence plots, usually the  \"second type\" of recurrence times as defined by Gao and Cai [1] are considered, i.e. the distance between consecutive (but separated) recurrent structures in the vertical direction of the matrix. But that distance is not uniquely defined when the vertical recurrent structures are longer than one point. The recurrence times calculated here are the distance between the midpoints of consecutive lines, which is a balanced estimator of the Poincaré recurrence times [2].\n\nReferences\n\n[1] J. Gao & H. Cai. \"On the structures and quantification of recurrence plots\". Physics Letters A, 270(1-2), 75–87 (2000).\n\n[2] N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#Recurrence-Plots","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/#Recurrence-Matrices","page":"Recurrence Plots","title":"Recurrence Matrices","text":"","category":"section"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"A Recurrence plot (which refers to the plot of a matrix) is a way to quantify recurrences that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the phase space that it was at some previous time.","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"The central structure used in these recurrences is the (cross-) recurrence matrix:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"Ri j = begincases\n1 quad textifquad d(xi yj) le varepsilon\n0 quad textelse\nendcases","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"where d(xi yj) stands for the distance between trajectory x at point i and trajectory y at point j. Both x y can be single timeseries, full trajectories or embedded timeseries (which are also trajectories).","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"If xequiv y then R is called recurrence matrix, otherwise it is called cross-recurrence matrix. There is also the joint-recurrence variant, see below. With RecurrenceAnalysis you can use the following functions to access these matrices","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"RecurrenceMatrix\nCrossRecurrenceMatrix\nJointRecurrenceMatrix","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.RecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceMatrix","text":"RecurrenceMatrix(x, ε; kwargs...)\nRecurrenceMatrix{FAN}(...)\n\nCreate a recurrence matrix from trajectory x. Objects of type <:AbstractRecurrenceMatrix are displayed as a recurrenceplot.\n\nDescription\n\nThe recurrence matrix is a numeric representation of a \"recurrence plot\" [1, 2], in the form of a sparse square matrix of Boolean values.\n\nx must be a Vector or an AbstractDataset (possibly representing an embedded phase space; see embed). If d(x[i], x[j]) ≤ ε (with d the distance function), then the cell (i, j) of the matrix will have a true value. The criteria to evaluate distances between data points are defined by the following keyword arguments:\n\nscale=1 : a function of the distance matrix (see distancematrix), or a fixed number, used to scale the value of ε. Typical choices are maximum or mean, such that the threshold ε is defined as a ratio of the maximum or the mean distance between data points, respectively (using mean or maximum calls specialized versions that are faster than the naive approach).  Use 1 to keep the distances unscaled (default).\nfixedrate::Bool=false : a flag that indicates if ε should be taken as a target fixed recurrence rate (see recurrencerate). If fixedrate is set to true, ε must be a value between 0 and 1, and scale is ignored.\nmetric=\"euclidean\" : metric of the distances, either Metric or a string,  as in distancematrix.\nparallel::Bool=false : whether to parallelize the computation of the recurrence  matrix.  This will split the computation of the matrix across multiple threads.\n\nThe parametrized constructor RecurrenceMatrix{NeighborNumber} creates the recurrence matrix with a fixed number of neighbors for each point in the phase space, i.e. the number of recurrences is the same for all columns of the recurrence matrix. In such case, ε is taken as the target fixed local recurrence rate, defined as a value between 0 and 1, and scale and fixedrate are ignored. This is often referred to in the literature as the method of \"Fixed Amount of Nearest Neighbors\" (or FAN for short); RecurrenceMatrix{FAN} can be used as a convenient alias for RecurrenceMatrix{NeighborNumber}.\n\nIf no parameter is specified, RecurrenceMatrix returns a RecurrenceMatrix{WithinRange} object, meaning that recurrences will be taken for pairs of data points whose distance is within the range determined by the input arguments. Note that while recurrence matrices with neighbors defined within a given range are always symmetric, those defined by a fixed amount of neighbors can be non-symmetric.\n\nSee also: CrossRecurrenceMatrix, JointRecurrenceMatrix and use recurrenceplot to turn the result of these functions into a plottable format.\n\nReferences\n\n[1] : N. Marwan et al., \"Recurrence plots for the analysis of complex systems\", Phys. Reports 438(5-6), 237-329 (2007). DOI:10.1016/j.physrep.2006.11.001\n\n[2] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.), Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 3-43 (2015).\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#RecurrenceAnalysis.CrossRecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.CrossRecurrenceMatrix","text":"CrossRecurrenceMatrix(x, y, ε; kwargs...)\nCrossRecurrenceMatrix{FAN}(...)\n\nCreate a cross recurrence matrix from trajectories x and y.\n\nThe cross recurrence matrix is a bivariate extension of the recurrence matrix. For the time series x, y, of length n and m, respectively, it is a sparse n×m matrix of Boolean values, such that if d(x[i], y[j]) ≤ ε, then the cell (i, j) of the matrix will have a true value.\n\nNote that, unlike univariate recurrence matrices, cross recurrence matrices are not generally symmetric, regardless of the method used to make them.\n\nSee RecurrenceMatrix for details, references and keywords. See also: JointRecurrenceMatrix.\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#RecurrenceAnalysis.JointRecurrenceMatrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.JointRecurrenceMatrix","text":"JointRecurrenceMatrix(x, y, ε; kwargs...)\nJointRecurrenceMatrix{FAN}(...)\n\nCreate a joint recurrence matrix from x and y.\n\nThe joint recurrence matrix considers the recurrences of the trajectories of x and y separately, and looks for points where both recur simultaneously. It is calculated by the element-wise multiplication of the recurrence matrices of x and y. If x and y are of different length, the recurrences are only calculated until the length of the shortest one.\n\nSee RecurrenceMatrix for details, references and keywords. See also: CrossRecurrenceMatrix.\n\n\n\n\n\nJointRecurrenceMatrix(R1, R2; kwargs...)\n\nCreate a joint recurrence matrix from given recurrence matrices R1, R2.\n\n\n\n\n\n","category":"type"},{"location":"rqa/rplots/#Simple-Recurrence-Plots","page":"Recurrence Plots","title":"Simple Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"The recurrence matrices are internally stored as sparse matrices with boolean values. Typically in the literature one does not \"see\" the matrices themselves but instead a plot of them (hence \"Recurrence Plots\"). By default, when a Recurrence Matrix is created we \"show\" a mini plot of it which is a text-based scatterplot.","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"Here is an example recurrence plot/matrix of a full trajectory of the Roessler system:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"using DynamicalSystems\nro = Systems.roessler(ones(3), a=0.15, b=0.20, c=10.0)\nN = 2000; dt = 0.05\ntr = trajectory(ro, N*dt; dt = dt, Ttr = 10.0)\n\nR = RecurrenceMatrix(tr, 5.0; metric = \"euclidean\")\nrecurrenceplot(R; ascii = true)","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"typeof(R)","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"summary(R)","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"The above simple plotting functionality is possible through the package UnicodePlots. The following function creates the plot:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"recurrenceplot","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.recurrenceplot","page":"Recurrence Plots","title":"RecurrenceAnalysis.recurrenceplot","text":"recurrenceplot([io,] R; minh = 25, maxh = 0.5, ascii, kwargs...) -> u\n\nCreate a text-based scatterplot representation of a recurrence matrix R to be displayed in io (by default stdout) using UnicodePlots. The matrix spans at minimum minh rows and at maximum maxh*displaysize(io)[1] (i.e. by default half the display). As we always try to plot in equal aspect ratio, if the width of the plot is even less, the minimum height is dictated by the width.\n\nThe keyword ascii::Bool can ensure that all elements of the plot are ASCII characters (true) or Unicode (false).\n\nThe rest of the kwargs are propagated into UnicodePlots.scatterplot.\n\nNotice that the accuracy of this function drops drastically for matrices whose size is significantly bigger than the width and height of the display (assuming each index of the matrix is one character).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"Here is the same plot but using Unicode Braille characters","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"recurrenceplot(R; ascii = false)","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"As you can see, the Unicode based plotting doesn't display nicely everywhere. It does display perfectly in e.g. Juno, which is where it is the default printing type. Here is how it looks like in a dark background:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/#Advanced-Recurrence-Plots","page":"Recurrence Plots","title":"Advanced Recurrence Plots","text":"","category":"section"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"A text-based plot is cool, fast and simple. But often one needs the full resolution offered by the data of a recurrence matrix.","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"There are two more ways to plot a recurrence matrix using RecurrenceAnalysis:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"coordinates\ngrayscale","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.coordinates","page":"Recurrence Plots","title":"RecurrenceAnalysis.coordinates","text":"coordinates(R) -> xs, ys\n\nReturn the coordinates of the recurrence points of R (in indices).\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/#RecurrenceAnalysis.grayscale","page":"Recurrence Plots","title":"RecurrenceAnalysis.grayscale","text":"grayscale(R [, bwcode]; width::Int, height::Int, exactsize=false)\n\nTransform the recurrence matrix R into a full matrix suitable for plotting as a grayscale image. By default it returns a matrix with the same size as R, but switched axes, containing \"black\" values in the cells that represent recurrent points, and \"white\" values in the empty cells and interpolating in-between for cases with both recurrent and empty cells, see below.\n\nThe numeric codes for black and white are given in a 2-element tuple as a second optional argument. Its default value is (0.0, 1.0), i.e. black is coded as 0.0 (no brightness) and white as 1.0 (full brightness). The type of the elements in the tuple defines the type of the returned matrix. This must be taken into account if, for instance, the image is coded as a matrix of integers corresponding to a grayscale; in such case the black and white codes must be given as numbers of the required integer type.\n\nThe keyword arguments width and height can be given to define a custom size of the image. If only one dimension is given, the other is automatically calculated. If both dimensions are given, by default they are adjusted to keep an aspect proportional to the original matrix, such that the returned matrix fits into a matrix of the given dimensions. This automatic adjustment can be disabled by passing the keyword argument exactsize=true.\n\nIf the image has different dimensions than R, the cells of R are distributed in a grid with the size of the image, and a gray level between white and black is calculated for each element of the grid, proportional to the number of recurrent points contained in it. The levels of gray are coded as numbers of the same type as the black and white codes.\n\nIt is advised to use width, height arguments for large matrices otherwise plots using functions like e.g. imshow could be misleading.\n\n\n\n\n\n","category":"function"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"For example, here is the representation of the above R from the Roessler system using both plotting approaches:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot\nfigure(figsize = (10,5))\n\nax = subplot(121)\nxs, ys = coordinates(R)\nscatter(xs, ys, color = \"k\", s = 1)\nxlim(1, size(R)[1]); ylim(1, size(R)[2]);\nax.set_aspect(\"equal\")\n\nsubplot(122)\nRg = grayscale(R)\nimshow(Rg, cmap = \"binary_r\", extent = (1, size(R)[1], 1, size(R)[2]))\nsavefig(\"different_rplots.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"and here is exactly the same process, but using the embedded trajectory instead","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot # hide\ny = tr[:, 2]\nτ = estimate_delay(y, \"mi_min\")\nm = embed(y, 3, τ)\nR = RecurrenceMatrix(m, 5.0; metric = \"euclidean\")\n\nfigure(figsize = (5,5))\n\nxs, ys = coordinates(R)\nscatter(xs, ys, color = \"k\", s = 1)\nxlim(1, size(R)[1]); ylim(1, size(R)[2]);\nsavefig(\"rmatrix2.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"which justifies why recurrence plots are so fitting to be used in embedded timeseries.","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"warning: Careful when using Recurrence Plots\nIt is easy when using grayscale to not change the width/height parameters. These are however very important when the matrix size exceeds the display size! Most plotting libraries may resample arbitrarily or simply limit the displayed pixels, so one needs to be extra careful.Besides graphical problems there are also other potential pitfalls dealing with the conceptual understanding and use of recurrence plots. All of these are summarized in the following paper which we suggest users to take a look at:N. Marwan, How to avoid potential pitfalls in recurrence plot based data analysis, Int. J. of Bifurcations and Chaos (arXiv).","category":"page"},{"location":"rqa/rplots/#Example","page":"Recurrence Plots","title":"Example","text":"","category":"section"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"In the following we will plot recurrence plots of the Lorenz system for a periodic and chaotic regime (using scatter plot).","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"using PyPlot # hide\nlor = Systems.lorenz()\nfigure(figsize = (10,10))\n\nfor (i, ρ) in enumerate((69.75, 28.0))\n    set_parameter!(lor, 2, ρ)\n    t, dt = 20.0, 0.01\n    tr = trajectory(lor, t; dt = dt, Ttr = 2000.0)\n    tvec = 0:dt:t\n\n    subplot(2,2, i)\n    plot(tr[:, 1], tr[:, 3], color = \"C$(i+1)\", label = \"X vs Z\")\n    title(\"ρ = $ρ, \" * (i != 1 ? \"not periodic\" : \"periodic\")); legend()\n\n    ε = i == 1 ? 5.0 : 3.0\n    R = RecurrenceMatrix(tr, ε)\n\n    subplot(2,2,i+2)\n    x, y = coordinates(R)\n    scatter(tvec[x], tvec[y], s = 1, alpha = 0.2, color = \"C$(i+1)\")\n    xlim(0, t); ylim(0, t); gca().set_aspect(\"equal\")\n    xlabel(\"t\"); i == 1 && ylabel(\"t\");\nend\nPyPlot.tight_layout()\nsavefig(\"rplotexamples.png\"); nothing # hide","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"(Image: )","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"On the left we see long (infinite) diagonals repeated over and over for different times. This is the case for periodic systems as they visit exactly the same area on the phase space again and again. The distance between the offset diagonals also coincides with the periodicity of the system, which is around t ≈ 4.","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"On the right we see a structure typical of chaotic motion on a strange attractor such as the one of the Lorenz system: the orbit visits neighborhoods of previous points but then quickly diverges again. This results in many small diagonal lines.","category":"page"},{"location":"rqa/rplots/#Distances","page":"Recurrence Plots","title":"Distances","text":"","category":"section"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"The distance function used in RecurrenceMatrix and co. can be specified either as a string or as any Metric instance from Distances. In addition, the following function returns a matrix with the cross-distances across all points in one or two trajectories:","category":"page"},{"location":"rqa/rplots/","page":"Recurrence Plots","title":"Recurrence Plots","text":"distancematrix","category":"page"},{"location":"rqa/rplots/#RecurrenceAnalysis.distancematrix","page":"Recurrence Plots","title":"RecurrenceAnalysis.distancematrix","text":"distancematrix(x [, y = x], metric = \"euclidean\")\n\nCreate a matrix with the distances between each pair of points of the time series x and y using metric.\n\nThe time series x and y can be AbstractDatasets or vectors or matrices with data points in rows. The data point dimensions (or number of columns) must be the same for x and y. The returned value is a n×m matrix, with n being the length (or number of rows) of x, and m the length of y.\n\nThe metric can be identified by a string, or any of the Metrics defined in the Distances package. The list of strings available to define the metric are:\n\n\"max\" or \"inf\" for the maximum or L∞ norm (Chebyshev() in the Distances package).\n\"euclidean\" for the L2 or Euclidean norm, used by default (Euclidean() in Distances).\n\"manhattan\", \"cityblock\", \"taxicab\" or \"min\" for the Manhattan or L1 norm (Cityblock() in Distances).\n\n\n\n\n\n","category":"function"},{"location":"entropies/api/#Entropies-and-Probabilities","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"","category":"section"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"Here we discuss obtaining probabilities and entropies from a given dataset (that typically represents a trajectory or set in the state space of a dynamical system). The data are expected in the form discussed in Numerical Data.","category":"page"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"The main API for this is contained in two functions:","category":"page"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"probabilities which computes probability distributions of given datasets\ngenentropy which uses the output of probabilities, or a set of pre-computed Probabilities, to calculate entropies.","category":"page"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"These functions dispatch on subtypes of ProbabilitiesEstimator, which are summarized in the Probabilities Estimators page.","category":"page"},{"location":"entropies/api/#Probabilities","page":"Entropies & Probabilities","title":"Probabilities","text":"","category":"section"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"Probabilities\nprobabilities\nprobabilities!","category":"page"},{"location":"entropies/api/#Entropies.Probabilities","page":"Entropies & Probabilities","title":"Entropies.Probabilities","text":"Probabilities(x) → p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"entropies/api/#Entropies.probabilities","page":"Entropies & Probabilities","title":"Entropies.probabilities","text":"probabilities(x::Vector_or_Dataset, est::ProbabilitiesEstimator) → p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator and return them as a Probabilities container (Vector-like). The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more.\n\nThe configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Vector_or_Dataset, ε::AbstractFloat) → p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length ε, and formally we use est = VisitationFrequency(RectangularBinning(ε)) as an estimator, see VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. To obtain the bin information along with p, use binhist.\n\nprobabilities(x::Vector_or_Dataset, n::Integer) → p::Probabilities\n\nSame as the above method, but now each dimension of the data is binned into n::Int equal sized bins instead of bins of length ε::AbstractFloat.\n\nprobabilities(x::Vector_or_Dataset) → p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, or other processing (mostly useful when x contains categorical or integer data).\n\n\n\n\n\n","category":"function"},{"location":"entropies/api/#Entropies.probabilities!","page":"Entropies & Probabilities","title":"Entropies.probabilities!","text":"probabilities!(args...)\n\nIdentical to probabilities(args...), but allows pre-allocation of temporarily used containers.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"entropies/api/#Fast-histograms","page":"Entropies & Probabilities","title":"Fast histograms","text":"","category":"section"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"binhist","category":"page"},{"location":"entropies/api/#Entropies.binhist","page":"Entropies & Probabilities","title":"Entropies.binhist","text":"binhist(x::AbstractDataset, ε::Real) → p, bins\nbinhist(x::AbstractDataset, ε::RectangularBinning) → p, bins\n\nHyper-optimized histogram calculation for x with rectangular binning ε. Returns the probabilities p of each bin of the histogram as well as the bins. Notice that bins are the starting corners of each bin. If ε isa Real, then the actual bin size is ε across each dimension. If ε isa RectangularBinning, then the bin size for each dimension will depend on the binning scheme.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"function"},{"location":"entropies/api/#Generalized-entropy","page":"Entropies & Probabilities","title":"Generalized entropy","text":"","category":"section"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"In the study of dynamical systems there are many quantities that identify as \"entropy\". Notice that these quantities are not the thermodynamic ones, used in Statistical Physics. Rather, they are more like the to the entropies of information theory.","category":"page"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"All of the entropy-related quantities boil down to one thing: first extracting probabilities from a dataset and then applying the generalized entropy formula using genentropy.","category":"page"},{"location":"entropies/api/","page":"Entropies & Probabilities","title":"Entropies & Probabilities","text":"genentropy\npermentropy","category":"page"},{"location":"entropies/api/#Entropies.genentropy","page":"Entropies & Probabilities","title":"Entropies.genentropy","text":"genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n\nCompute the generalized order-q entropy of some probabilities returned by the probabilities function. Alternatively, compute entropy from pre-computed Probabilities.\n\ngenentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the entropy of the result (and thus est can be a ProbabilitiesEstimator or simply ε::Real).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"entropies/api/#ChaosTools.permentropy","page":"Entropies & Probabilities","title":"ChaosTools.permentropy","text":"permentropy(x, m = 3; τ = 1, base = Base.MathConstants.e)\n\nCompute the permutation entropy[Brandt2002] of given order m from the x timeseries.\n\nThis method is equivalent with\n\ngenentropy(x, SymbolicPermutation(; m, τ); base)\n\n[Bandt2002]: C. Bandt, & B. Pompe, Phys. Rev. Lett. 88 (17), pp 174102 (2002)\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Detecting-and-Categorizing-Chaos","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Being able to detect and distinguish chaotic from regular behavior is crucial in the study of dynamical systems. Most of the time a positive maximum lyapunov exponent and a bounded system indicate chaos.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"However, the convergence of the Lyapunov exponent can be slow, or even misleading, as the types of chaotic behavior vary with respect to their predictability. There are many alternatives, some more efficient and some more accurate in characterizing chaotic and regular motion. Some of these methods are included in DynamicalSystems.jl.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"info: Performance depends on the solver\nNotice that the performance of functions that use ContinuousDynamicalSystems depend crucially on the chosen solver. Please see the documentation page on Choosing a solver for an in-depth discussion.","category":"page"},{"location":"chaos/chaos_detection/#Generalized-Alignment-Index","page":"Detecting & Categorizing Chaos","title":"Generalized Alignment Index","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"\"GALI\" for sort, is a method that relies on the fact that initially orthogonal deviation vectors tend to align towards the direction of the maximum Lyapunov exponent for chaotic motion. It is one of the most recent and cheapest methods for distinguishing chaotic and regular behavior, introduced first in 2007 by Skokos, Bountis & Antonopoulos.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"gali","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.gali","page":"Detecting & Categorizing Chaos","title":"ChaosTools.gali","text":"gali(ds::DynamicalSystem, tmax, k::Int | Q0; kwargs...) -> GALI_k, t\n\nCompute textGALI_k[Skokos2007] for a given k up to time tmax. Return textGALI_k(t) and time vector t.\n\nThe third argument, which sets the order of gali, can be an integer k, or a matrix with its columns being the deviation vectors (then k = size(Q0)[2]). In the first case random orthonormal vectors are chosen.\n\nKeyword Arguments\n\nthreshold = 1e-12 : If GALI_k falls below the threshold iteration is terminated.\ndt = 1 : Time-step between deviation vector normalizations. For continuous systems this is approximate.\nu0 : Initial state for the system. Defaults to get_state(ds).\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nThe Generalized Alignment Index, textGALI_k, is an efficient (and very fast) indicator of chaotic or regular behavior type in D-dimensional Hamiltonian systems (D is number of variables). The asymptotic behavior of textGALI_k(t) depends critically on the type of orbit resulting from the initial condition. If it is a chaotic orbit, then\n\ntextGALI_k(t) sim\nexpleftsum_j=1^k (lambda_1 - lambda_j)t right\n\nwith lambda_j being the j-th Lyapunov exponent (see lyapunov, lyapunovspectrum). If on the other hand the orbit is regular, corresponding to movement in d-dimensional torus with 1 le d le D2 then it holds\n\ntextGALI_k(t) sim\n    begincases\n      textconst  textif  2 le k le d    textand\n       d  1 \n      t^-(k - d)  textif   d  k le D - d \n      t^-(2k - D)  textif   D - d  k le D\n    endcases\n\nTraditionally, if textGALI_k(t) does not become less than the threshold until tmax the given orbit is said to be chaotic, otherwise it is regular.\n\nOur implementation is not based on the original paper, but rather in the method described in[Skokos2016b], which uses the product of the singular values of A, a matrix that has as columns the deviation vectors.\n\nPerformance Notes\n\nThis function uses a tangent_integrator. For loops over initial conditions and/or parameter values one should use the low level method that accepts an integrator, and reinit! it to new initial conditions. See the \"advanced documentation\" for info on the integrator object. The low level method is\n\nChaosTools.gali(tinteg, tmax, dt, threshold)\n\n[Skokos2007]: Skokos, C. H. et al., Physica D 231, pp 30–54 (2007)\n\n[Skokos2016b]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 5\n\n(section 5.3.1 and ref. [85] therein), Lecture Notes in Physics 915, Springer (2016)\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"","category":"page"},{"location":"chaos/chaos_detection/#Discrete-Example","page":"Detecting & Categorizing Chaos","title":"Discrete Example","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We will use 3 coupled standard maps as an example for a discrete system:","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems\nusing PyPlot\nM = 3; ks = 3ones(M); Γ = 0.1;\nstable = [π, π, π, 0.01, 0, 0] .+ 0.1\nchaotic = rand(2M)\n\nds = Systems.coupledstandardmaps(M, stable; ks=ks, Γ = Γ)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"First, let's see the behavior of GALI for a stable orbit","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 100000)\n\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,1+M], alpha = 0.5,\nlabel=\"stable\",marker=\"o\", ms=1, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [4, 5, 6]\n    g, t = gali(ds, 1e5, k; threshold=1e-12)\n    lt = log10.(t); lg = log10.(g)\n    plot(lt, lg, label=\"GALI_$(k)\")\nend\nlt = 2:0.5:5.5\nplot(lt, -2(lt .- 3), label=\"slope -2\")\nplot(lt, -4(lt .- 3), label=\"slope -4\")\nplot(lt, -6(lt .- 3), label=\"slope -6\")\n\nxlim(2, 5.5)\nylim(-12, 2)\nlegend()\ntight_layout()\nsavefig(\"gali_discrete_stable.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_discrete_stable)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Now do the same for a chaotic orbit","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 100000, chaotic)\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,1+M], alpha = 0.5,\nlabel=\"chaotic\",marker=\"o\", ms=1, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nls = lyapunovspectrum(ds, 100000; u0 = chaotic)\nfor k in [2,3,6]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = chaotic)\n    semilogy(t, exp.(-ex.*t), label=\"exp. k=$k\")\n    semilogy(t, g, label=\"GALI_$(k)\")\nend\nlegend()\nxlim(0,100)\nylim(1e-12, 1)\nsavefig(\"gali_discrete_chaos.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_discrete_chaos)","category":"page"},{"location":"chaos/chaos_detection/#Continuous-Example","page":"Detecting & Categorizing Chaos","title":"Continuous Example","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"As an example of a continuous system, let's see the Henon-Heiles:","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems\nusing PyPlot, OrdinaryDiffEq\nsp = [0, .295456, .407308431, 0] # stable periodic orbit: 1D torus\nqp = [0, .483000, .278980390, 0] # quasiperiodic orbit: 2D torus\nch = [0, -0.25, 0.42081, 0]      # chaotic orbit\nds = Systems.henonheiles(sp)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"First, we see the behavior with a stable periodic orbit","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\nsubplot(1,2,1)\ndt = 1.0\n\ndiffeq = (abstol=1e-9, reltol=1e-9, alg = Tsit5(), maxiters = typemax(Int))\ntr = trajectory(ds, 10000.0; dt=dt, diffeq...)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"sp\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; dt = dt, diffeq...)\n    loglog(t, g, label=\"GALI_$(k)\")\n    if k < 4\n        loglog(t, 100 ./ t.^(k-1), label=\"slope -$(k-1)\")\n    else\n        loglog(t, 10000 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    end\nend\nylim(1e-12, 2)\nlegend();\nsavefig(\"gali_cont_stable.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_stable)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Next, let's see what happens with a quasi-periodic orbit. Don't forget to change the u0 arguments!","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\nsubplot(1,2,1)\ntr = trajectory(ds, 10000.0, qp; dt=dt, diffeq...)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"qp\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; u0 = qp, dt = dt, diffeq...)\n    loglog(t, g, label=\"GALI_$(k)\")\n    if k == 2\n        loglog(t, 1 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    else\n        loglog(t, 100 ./ t.^(2k-4), label=\"slope -$(2k-4)\")\n    end\nend\nylim(1e-12, 2)\nlegend()\ntight_layout()\nsavefig(\"gali_cont_quasi.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_quasi)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Finally, here is GALI of a continuous system with a chaotic orbit","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure(figsize = (8,4))\ntr = trajectory(ds, 10000.0, ch; dt=dt, diffeq...)\nsubplot(1,2,1)\nplot(tr[:,1], tr[:,3], alpha = 0.5,\nlabel=\"ch\",marker=\"o\",markersize=2, linewidth=0)\nlegend()\n\nsubplot(1,2,2)\nls = lyapunovspectrum(ds, 5000.0; dt=dt, u0 = ch, diffeq...)\nfor k in [2,3,4]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = ch, dt = dt, diffeq...)\n    semilogy(t, exp.(-ex.*t), label=\"exp. k=$k\")\n    semilogy(t, g, label=\"GALI_$(k)\")\nend\nlegend()\nylim(1e-16, 1)\ntight_layout()\nsavefig(\"gali_cont_chaos.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: gali_cont_chaos)","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"As you can see, the results of both discrete and continuous systems match very well the theory described in gali.","category":"page"},{"location":"chaos/chaos_detection/#Using-GALI","page":"Detecting & Categorizing Chaos","title":"Using GALI","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"No-one in their right mind would try to fit power-laws in order to distinguish between chaotic and regular behavior, like the above examples. These were just proofs that the method works as expected in all cases.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The most common usage of textGALI_k is to define a (sufficiently) small amount of time and a (sufficiently) small threshold and see whether textGALI_k stays below it, for a (sufficiently) big k.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The following is an example of advanced usage (see Advanced documentation):","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using DynamicalSystems, PyPlot\n\nfunction main(k)\n# Measure of chaoticity: final time of gali_2\ndens = 201\nchaoticity = zeros(Int, dens, dens)\n\nθs = ps = range(0, stop = 2π, length = dens+1)\nds = Systems.standardmap(k = k)\n\ntinteg = tangent_integrator(ds, 2)\n\nfor (i, θ) ∈ enumerate(θs[1:dens])\n    println(\"i = $(i)\")\n    for (j, p) ∈ enumerate(ps[1:dens])\n\n        # new initial state is the system initial state\n        u0 = SVector{2}(θ, p)\n        reinit!(tinteg, u0, orthonormal(2,2))\n\n        # Low-level call signature of gali:\n        #  gali(tinteg, tmax, dt, threshold)\n        chaoticity[i, j] = gali(tinteg, 500, 1, 1e-12)[2][end]\n    end\nend\nfigure()\npcolormesh(θs .- (θs[2] - θs[1])/2, ps .- (ps[2] - ps[1])/2,\nchaoticity')\ncolorbar()\nxlabel(\"\\$\\\\theta\\$\")\nylabel(\"\\$p\\$\")\nreturn\nend\n\nmain(0.9);","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: )","category":"page"},{"location":"chaos/chaos_detection/#Regular-orbits-in-the-Henon-Heiles-system","page":"Detecting & Categorizing Chaos","title":"Regular orbits in the Henon-Heiles system","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"In this example we use the poincaresos function to produce surfaces of section of the Systems.henonheiles system at different energies. At each energy gali is used to color-code each initial condition according to how chaotic/regular it is, i.e. how much time does it need to exceed the threshold of gali.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"<video width=\"100%\" height=\"auto\" controls> <source src=\"https://raw.githubusercontent.com/JuliaDynamics/JuliaDynamics/master/videos/chaos/galipsoshenonhelies.mp4?raw=true\" type=\"video/mp4\"> </video>","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"You can download the video using this link.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"You can find the script that produced this animation in DynamicalSystems/docs/coolanimations/gali_psos_henonhelies.jl.","category":"page"},{"location":"chaos/chaos_detection/#Predictability-of-a-chaotic-system","page":"Detecting & Categorizing Chaos","title":"Predictability of a chaotic system","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Even if a system is \"formally\" chaotic, it can still be in phases where it is very predictable, because the correlation coefficient between nearby trajectories vanishes very slowly with time. Wernecke, Sándor & Gros have developed an algorithm that allows one to classify a dynamical system to one of three categories: strongly chaotic, partially predictable chaos or regular (called laminar in their paper).","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We have implemented their algorithm in the function predictability. Note that we set up the implementation to always return regular behavior for negative Lyapunov exponent. You may want to override this for research purposes.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"predictability","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.predictability","page":"Detecting & Categorizing Chaos","title":"ChaosTools.predictability","text":"predictability(ds::DynamicalSystem; kwargs...) -> chaos_type, ν, C\n\nDetermine whether ds displays strongly chaotic, partially-predictable chaotic or regular behaviour, using the method by Wernecke et al. described in[Wernecke2017].\n\nReturn the type of the behavior, the cross-distance scaling coefficient ν and the correlation coefficient C. Typical values for ν, C and chaos_type are given in Table 2 of[Wernecke2017]:\n\nchaos_type ν C\n:SC 0 0\n:PPC 0 1\n:REG 1 1\n\nKeyword Arguments\n\nTtr = 200 : Extra \"transient\" time to evolve the system before sampling from  the trajectory. Should be Int for discrete systems.\nT_sample = 1e4 : Time to evolve the system for taking samples. Should be Int for discrete systems.\nn_samples = 500 : Number of samples to take for use in calculating statistics.\nλ_max = lyapunov(ds, 5000) : Value to use for largest Lyapunov exponent for finding the Lyapunov prediction time. If it is less than zero a regular result is returned immediatelly.\nd_tol = 1e-3 : tolerance distance to use for calculating Lyapunov prediction time.\nT_multiplier = 10 : Multiplier from the Lyapunov prediction time to the evaluation time.\nT_max = Inf : Maximum time at which to evaluate trajectory distance. If the internally  computed evaluation time is larger than T_max, stop at T_max instead.\nδ_range = 10.0 .^ (-9:-6) : Range of initial condition perturbation distances  to use to determine scaling ν.\ndiffeq... : Keyword arguments propagated into init of DifferentialEquations.jl. See trajectory for examples. Only valid for continuous systems.\n\nDescription\n\nSamples points from a trajectory of the system to be used as initial conditions. Each of these initial conditions is randomly perturbed by a distance δ, and the trajectories for both the original and perturbed initial conditions are computed to the 'evaluation time' T.\n\nThe average (over the samples) distance and cross-correlation coefficient of the state at time T is computed. This is repeated for a range of δ (defined by δ_range), and linear regression is used to determine how the distance and cross-correlation scale with δ, allowing for identification of chaos type.\n\nThe evaluation time T is calculated as T = T_multiplier*Tλ, where the Lyapunov prediction time Tλ = log(d_tol/δ)/λ_max. This may be very large if the λ_max is small, e.g. when the system is regular, so this internally computed time T can be overridden by a smaller T_max set by the user.\n\nPerformance Notes\n\nFor continuous systems, it is likely that the maxiters used by the integrators needs to be increased, e.g. to 1e9. This is part of the diffeq kwargs. In addition, be aware that this function does a lot of internal computations. It is operating in a different speed than e.g. lyapunov.\n\n[Wernecke2017]: Wernecke, H., Sándor, B. & Gros, C. How to test for partially predictable chaos. Scientific Reports 7, (2017).\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Example-Hénon-Map","page":"Detecting & Categorizing Chaos","title":"Example Hénon Map","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We will create something similar to figure 2 of the paper, but for the Hénon map.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"figure()\nhe = Systems.henon()\nas = 0.8:0.01:1.225\nod = orbitdiagram(he, 1, 1, as; n = 2000, Ttr = 2000)\ncolors = Dict(:REG => \"b\", :PPC => \"g\", :SC => \"r\")\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    chaos_type, ν, C = predictability(he; T_max = 400000, Ttr = 2000)\n    scatter(a .* ones(length(od[i])), od[i], c = colors[chaos_type], s = 2,\n    alpha = 0.05)\nend\nxlabel(\"\\$a\\$\"); ylabel(\"\\$x\\$\")\ntitle(\"predictability of Hénon map\"); tight_layout()\nsavefig(\"partial_henon.png\"); nothing # hide","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"(Image: partial_henon)","category":"page"},{"location":"chaos/chaos_detection/#The-0-1-test-for-chaos","page":"Detecting & Categorizing Chaos","title":"The 0-1 test for chaos","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The methods mentioned in this page so far require a DynamicalSystem instance. But of course this is not always the case. The so-called \"0 to 1\" test for chaos, by Gottwald & Melbourne, takes as an input a timeseries and outputs a boolean true if the timeseries is chaotic or false if it is not.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Notice that the method does have a lot of caveats, so you should read the review paper before using.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"testchaos01","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.testchaos01","page":"Detecting & Categorizing Chaos","title":"ChaosTools.testchaos01","text":"testchaos01(φ::Vector [, cs, N0]) -> chaotic?\n\nPerform the so called \"0-1\" test for chaos introduced by Gottwald and Melbourne[Gottwald2016] on the timeseries φ. Return true if φ is chaotic, false otherwise.\n\nDescription\n\nThis method tests if the given timeseries is chaotic or not by transforming it into a two-dimensional diffusive process. If the timeseries is chaotic, the mean square displacement of the process grows as sqrt(length(φ)), while it stays constant if the timeseries is regular. The implementation here computes K, the correlation coefficient (median of Kc for c ∈ cs), and simply checks if K > 0.5.\n\nIf you want to access the various Kc you should call the method testchaos01(φ, c::Real, N0) which returns Kc.\n\ncs defaults to 3π/5*rand(10) + π/4 and N0, the length of the two-dimensional process, is N0 = length(φ)/10.\n\nNotice that for data sampled from continous dynamical systems, some care must be taken regarding the values of cs, see[Gottwald2016].\n\n[Gottwald2016]: Gottwald & Melbourne, “The 0-1 test for chaos: A review” Lect. Notes Phys., vol. 915, pp. 221–247, 2016.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#Expansion-entropy","page":"Detecting & Categorizing Chaos","title":"Expansion entropy","text":"","category":"section"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The expansion entropy is a quantity that is suggested by B. Hunt and E. Ott as a measure that can define chaos (so far no widely accepted definition of chaos exists). Positive expansion entropy means chaos.","category":"page"},{"location":"chaos/chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"expansionentropy\nboxregion\nexpansionentropy_sample\nexpansionentropy_batch","category":"page"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy","text":"expansionentropy(ds::DynamicalSystem, sampler, restraining; kwargs...)\n\nCalculate the expansion entropy[Hunt2015] of ds, in the restraining region S defined by restraining, by estimating the slope of the biggest linear region of the curve log E_t0+T t0(f S) versus T (using linear_region). This is an approximation of the expansion entropy H_0, according to[Hunt2015].\n\nsampler is a 0-argument function that generates a random initial condition (a sample) of ds. restraining is a 1-argument function restraining(u) that given the state u it returns true if the state is inside the restraining region S.\n\nUse boxregion for an easy way to define sampler and restraining on a multidimension box.\n\nKeyword Arguments\n\nN = 1000 : Number of samples taken at each batch (same as N of [1]).\nsteps = 40 : The maximal steps for which the system will be run.\nTtr = 0 : Transient time to evolve each initial condition before starting to comute E. This is t0 of [1] and of the following notation.\nbatches = 100 : Number of batches to run the calculation, see below.\ndiffeq... : Other keywords are propagated to the solvers of DifferentialEquations.jl.\n\nDescription\n\nN samples are initialized and propagated forwards in time (along with their tangent space). At every time t in [t0+dt, t0+2dt, ... t0+steps*dt] we calculate H:\n\nHt = log E_t0+T t0(f S)\n\nwith\n\nE_t0+T t0(f S) = frac 1 N sum_i G(Df_t0+t t0(x_i))\n\n(using same notation as [Hunt2015]). In principle E is the average largest possible growth ratio within the restraining region (sampled by the initial conditions). The summation is only over x_i that stay inside the region S defined by the boolean function restraining. This process is done by the expansionentropy_sample function.\n\nThen, this is repeated for batches amount of times, as recommended in[Hunt2015]. From all these batches, the mean and std of H is computed at every time point. This is done by the expansionentropy_batch function. When plotted versus t, these create the curves and error bars of e.g. Figs 2, 3 of [1].\n\nThis function expansionentropy simply returns the slope of the biggest linear region of the curve H versus t, which approximates the expansion entropy H_0. It is therefore recommended to use expansionentropy_batch directly and evaluate the result yourself, as this step is known to be inaccurate for non-chaotic systems (where H fluctuates strongly around 0).\n\n[Hunt2015]: B. Hunt & E. Ott, ‘Defining Chaos’, Chaos 25.9 (2015)\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.boxregion","page":"Detecting & Categorizing Chaos","title":"ChaosTools.boxregion","text":"boxregion(as, bs) -> sampler, restraining\n\nDefine a box in mathbbR^d with edges the as and bs and then return two functions: sampler, which generates a random initial condition in that box and restraining that returns true if a given state is in the box.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy_sample","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy_sample","text":"expansionentropy_sample(ds, sampler, restraining; kwargs...)\n\nReturn times, H for one sample of ds (see expansionentropy). Accepts the same argumets as expansionentropy, besides batches.\n\n\n\n\n\n","category":"function"},{"location":"chaos/chaos_detection/#ChaosTools.expansionentropy_batch","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy_batch","text":"expansionentropy_batch(ds, sampler, restraining; kwargs...)\n\nRun expansionentropy_sample batch times, and return times, mean(H), std(H) for all resulting H, see expansionentropy.\n\nAccepts the same arguments as expansionentropy.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#Numerical-Data","page":"Numerical Data","title":"Numerical Data","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"info: Trajectory and Timeseries\nThe word \"timeseries\" can be confusing, because it can mean a univariate (also called scalar or one-dimensional) timeseries or a multivariate (also called multi-dimensional) timeseries. To resolve this confusion, in DynamicalSystems.jl we have the following convention: \"timeseries\" always refers to a one-dimensional vector of numbers, which exists with respect to some other one-dimensional vector of numbers that corresponds to a time-vector. On the other hand, the word \"trajectory\" is used to refer to a multi-dimensional timeseries, which is of course simply a group/set of one-dimensional timeseries.","category":"page"},{"location":"embedding/dataset/#Datasets","page":"Numerical Data","title":"Datasets","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Trajectories (and in general sets in state space) in DynamicalSystems.jl are represented by a structure called Dataset (while timeseries are standard Julia Vectors).","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Dataset","category":"page"},{"location":"embedding/dataset/#DelayEmbeddings.Dataset","page":"Numerical Data","title":"DelayEmbeddings.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are contained in the field .data of a dataset, as a standard Julia Vector{SVector}.\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others, and when iterated over, it iterates over its contained points.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges).\n\ndata[i] gives the ith datapoint (returns an SVector)\ndata[v1] will return a vector of datapoints\ndata[v1, :] using a Colon as a second index will return a Dataset of these points\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"In essence a Dataset is simply a wrapper for a Vector of SVectors. However, it is visually represented as a matrix, similarly to how numerical data would be printed on a spreadsheet (with time being the column direction). It also offers a lot more functionality than just pretty-printing. Besides the examples in the documentation string, you can e.g. iterate over data points","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"using DynamicalSystems\nhen = Systems.henon()\ndata = trajectory(hen, 10000) # this returns a dataset\nfor point in data\n    # stuff\nend","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Most functions from DynamicalSystems.jl that manipulate and use multidimensional data are expecting a Dataset. This allows us to define efficient methods that coordinate well with each other, like e.g. embed.","category":"page"},{"location":"embedding/dataset/#Dataset-Functions","page":"Numerical Data","title":"Dataset Functions","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"minima\nmaxima\nminmaxima\ncolumns","category":"page"},{"location":"embedding/dataset/#DelayEmbeddings.minima","page":"Numerical Data","title":"DelayEmbeddings.minima","text":"minima(dataset)\n\nReturn an SVector that contains the minimum elements of each timeseries of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.maxima","page":"Numerical Data","title":"DelayEmbeddings.maxima","text":"maxima(dataset)\n\nReturn an SVector that contains the maximum elements of each timeseries of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.minmaxima","page":"Numerical Data","title":"DelayEmbeddings.minmaxima","text":"minmaxima(dataset)\n\nReturn minima(dataset), maxima(dataset) without doing the computation twice.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#DelayEmbeddings.columns","page":"Numerical Data","title":"DelayEmbeddings.columns","text":"columns(dataset) -> x, y, z, ...\n\nReturn the individual columns of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"embedding/dataset/#Dataset-I/O","page":"Numerical Data","title":"Dataset I/O","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Input/output functionality for an AbstractDataset is already achieved using base Julia, specifically writedlm and readdlm. To write and read a dataset, simply do:","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"using DelimitedFiles\n\ndata = Dataset(rand(1000, 2))\n\n# I will write and read using delimiter ','\nwritedlm(\"data.txt\", data, ',')\n\n# Don't forget to convert the matrix to a Dataset when reading\ndata = Dataset(readdlm(\"data.txt\", ',', Float64))","category":"page"},{"location":"embedding/dataset/#Neighborhoods","page":"Numerical Data","title":"Neighborhoods","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Neighborhoods refer to the common act of finding points in a dataset that are nearby a given point (which typically belongs in the dataset). DynamicalSystems.jl bases this interface on Neighborhood.jl. You can go to its documentation if you are interested in finding neighbors in a dataset for e.g. a custom algorithm implementation.","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"For DynamicalSystems.jl, what is relevant are the two types of neighborhoods that exist:","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"NeighborNumber\nWithinRange","category":"page"},{"location":"embedding/dataset/#Neighborhood.NeighborNumber","page":"Numerical Data","title":"Neighborhood.NeighborNumber","text":"NeighborNumber(k::Int) <: SearchType\n\nSearch type representing the k nearest neighbors of the query (or approximate neighbors, depending on the search structure).\n\n\n\n\n\n","category":"type"},{"location":"embedding/dataset/#Neighborhood.WithinRange","page":"Numerical Data","title":"Neighborhood.WithinRange","text":"WithinRange(r::Real) <: SearchType\n\nSearch type representing all neighbors with distance ≤ r from the query (according to the search structure's metric).\n\n\n\n\n\n","category":"type"},{"location":"embedding/dataset/#Theiler-window","page":"Numerical Data","title":"Theiler window","text":"","category":"section"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"The Theiler window is a concept that is useful when finding neighbors in a dataset that is coming from the sampling of a continuous dynamical system. As demonstrated in the figure below, it tries to eliminate spurious \"correlations\" (wrongly counted neighbors) due to a potentially dense sampling of the trajectory (e.g. by giving small sampling time in trajectory).","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"The figure below demonstrates a typical WithinRange search around the black point with index i. Black, red and green points are found neighbors, but points within indices j that satisfy |i-j| ≤ w should not be counted as \"true\" neighbors. These neighbors are typically the same around any state space point, and thus wrongly bias calculations by providing a non-zero baseline of neighbors. For the sketch below, w=3 would have been used.","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"Typically a good choice for w coincides with the choice an optimal delay time, see estimate_delay, for any of the timeseries of the dataset.","category":"page"},{"location":"embedding/dataset/","page":"Numerical Data","title":"Numerical Data","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#Nonlinear-Timeseries-Analysis","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"section"},{"location":"chaos/nlts/#Numerical-Lyapunov-Exponent","page":"Nonlinear Timeseries Analysis","title":"Numerical Lyapunov Exponent","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Given any timeseries, one can first embed it using delay coordinates, and then calculate a maximum Lyapunov exponent for it. This is done with","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"numericallyapunov","category":"page"},{"location":"chaos/nlts/#ChaosTools.numericallyapunov","page":"Nonlinear Timeseries Analysis","title":"ChaosTools.numericallyapunov","text":"numericallyapunov(R::Dataset, ks;  refstates, w, distance, ntype)\n\nReturn E = [E(k) for k ∈ ks], where E(k) is the average logarithmic distance between states of a neighborhood that are evolved in time for k steps (k must be integer). Typically R is the result of delay coordinates of a single timeseries.\n\nKeyword Arguments\n\nrefstates = 1:(length(R) - ks[end]) : Vector of indices that notes which states of the reconstruction should be used as \"reference states\", which means that the algorithm is applied for all state indices contained in refstates.\nw::Int = 1 : The Theiler window.\nntype = NeighborNumber(1) : The neighborhood type. Either NeighborNumber or WithinRange. See Neighborhoods for more info.\ndistance::Metric = Cityblock() : The distance function used in the logarithmic distance of nearby states. The allowed distances are Cityblock() and Euclidean(). See below for more info. The metric for finding neighbors is always the Euclidean one.\n\nDescription\n\nIf the dataset exhibits exponential divergence of nearby states, then it should hold\n\nE(k) approx lambdacdot k cdot Delta t + E(0)\n\nfor a well defined region in the k axis, where lambda is the approximated maximum Lyapunov exponent. Delta t is the time between samples in the original timeseries. You can use linear_region with arguments (ks .* Δt, E) to identify the slope (= lambda) immediatelly, assuming you have choosen sufficiently good ks such that the linear scaling region is bigger than the saturated region.\n\nThe algorithm used in this function is due to Parlitz[Skokos2016], which itself expands upon Kantz [Kantz1994]. In sort, for each reference state a neighborhood is evaluated. Then, for each point in this neighborhood, the logarithmic distance between reference state and neighborhood state(s) is calculated as the \"time\" index k increases. The average of the above over all neighborhood states over all reference states is the returned result.\n\nIf the Metric is Euclidean() then use the Euclidean distance of the full D-dimensional points (distance d_E in ref.[Skokos2016]). If however the Metric is Cityblock(), calculate the absolute distance of only the first elements of the m+k and n+k points of R (distance d_F in ref.[Skokos2016], useful when R comes from delay embedding).\n\n[Skokos2016]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 1 (section 1.3.2), Lecture Notes in Physics 915, Springer (2016)\n\n[Kantz1994]: Kantz, H., Phys. Lett. A 185, pp 77–87 (1994)\n\n\n\n\n\n","category":"function"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The function numericallyapunov has a total of 4 different approaches for the algorithmic process, by combining 2 types of distances with 2 types of neighborhoods.","category":"page"},{"location":"chaos/nlts/#Example-of-Numerical-Lyapunov-computation","page":"Nonlinear Timeseries Analysis","title":"Example of Numerical Lyapunov computation","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nds = Systems.henon()\ndata = trajectory(ds, 100000)\nx = data[:, 1] #fake measurements for the win!\n\nks = 1:20\nℜ = 1:10000\nfig = figure(figsize=(10,6))\n\nfor (i, di) in enumerate([Euclidean(), Cityblock()])\n    subplot(1, 2, i)\n    ntype = NeighborNumber(2)\n    title(\"Distance: $(di)\", size = 18)\n    for D in [2, 4, 7]\n        R = embed(x, D, 1)\n        E = numericallyapunov(R, ks;\n        refstates = ℜ, distance = di, ntype = ntype)\n        Δt = 1\n        λ = linear_region(ks.*Δt, E)[2]\n        # gives the linear slope, i.e. the Lyapunov exponent\n        plot(ks .- 1, E .- E[1], label = \"D=$D, λ=$(round(λ, digits = 3))\")\n        legend()\n        tight_layout()\n    end\nend\nfig.savefig(\"numerlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/#Bad-Time-axis-(ks)-length","page":"Nonlinear Timeseries Analysis","title":"Bad Time-axis (ks) length","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"danger: Large `ks`\nThis simply cannot be stressed enough! It is just too easy to overshoot the range at which the exponential expansion region is valid!","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Let's revisit the example of the previous section:","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ds = Systems.henon()\ndata = trajectory(ds, 100000)\nx = data[:, 1]\nlength(x)","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The timeseries of such length could be considered big. A time length of 100 seems very small. Yet it turns out it is way too big! The following","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks = 1:100\nR = embed(x, 2, 1)\nE = numericallyapunov(R, ks, ntype = NeighborNumber(2))\nfig = figure()\nplot(ks .- 1, E .- E[1])\ntitle(\"Lyappunov: $(linear_region(ks, E)[2])\")\nfig.savefig(\"badlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Notice that even though this value for the Lyapunov exponent is correct, it happened to be correct simply due to the jitter of the saturated region. Since the saturated region is much bigger than the linear scaling region, if it wasn't that jittery the function linear_region would not give the scaling of the linear region, but instead a slope near 0! (or if you were to give bigger tolerance as a keyword argument)","category":"page"},{"location":"chaos/nlts/#Case-of-a-Continuous-system","page":"Nonlinear Timeseries Analysis","title":"Case of a Continuous system","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"The process for continuous systems works identically with discrete, but one must be a bit more thoughtful when choosing parameters. The following example helps the users get familiar with the process:","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nds = Systems.lorenz()\n# create a timeseries of 1 dimension\ndt = 0.05\nx = trajectory(ds, 1000.0; dt = dt)[:, 1]","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"We know that we have to use much bigger ks than 1:20, because this is a continuous case! (See reference given in numericallyapunovspectrum)","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks1 = 0:200","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"and in fact it is even better to not increment the ks one by one but instead do","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"ks2 = 0:4:200","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Now we plot some example computations","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"figure()\nntype = NeighborNumber(5) #5 nearest neighbors of each state\n\nfor d in [4, 8], τ in [7, 15]\n    r = embed(x, d, τ)\n\n    # E1 = numericallyapunov(r, ks1; ntype)\n    # λ1 = linear_region(ks1 .* dt, E1)[2]\n    # plot(ks1,E1.-E1[1], label = \"dense, d=$(d), τ=$(τ), λ=$(round(λ1, 3))\")\n\n    E2 = numericallyapunov(r, ks2; ntype)\n    λ2 = linear_region(ks2 .* dt, E2)[2]\n    plot(ks2,E2.-E2[1], label = \"d=$(d), τ=$(τ), λ=$(round(λ2, digits = 3))\")\nend\n\nlegend()\nxlabel(\"k (0.05×t)\")\nylabel(\"E - E(0)\")\ntitle(\"Continuous Reconstruction Lyapunov\")\ntight_layout()\nsavefig(\"continuousnumlyap.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"As you can see, using τ = 15 is not a great choice! The estimates with τ = 7 though are very good (the actual value is around λ ≈ 0.89...).","category":"page"},{"location":"chaos/nlts/#Broomhead-King-Coordinates","page":"Nonlinear Timeseries Analysis","title":"Broomhead-King Coordinates","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"broomhead_king","category":"page"},{"location":"chaos/nlts/#ChaosTools.broomhead_king","page":"Nonlinear Timeseries Analysis","title":"ChaosTools.broomhead_king","text":"broomhead_king(s::AbstractVector, d::Int) -> U, S, Vtr\n\nReturn the Broomhead-King coordinates of a timeseries s by performing svd on high-dimensional embedding if s with dimension d with minimum delay.\n\nDescription\n\nBroomhead and King coordinates is an approach proposed in [Broomhead1987] that applies the Karhunen–Loève theorem to delay coordinates embedding with smallest possible delay.\n\nThe function performs singular value decomposition on the d-dimensional matrix X of s,\n\nX = frac1sqrtNleft(\nbeginarraycccc\nx_1  x_2  ldots  x_d \nx_2  x_3  ldots  x_d+1\nvdots  vdots  vdots  vdots \nx_N-d+1  x_N-d+2 ldots  x_N\nendarray\nright) = Ucdot S cdot V^tr\n\nwhere x = s - bars. The columns of U can then be used as a new coordinate system, and by considering the values of the singular values S you can decide how many columns of U are \"important\". See the documentation page for example application.\n\n[Broomhead1987]: D. S. Broomhead, R. Jones and G. P. King, J. Phys. A 20, 9, pp L563 (1987)\n\n\n\n\n\n","category":"function"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"This alternative/improvement of the traditional delay coordinates can be a very powerful tool. An example where it shines is noisy data where there is the effect of superficial dimensions due to noise.","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Take the following example where we produce noisy data from a system and then use Broomhead-King coordinates as an alternative to \"vanilla\" delay coordinates:","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"using DynamicalSystems, PyPlot\n\nds = Systems.gissinger()\ndata = trajectory(ds, 1000.0, dt = 0.05)\nx = data[:, 1]\n\nL = length(x)\ns = x .+ 0.5rand(L) #add noise\n\nU, S = broomhead_king(s, 40)\nsummary(U)","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Now let's simply compare the above result with the one you get from doing a \"standard\" call to embed:","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"fig=figure(figsize= (10,6))\nsubplot(1,2,1)\nplot(U[:, 1], U[:, 2])\ntitle(\"Broomhead-King of s\")\n\nsubplot(1,2,2)\nR = embed(s, 2, 30)\nplot(columns(R)...; color = \"C3\")\ntitle(\"2D embedding of s\")\ntight_layout()\nfig.savefig(\"broomhead_king.png\"); nothing # hide","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"(Image: )","category":"page"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"we have used the same system as in the Delay Coordinates Embedding example, and picked the optimal delay time of τ = 30 (for same dt = 0.05). Regardless, the vanilla delay coordinates is much worse than the Broomhead-King coordinates.","category":"page"},{"location":"chaos/nlts/#Nearest-Neighbor-Prediction","page":"Nonlinear Timeseries Analysis","title":"Nearest Neighbor Prediction","text":"","category":"section"},{"location":"chaos/nlts/","page":"Nonlinear Timeseries Analysis","title":"Nonlinear Timeseries Analysis","text":"Nearest neighbor timeseries prediction is a method commonly listed under nonlinear timeseries analysis. This is not part of DynamicalSystems.jl, because in JuliaDynamics we have a dedicated package for this, TimeseriesPrediction.jl.","category":"page"},{"location":"entropies/estimators/#Probabilities-Estimators","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"In this page we list the various estimators (and further functions) that can be used to obtain probabilities representing a given dataset, or entropies directly. See Entropies & Probabilities for more.","category":"page"},{"location":"entropies/estimators/#Visitation-frequency-(binning)","page":"Probabilities Estimators","title":"Visitation frequency (binning)","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"VisitationFrequency","category":"page"},{"location":"entropies/estimators/#Entropies.VisitationFrequency","page":"Probabilities Estimators","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r.\n\nExample\n\n# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\nb = RectangularBinning(5)\n\n# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n# over the boxes of the binning, constructed as describedon the previous line.\nest = VisitationFrequency(b)\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Specifying-binning/boxes","page":"Probabilities Estimators","title":"Specifying binning/boxes","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"RectangularBinning","category":"page"},{"location":"entropies/estimators/#Entropies.RectangularBinning","page":"Probabilities Estimators","title":"Entropies.RectangularBinning","text":"RectangularBinning(ϵ) <: RectangularBinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme ϵ.  Binning instructions are deduced from the type of ϵ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning  is applied, as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting   from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size ϵ[i], starting   from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\nϵ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals   along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides   each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into 10 equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into 10 equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. \n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid  that is fixed over the intervals [x₁, x₂] for the first dimension, over [y₁, y₂] for the second dimension, and over [z₁, z₂] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded). \n\nThe following binning specification produces the desired (hyper-)rectangular boxes. \n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nx₁, x₂ = 0.5, 1 # not completely covering the data, which are on [0, 1]\ny₁, y₂ = -2, 1.5 # covering the data, which are on [0, 1]\nz₁, z₂ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nϵ = [(x₁, x₂), (y₁, y₂), (z₁, z₂)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(ϵ)\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#CountOccurrences-(counting)","page":"Probabilities Estimators","title":"CountOccurrences (counting)","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"CountOccurrences","category":"page"},{"location":"entropies/estimators/#Entropies.CountOccurrences","page":"Probabilities Estimators","title":"Entropies.CountOccurrences","text":"CountOccurrences  <: CountingBasedProbabilityEstimator\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. From these counts, construct histograms. Sum-normalize histograms to obtain probability distributions.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Kernel-density","page":"Probabilities Estimators","title":"Kernel density","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"NaiveKernel","category":"page"},{"location":"entropies/estimators/#Entropies.NaiveKernel","page":"Probabilities Estimators","title":"Entropies.NaiveKernel","text":"NaiveKernel(ϵ::Real, method::KernelEstimationMethod = TreeDistance()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by  counting how many other points occupy the space spanned by  a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( mathbfx epsilon) approx dfrac1N sum_s neq i  Kleft( dfracmathbfx_i - mathbfx_s epsilon right)\n\nwhere K(z) = 1 if z  1 and zero otherwise. Probabilities are then normalized.\n\nMethods\n\nTree-based evaluation of distances using TreeDistance. Faster, but more   memory allocation.\nDirect evaluation of distances using DirectDistance. Slower, but less    memory allocation. Also works for complex numbers.\n\nEstimation\n\nProbabilities or entropies can be estimated from Datasets.\n\nprobabilities(x::AbstractDataset, est::NaiveKernel). Associates a probability p to    each point in x.\ngenentropy(x::AbstractDataset, est::NaiveKernel).  Associate probability p to each    point in x, then compute the generalized entropy from those probabilities.\n\nExamples\n\nusing Entropy, DelayEmbeddings\npts = Dataset([rand(5) for i = 1:10000]);\nϵ = 0.2\nest_direct = NaiveKernel(ϵ, DirectDistance())\nest_tree = NaiveKernel(ϵ, TreeDistance())\n\np_direct = probabilities(pts, est_direct)\np_tree = probabilities(pts, est_tree)\n\n# Check that both methods give the same probabilities\nall(p_direct .== p_tree)\n\nSee also: DirectDistance, TreeDistance.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Distance-evaluation-methods","page":"Probabilities Estimators","title":"Distance evaluation methods","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"TreeDistance\nDirectDistance","category":"page"},{"location":"entropies/estimators/#Entropies.TreeDistance","page":"Probabilities Estimators","title":"Entropies.TreeDistance","text":"TreeDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated using a tree-based approach with the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Entropies.DirectDistance","page":"Probabilities Estimators","title":"Entropies.DirectDistance","text":"DirectDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated directly using the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Example","page":"Probabilities Estimators","title":"Example","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point p, measured by how many points are within radius 1.5 of p. Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot.","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"using Distributions, PyPlot, DelayEmbeddings, Entropies\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = Dataset(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(D, NaiveKernel(1.5))\nfigure()\nsurf(x, y, p.p)\nxlabel(\"x\"); ylabel(\"y\")\nsavefig(\"kernel_surface.png\"); nothing # hide","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"(Image: )","category":"page"},{"location":"entropies/estimators/#Nearest-neighbor-estimators","page":"Probabilities Estimators","title":"Nearest neighbor estimators","text":"","category":"section"},{"location":"entropies/estimators/#Kraskov","page":"Probabilities Estimators","title":"Kraskov","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"Kraskov","category":"page"},{"location":"entropies/estimators/#Entropies.Kraskov","page":"Probabilities Estimators","title":"Entropies.Kraskov","text":"k-th nearest neighbour(kNN) based\n\nKraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on k-th nearest neighbor searches[Kraskov2004]. w is the number of nearest neighbors to exclude when searching for neighbours  (defaults to 0, meaning that only the point itself is excluded).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities cannot be obtained directly.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Kozachenko-Leonenko","page":"Probabilities Estimators","title":"Kozachenko-Leonenko","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"KozachenkoLeonenko","category":"page"},{"location":"entropies/estimators/#Entropies.KozachenkoLeonenko","page":"Probabilities Estimators","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko(; w::Int = 0) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on nearest neighbors. This implementation is based on Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzyńska and Gambin (2016)[Charzyńska2016].\n\nw is the Theiler window (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities cannot be obtained directly.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Example-2","page":"Probabilities Estimators","title":"Example","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"This example reproduces Figure in Charzyńska & Gambin (2016)[Charzyńska2016]. Both estimators nicely converge to the true entropy with increasing time series length. For a uniform 1D distribution U(0 1), the true entropy is 0 (red line).","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"using DynamicalSystems, PyPlot\nimport Distributions: Uniform, Normal\n\nNs = [100:100:500; 1000:1000:10000]\nEkl = Vector{Vector{Float64}}(undef, 0)\nEkr = Vector{Vector{Float64}}(undef, 0)\n\nest_nn = KozachenkoLeonenko(w = 0)\n# with k = 1, Kraskov is virtually identical to KozachenkoLeonenko, so pick a higher\n# number of neighbors\nest_knn = Kraskov(w = 0, k = 3)\n\nnreps = 50\nfor N in Ns\n    kl = Float64[]\n    kr = Float64[]\n    for i = 1:nreps\n        pts = Dataset([rand(Uniform(0, 1), 1) for i = 1:N]);\n        push!(kl, genentropy(pts, est_nn))\n         # with k = 1 almost identical\n        push!(kr, genentropy(pts, est_knn))\n    end\n    push!(Ekl, kl)\n    push!(Ekr, kr)\nend\n\n# Plot\nusing PyPlot, StatsBase\nf = figure(figsize = (5,6))\nax = subplot(211)\npx = PyPlot.plot(Ns, mean.(Ekl); color = \"C1\", label = \"KozachenkoLeonenko\");\nPyPlot.plot(Ns, mean.(Ekl) .+ StatsBase.std.(Ekl); color = \"C1\", label = \"\");\nPyPlot.plot(Ns, mean.(Ekl) .- StatsBase.std.(Ekl); color = \"C1\", label = \"\");\n\nxlabel(\"Time step\"); ylabel(\"Entropy (nats)\"); legend()\nay = subplot(212)\npy = PyPlot.plot(Ns, mean.(Ekr); color = \"C2\", label = \"Kraskov\");\nPyPlot.plot(Ns, mean.(Ekr) .+ StatsBase.std.(Ekr); color = \"C2\", label = \"\");\nPyPlot.plot(Ns, mean.(Ekr) .- StatsBase.std.(Ekr); color = \"C2\", label = \"\");\n\nxlabel(\"Time step\"); ylabel(\"Entropy (nats)\"); legend()\ntight_layout()\nPyPlot.savefig(\"nn_entropy_example.png\"); nothing # hide","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"(Image: )","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.","category":"page"},{"location":"entropies/estimators/#Permutation-(symbolic)","page":"Probabilities Estimators","title":"Permutation (symbolic)","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"SymbolicPermutation","category":"page"},{"location":"entropies/estimators/#Entropies.SymbolicPermutation","page":"Probabilities Estimators","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n\nSymbolic, permutation-based probabilities/entropy estimators.\n\nUses embedding dimension m = 3 with embedding lag tau = 1 by default. The minimum dimension m is 2 (there are no sorting permutations of single-element state vectors).\n\nRepeated values during symbolization\n\nIn the original implementation of permutation entropy [BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword lt accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (lt = Entropies.isless_rand). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n\nTo get the behaviour described in Bandt and Pompe (2002), use lt = Base.isless).\n\nProperties of original signal preserved\n\nSymbolicPermutation: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt & Pompe et al. (2002)[BandtPompe2002] and   Berger et al. (2019) [Berger2019].\nSymbolicWeightedPermutation: Like SymbolicPermutation, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)[Fadlallah2013].\nSymbolicAmplitudeAwarePermutation: Like SymbolicPermutation, but also encodes   amplitude information by considering a weighted combination of absolute amplitudes   of state vectors, and relative differences between elements of state vectors. See   description below for explanation of the weighting parameter A. This implementation   is based on Azami & Escudero (2016) [Azami2016].\n\nProbability estimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicProbabilityEstimator). Constructs state vectors   from x using embedding lag τ and embedding dimension m, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est),   then computer the order-α generalized entropy to the given base.\n\nSpeeding up repeated computations\n\nA pre-allocated integer symbol array s can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n\nNote: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that length(x) == length(s) if x is a Dataset, or length(s) == length(x) - (m-1)τ if x is a univariate signal that is to be embedded first.\n\nUse the following signatures (only works for SymbolicPermutation).\n\nprobabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities\nprobabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities\n\nMultivariate datasets\n\nAlthough not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the L existing state vectors mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator). Compute ordinal patterns of the   state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\ngenentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator). Computes probabilities from   symbol frequencies using probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator),   then computes the order-α generalized (permutation) entropy to the given base.\n\nCaveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nAll symbolic estimators use the same underlying approach to estimating probabilities.\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.\n\nEach unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nSymbolicPermutation\n\nThe probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [Fadlallah2013]),\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) sum_k=1^N mathbf1_uS(u) in Pi left(mathbfx_k^m tau right) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) N\n\nwhere the function mathbf1_A(u) is the indicator function of a set A. That     is, mathbf1_A(u) = 1 if u in A, and mathbf1_A(u) = 0 otherwise.\n\nSymbolicAmplitudeAwarePermutation\n\nAmplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N mathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1 sum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSymbolicWeightedPermutation\n\nWeighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0). Weights are dictated by the variance of the state vectors.\n\nLet the aritmetic mean of state vector mathbfx_i be denoted by\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nWeights are then computed as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nNote: in equation 7, section III, of the original paper, the authors write\n\nw_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2\n\nBut given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\nEntropy computation\n\nThe generalized order-α Renyi entropy[Rényi1960] can be computed over the probability distribution of symbols as H(m tau alpha) = dfracalpha1-alpha log left( sum_j=1^R p_j^alpha right). Permutation entropy, as described in Bandt and Pompe (2002), is just the limiting case as α to1, that is H(m tau) = - sum_j^R p(pi_j^m tau) ln p(pi_j^m tau).\n\nNote: Do not confuse the order of the generalized entropy (α) with the order m of the permutation entropy (which controls the symbol size). Permutation entropy is usually estimated with α = 1, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Example-3","page":"Probabilities Estimators","title":"Example","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"This example reproduces an example from Bandt and Pompe (2002), where the permutation entropy is compared with the largest Lyapunov exponents from time series of the chaotic logistic map. Entropy estimates using SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation are added here for comparison.","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"using DynamicalSystems, PyPlot\n\nds = Systems.logistic()\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nlyaps = Float64[]\nhs_perm = Float64[]\nhs_wtperm = Float64[]\nhs_ampperm = Float64[]\n\nbase = Base.MathConstants.e\nfor r in rs\n    ds.p[1] = r\n    push!(lyaps, lyapunov(ds, N_lyap))\n\n    x = trajectory(ds, N_ent) # time series\n    hperm = Entropies.genentropy(x, SymbolicPermutation(m = m, τ = τ), base = base)\n    hwtperm = Entropies.genentropy(x, SymbolicWeightedPermutation(m = m, τ = τ), base = base)\n    hampperm = Entropies.genentropy(x, SymbolicAmplitudeAwarePermutation(m = m, τ = τ), base = base)\n\n    push!(hs_perm, hperm); push!(hs_wtperm, hwtperm); push!(hs_ampperm, hampperm)\nend\n\nf = figure(figsize = (6, 8))\na1 = subplot(411)\nplot(rs, lyaps); ylim(-2, log(2)); ylabel(\"\\$\\\\lambda\\$\")\na1.axes.get_xaxis().set_ticklabels([])\nxlim(rs[1], rs[end]);\n\na2 = subplot(412)\nplot(rs, hs_perm; color = \"C2\"); xlim(rs[1], rs[end]);\nxlabel(\"\"); ylabel(\"\\$h_6 (SP)\\$\")\n\na3 = subplot(413)\nplot(rs, hs_wtperm; color = \"C3\"); xlim(rs[1], rs[end]);\nxlabel(\"\"); ylabel(\"\\$h_6 (SWP)\\$\")\n\na4 = subplot(414)\nplot(rs, hs_ampperm; color = \"C4\"); xlim(rs[1], rs[end]);\nxlabel(\"\\$r\\$\"); ylabel(\"\\$h_6 (SAAP)\\$\")\ntight_layout()\nsavefig(\"permentropy.png\"); nothing # hide","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"(Image: )","category":"page"},{"location":"entropies/estimators/#Time-scale-(wavelet)","page":"Probabilities Estimators","title":"Time-scale (wavelet)","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"TimeScaleMODWT","category":"page"},{"location":"entropies/estimators/#Entropies.TimeScaleMODWT","page":"Probabilities Estimators","title":"Entropies.TimeScaleMODWT","text":"TimeScaleMODWT <: WaveletProbabilitiesEstimator\nTimeScaleMODWT(wl::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{12}())\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. This implementation is based on Rosso et al. (2001)[Rosso2001]. Optionally specify a wavelet to be used.\n\nThe probability p[i] is the relative/total energy for the i-th wavelet scale.\n\nExample\n\nManually picking a wavelet is done as follows.\n\nusing Entropies, Wavelets\nN = 200\na = 10\nt = LinRange(0, 2*a*π, N)\nx = sin.(t .+  cos.(t/0.1)) .- 0.1;\n\n# Pick a wavelet (if no wavelet provided, defaults to Wavelets.WL.Daubechies{12}())\nwl = Wavelets.WT.Daubechies{12}()\n\n# Compute the probabilities (relative energies) at the different wavelet scales\nprobabilities(x, TimeScaleMODWT(wl))\n\nIf no wavelet provided, the default is Wavelets.WL.Daubechies{12}()).\n\n[Rosso2001]: Rosso, O. A., Blanco, S., Yordanova, J., Kolev, V., Figliola, A., Schürmann, M., & Başar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"entropies/estimators/#Example-4","page":"Probabilities Estimators","title":"Example","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales).","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"using DynamicalSystems, PyPlot\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+  cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nest = TimeScaleMODWT()\nh_x = genentropy(x, est)\nh_y = genentropy(y, est)\nh_z = genentropy(z, est)\n\nf = figure(figsize = (10,6))\nax = subplot(311)\npx = plot(t, x; color = \"C1\", label = \"h=$(h=round(h_x, sigdigits = 5))\");\nylabel(\"x\"); legend()\nay = subplot(312)\npy = plot(t, y; color = \"C2\", label = \"h=$(h=round(h_y, sigdigits = 5))\");\nylabel(\"y\"); legend()\naz = subplot(313)\npz = plot(t, z; color = \"C3\", label = \"h=$(h=round(h_z, sigdigits = 5))\");\nylabel(\"z\"); xlabel(\"Time\"); legend()\ntight_layout()\nsavefig(\"waveletentropy.png\"); nothing # hide","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"(Image: )","category":"page"},{"location":"entropies/estimators/#Utility-methods","page":"Probabilities Estimators","title":"Utility methods","text":"","category":"section"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"Some convenience functions for symbolization are provided.","category":"page"},{"location":"entropies/estimators/","page":"Probabilities Estimators","title":"Probabilities Estimators","text":"Entropies.encode_as_bin\nEntropies.joint_visits\nEntropies.marginal_visits\nEntropies.symbolize\nEntropies.encode_motif","category":"page"},{"location":"entropies/estimators/#Entropies.encode_as_bin","page":"Probabilities Estimators","title":"Entropies.encode_as_bin","text":"encode_as_bin(point, reference_point, edgelengths) → Vector{Int}\n\nEncode a point into its integer bin labels relative to some reference_point (always counting from lowest to highest magnitudes), given a set of box  edgelengths (one for each axis). The first bin on the positive side of  the reference point is indexed with 0, and the first bin on the negative  side of the reference point is indexed with -1.\n\nSee also: joint_visits, marginal_visits.\n\nExample\n\nusing Entropies\n\nrefpoint = [0, 0, 0]\nsteps = [0.2, 0.2, 0.3]\nencode_as_bin(rand(3), refpoint, steps)\n\n\n\n\n\n","category":"function"},{"location":"entropies/estimators/#Entropies.joint_visits","page":"Probabilities Estimators","title":"Entropies.joint_visits","text":"joint_visits(points, binning_scheme::RectangularBinning) → Vector{Vector{Int}}\n\nDetermine which bins are visited by points given the rectangular binning scheme ϵ. Bins are referenced relative to the axis minima, and are  encoded as integers, such that each box in the binning is assigned a unique integer array (one element for each dimension). \n\nFor example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.\n\nSee also: marginal_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\n\npts = Dataset([rand(5) for i = 1:100]);\njoint_visits(pts, RectangularBinning(0.2))\n\n\n\n\n\n","category":"function"},{"location":"entropies/estimators/#Entropies.marginal_visits","page":"Probabilities Estimators","title":"Entropies.marginal_visits","text":"marginal_visits(points, binning_scheme::RectangularBinning, dims) → Vector{Vector{Int}}\n\nDetermine which bins are visited by points given the rectangular binning scheme ϵ, but only along the desired dimensions dims. Bins are referenced  relative to the axis minima, and are encoded as integers, such that each box  in the binning is assigned a unique integer array (one element for each  dimension in dims). \n\nFor example, if a bin is visited three times, then the corresponding  integer array will appear three times in the array returned.\n\nSee also: joint_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\npts = Dataset([rand(5) for i = 1:100]);\n\n# Marginal visits along dimension 3 and 5\nmarginal_visits(pts, RectangularBinning(0.3), [3, 5])\n\n# Marginal visits along dimension 2 through 5\nmarginal_visits(pts, RectangularBinning(0.3), 2:5)\n\n\n\n\n\nmarginal_visits(joint_visits, dims) → Vector{Vector{Int}}\n\nIf joint visits have been precomputed using joint_visits, marginal  visits can be returned directly without providing the binning again  using the marginal_visits(joint_visits, dims) signature.\n\nSee also: joint_visits, encode_as_bin.\n\nExample\n\nusing DelayEmbeddings, Entropies\npts = Dataset([rand(5) for i = 1:100]);\n\n# First compute joint visits, then marginal visits along dimensions 1 and 4\njv = joint_visits(pts, RectangularBinning(0.2))\nmarginal_visits(jv, [1, 4])\n\n# Marginals along dimension 2\nmarginal_visits(jv, 2)\n\n\n\n\n\n","category":"function"},{"location":"entropies/estimators/#Entropies.symbolize","page":"Probabilities Estimators","title":"Entropies.symbolize","text":"symbolize(x::AbstractVector{T}, est::SymbolicPermutation) where {T} → Vector{Int}\nsymbolize!(s, x::AbstractVector{T}, est::SymbolicPermutation) where {T} → Vector{Int}\n\nIf x is a univariate time series, first x create a delay reconstruction of x using embedding lag est.τ and embedding dimension est.m, then symbolizing the resulting state vectors with encode_motif.\n\nOptionally, the in-place symbolize! can be used to put symbols in a pre-allocated integer vector s, where length(s) == length(x)-(est.m-1)*est.τ.\n\nsymbolize(x::AbstractDataset{m, T}, est::SymbolicPermutation) where {m, T} → Vector{Int}\nsymbolize!(s, x::AbstractDataset{m, T}, est::SymbolicPermutation) where {m, T} → Vector{Int}\n\nIf x is an m-dimensional dataset, then motif lengths are determined by the dimension of the input data, and x is symbolized by converting each m-dimensional state vector as a unique integer in the range 1 2 ldots m-1, using encode_motif.\n\nOptionally, the in-place symbolize! can be used to put symbols in a pre-allocated integer vector s, where length(s) == length(s).\n\nExamples\n\nSymbolize a 7-dimensional dataset. Motif lengths (or order of the permutations) are inferred to be 7.\n\nusing DelayEmbeddings, Entropies\nD = Dataset([rand(7) for i = 1:1000])\ns = symbolize(D, SymbolicPermutation())\n\nSymbolize a univariate time series by first embedding it in dimension 5 with embedding lag 2. Motif lengths (or order of the permutations) are therefore 5.\n\nusing DelayEmbeddings, Entropies\nn = 5000\nx = rand(n)\ns = symbolize(x, SymbolicPermutation(m = 5, τ = 2))\n\nThe integer vector s now has length n-(m-1)*τ = 4992, and each s[i] contains the integer symbol for the ordinal pattern of state vector x[i].\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"entropies/estimators/#Entropies.encode_motif","page":"Probabilities Estimators","title":"Entropies.encode_motif","text":"encode_motif(x, m::Int = length(x)) → s::Int\n\nEncode the length-m motif x (a vector of indices that would sort some vector v  in ascending order) into its unique integer symbol s in 1 2 ldots m - 1 ,  using Algorithm 1 in Berger et al. (2019)[Berger2019]. \n\nExample\n\nv = rand(5)\n\n# The indices that would sort `v` in ascending order. This is now a permutation \n# of the index permutation (1, 2, ..., 5)\nx = sortperm(v)\n\n# Encode this permutation as an integer.\nencode_motif(x)\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: DynamicalSystems.jl logo: The Double Pendulum)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl is an award-winning Julia software library for dynamical systems, nonlinear dynamics, deterministic chaos and nonlinear timeseries analysis. It is part of JuliaDynamics, an organization dedicated to creating high quality scientific software.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"To learn how to use this library please see Getting started below, and subsequently, the Contents page to get an overview of all offered functionality of DynamicalSystems.jl.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"tip: Latest news\nRework and improvement of optimal embedding dimension: optimal_traditional_de!","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Star us on GitHub!\nIf you have found this library useful, please consider starring it on GitHub. This gives us an accurate lower bound of the (satisfied) user count.","category":"page"},{"location":"#Getting-started","page":"Introduction","title":"Getting started","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl is a collection of Julia packages bundled together under a single package DynamicalSystems. To install this bundle you can do:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Pkg; Pkg.add(\"DynamicalSystems\")","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The documentation you are reading now was built with the following stable versions:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.status([\n    \"DelayEmbeddings\", \"RecurrenceAnalysis\",\n    \"DynamicalSystemsBase\", \"ChaosTools\",\n    \"Entropies\",\n])","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The individual packages that compose DynamicalSystems interact flawlessly with each other because of the following two structures:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The DynamicalSystem represents a dynamical system with known dynamic rule f. The system can be in discrete time (often called a map), vecu_n+1 = vecf(vecu_n p n), or in continuous time (often called an ordinary differential equation) fracdvecudt = vecf(vecu p t). In both cases u is the state of the dynamical system and p a parameter container. You should have a look at the page Dynamical System Definition for how to create this object. A list of several pre-defined systems exists in the Predefined Dynamical Systems page.\nNumerical data, that can represent measured experiments, sampled trajectories of dynamical systems, or just sets in the state space, are represented by Dataset, which is a container of equally-sized data points. Timeseries in DynamicalSystems.jl are represented by the already existing Vector type of the Julia language.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"These core structures DynamicalSystem, Dataset are used throughout the package to do useful calculations often used in the field of nonlinear dynamics and chaos. For example, using lyapunovspectrum and DynamicalSystem gives you the Lyapunov exponents of a dynamical system with known equations of motion. Alternatively, by using numericallyapunov and Dataset you can approximate the maximum Lyapunov exponent of a measured trajectory.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"All things possible in DynamicalSystems.jl are listed in the Contents page.","category":"page"},{"location":"#Tutorials","page":"Introduction","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Tutorials for DynamicalSystems.jl exist in the form of Jupyter notebooks.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In addition, a full 2-hours YouTube tutorial is available below:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"<iframe width=\"560\" height=\"400\" src=\"https://www.youtube.com/embed/A8g9rdEfdNg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Introductory textbooks\nOur library assumes some basic knowledge of nonlinear dynamics and complex systems.If you are new to the field but want to learn more, we can suggest the following textbooks as introductions:Chaos in Dynamical Systems - E. Ott\nNonlinear Time series Analysis - H. Kantz & T. Schreiber","category":"page"},{"location":"#Advanced-Installation","page":"Introduction","title":"Advanced Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Notice that for targeted usage of DynamicalSystems (e.g. you only need a specific function like lyapunovspectrum or rqa), you don't have to install the entire DynamicalSystems suite. You can leave with only installing the necessary package that exports the function you need. You see this information prefacing the function. E.g. for rqa you will see RecurrenceAnalysis.rqa, which means that you need to install RecurrenceAnalysis to use it. ","category":"page"},{"location":"#Our-Goals","page":"Introduction","title":"Our Goals","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"DynamicalSystems.jl was created with three goals in mind. The first was to fill the missing gap of a software for nonlinear dynamics and chaos of the highest quality (none exist in any programming language). The second was to create a useful library where students and scientists from different fields may come and learn about methods of nonlinear dynamics and chaos.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The third was to fundamentally change the perception of the role of code in both scientific education as well as research. It is rarely the case that real, runnable code is shown in the classroom, because it is often long and messy. This is especially hurtful for nonlinear dynamics, a field where computer-assisted exploration is critical. But published work in this field fares even worse, with the overwhelming majority of published research not sharing the code used to create the paper. This makes reproducing these papers difficult, while some times straight-out impossible.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"To achieve these goals we made DynamicalSystems.jl so that it is:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Transparent: extra care is taken so that the source code of all functions is clear and easy to follow, while remaining as small and concise as possible.\nIntuitive: a software simple to use and understand makes experimentation easier.\nEasy to install, easy to extend: This makes contributions more likely, and can motivate researchers to implement their method here, instead of leaving it in a cryptic script stored in some data server, never-to-be-published with the paper.\nReliable: the algorithm implementations are tested extensively.\nWell-documented: all implemented algorithms provide a high-level scientific description of their functionality in their documentation string as well as references to scientific papers.\nGeneral: all algorithms work just as well with any system, whether it is a simple continuous chaotic system, like the Lorenz model, or a high dimensional discrete system like coupled standard maps.\nPerformant: written entirely in Julia, and taking advantage of some of the best packages within the language, DynamicalSystems.jl is really fast.","category":"page"},{"location":"#Citing","page":"Introduction","title":"Citing","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"There is a (small) paper associated with DynamicalSystems.jl. If we have helped you in research that led to a publication, please be kind enough to cite it, using the DOI 10.21105/joss.00598 or the following BiBTeX entry:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"@article{Datseris2018,\n  doi = {10.21105/joss.00598},\n  url = {https://doi.org/10.21105/joss.00598},\n  year  = {2018},\n  month = {mar},\n  volume = {3},\n  number = {23},\n  pages = {598},\n  author = {George Datseris},\n  title = {DynamicalSystems.jl: A Julia software library for chaos and nonlinear dynamics},\n  journal = {Journal of Open Source Software}\n}","category":"page"},{"location":"#Issues-with-Bounties","page":"Introduction","title":"Issues with Bounties","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Money that DynamicalSystems.jl obtains from awards, sponsors or donators are converted into bounties for GitHub issues. The full list of issues that have a bounty is available here.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"By solving these issues you not only contribute to open source, but you also get some pocket money to boot :)","category":"page"},{"location":"#Contacting","page":"Introduction","title":"Contacting","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Feel free to open issues on GitHub if you have questions and/or suggestions. You can also join our chatroom for discussions and/or questions about the packages of the JuliaDynamics organization! If you are using the Julia Slack workplace, please join the channel #dynamics-bridged.","category":"page"},{"location":"#Contributing-and-Donating","page":"Introduction","title":"Contributing & Donating","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"TL;DR: See \"good first issues\" or \"wanted features\".","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Be sure to visit the Contributor Guide page, because you can help make this package better without having to write a single line of code! Also, if you find this package helpful please consider staring it on GitHub! This gives us an accurate lower bound of users that this package has already helped!","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Finally, you can donate for the development of DynamicalSystems.jl. You can do that by adding bounties to existing issues on the GitHub repositories (you can open new issues as well). Every issue has an automatic way to create a bounty using Bountysource, see the first comment of each issue.","category":"page"},{"location":"#Maintainers-and-Contributors","page":"Introduction","title":"Maintainers and Contributors","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The DynamicalSystems.jl software is maintained by George Datseris, who is also curating and writing this documentation page.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The software code however is built from the contributions of several individuals. For an accurate list of the names as well as contributions of each one, please visit the GitHub's contributor list for the sub-packages of DynamicalSystems.jl, e.g. ChaosTools.jl.","category":"page"},{"location":"rqa/networks/#Recurrence-Networks","page":"Recurrence Network Analysis","title":"Recurrence Networks","text":"","category":"section"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"Recurrence matrices can be reinterpreted as adjacency matrices of complex networks embedded in state space, such that each node or vertex of the network corresponds to a point of the timeseries, and the links of the network connect pairs of points that are mutually close the phase space. The relationship between a recurrence matrix R and its corresponding adjacency matrix A is:","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"Rij = Aij - deltaij","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"i.e. there is an edge in the associated network between every two neighboring points in the phase space, excluding self-connections (points in the Line Of Identity or main diagonal of R).","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"This definition assumes that A represents an undirected graph, so R must be a symmetric matrix as corresponding to a RecurrenceMatrix or a JointRecurrenceMatrix.","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"While RQA characterizes the properties of line structures in the recurrence plots, which consider dynamical aspects (e.g. continuity of recurrences, length of sequences, etc.), the analysis of recurrence networks does not take into account time information, since network properties are independent of the ordering of vertices. On the other hand, recurrence network analysis (RNA) provides information about geometric characteristics of the state space, like homogeneity of the connections, clustering of points, etc. More details about the theoretical framework of RNA can be found in the following papers:","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"R.V. Donner et al. \"Recurrence networks — a novel paradigm for nonlinear time series analysis\", New Journal of Physics 12, 033025 (2010)\nR.V. Donner et al. \"Complex Network Analysis of Recurrences\", in: Webber, C.L. & Marwan N. (eds.) Recurrence Quantification Analysis. Theory and Best Practices, Springer, pp. 101-165 (2015).","category":"page"},{"location":"rqa/networks/#Creation-and-visualization-of-Recurrence-Networks","page":"Recurrence Network Analysis","title":"Creation and visualization of Recurrence Networks","text":"","category":"section"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"The JuliaGraphs organization provides multiple packages for Julia to create, visualize and analyze complex networks. In particular, the package LightGraphs defines the type SimpleGraph that can be used to represent undirected networks. Such graphs can be created from symmetric recurrence matrices, as in the following example with a Hénon map:","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"using DynamicalSystems\nusing LightGraphs\n\nhe = Systems.henon([0.75, 0.15])\ntr = trajectory(he, 200)\nR = RecurrenceMatrix(tr, 0.25; metric = Chebyshev())\nnetwork = SimpleGraph(R)","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"There are various plotting tools that can be used to visualize such graphs. For instance, the following plot made with the package GraphPlot displays the links between the nodes of the network, which are positioned in the corresponding X-Y coordinates of the original time series for clarity:","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"using GraphPlot\ngplot(network, tr[:,1], tr[:,2])","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"(Image: )","category":"page"},{"location":"rqa/networks/#Recurrence-Network-measures","page":"Recurrence Network Analysis","title":"Recurrence Network measures","text":"","category":"section"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"LightGraphs has a large set of functions to extract local measures (associated to particular vertices or edges) and global coefficients associated to the whole network. For SimpleGraphs created from recurrence matrices, as the variable network in the previous example, the vertices are labelled with numeric indices following the same ordering as the rows or columns of the given matrix. So for instance degree(network, i) would give the degree of the i-th point of the timeseries (number of connections with other points), whereas degree(network) would give a vector of such measures ordered as the original timeseries.","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"As in RQA, we provide a function that computes a selection of commonly used global RNA measures, directly from the recurrence matrix:","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"rna","category":"page"},{"location":"rqa/networks/#RecurrenceAnalysis.rna","page":"Recurrence Network Analysis","title":"RecurrenceAnalysis.rna","text":"rna(R)\nrna(args...; kwargs...)\n\nCalculate a set of Recurrence Network parameters. The input R can be a symmetric recurrence matrix that is interpreted as the adjacency matrix of an undirected complex network, such that linked vertices are neighboring points in the phase space.\n\nAlternatively, the inputs can be a graph object or any  valid inputs to the SimpleGraph constructor of the LightGraphs package.\n\nReturn\n\nThe returned value is a dictionary that contains the following entries, with the corresponding global network properties[1, 2]:\n\n:density: edge density, approximately equivalent to the global recurrence rate in the phase space.\n:transitivity: network transitivity, which describes the\n\nglobal clustering of points following Barrat's and Weigt's formulation [3].\n\n:averagepath: mean value of the shortest path lengths taken over\n\nall pairs of connected vertices, related to the average separation between points in the phase.\n\n:diameter: maximum value of the shortest path lengths between\n\npairs of connected vertices, related to the phase space diameter.\n\nReferences\n\n[1]: R.V. Donner et al. \"Recurrence networks — a novel paradigm for nonlinear time series analysis\", New Journal of Physics 12, 033025 (2010) DOI:10.1088/1367-2630/12/3/033025\n\n[2]: R.V. Donner et al., The geometry of chaotic dynamics — a complex network perspective, Eur. Phys. J. B 84, 653–672 (2011) DOI:10.1140/epjb/e2011-10899-1\n\n[3]: A. Barrat & M. Weight, \"On the properties of small-world network models\", The European Physical Journal B 13, 547–560 (2000) DOI:10.1007/s100510050067\n\n\n\n\n\n","category":"function"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"note: Transitivity and global clustering coefficient\nThe concept of clustering coefficient at local level (for individual nodes of the network) is clearly defined as the fraction of connecting nodes that are also connected between them, forming \"triangles\". But at global level it is a source of confusion: the term of \"global clustering coefficient\" was originally used by Watts and Strogatz[1], referred to the average of local clustering coefficient across all the graph's nodes. But Barrat and Weigt proposed an alternative definition[2] that characterizes the effective global dimensionality of the system, giving equal weight to all triangles in the network[3].This second definition is often named with the distinctive term of \"transitivity\", as in the output of rna, whereas the corresponding function of the LightGraphs package is global_clustering_coefficient. The \"global clustering coefficient\" as by Watts and Strogatz could be obtained as mean(local_clustering_coefficient(network)) – with network being a graph object as in the previous example. (The function mean is in the Julia standard library, and can be brought into scope with the command using Statistics.)","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"[1]: D.J. Watts & S.H. Strogatz, \"Collective dynamics of 'small-world' networks\", Nature 393(6684), 440–442 (1998) DOI:10.1038%2F30918","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"[2]: A. Barrat & M. Weight, \"On the properties of small-world network models\", The European Physical Journal B 13, 547–560 (2000)  DOI:10.1007/s100510050067","category":"page"},{"location":"rqa/networks/","page":"Recurrence Network Analysis","title":"Recurrence Network Analysis","text":"[3]: R.V. Donner et al. \"Recurrence networks — a novel paradigm for nonlinear time series analysis\", New Journal of Physics 12, 033025 (2010) DOI:10.1088/1367-2630/12/3/033025","category":"page"},{"location":"embedding/unified/#Unified-Optimal-Embedding","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"","category":"section"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"Unified approaches try to create an optimal embedding by in parallel optimizing what combination of delay times and embedding dimensions suits best.","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"In addition, the unified approaches are the only ones that can accommodate multi-variate inputs. This means that if you have multiple measured input timeseries, you should be able to take advantage of all of them for the best possible embedding of the dynamical system's set.","category":"page"},{"location":"embedding/unified/#An-example","page":"Unified Optimal Embedding","title":"An example","text":"","category":"section"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"In following we illustrate the most recent unified optimal embedding method, called PECUZAL, on three examples. We start with a univariate case, i.e. we only feed in one time series, here the x-component of the Lorenz system.  ","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"using DynamicalSystems\n\nlo = Systems.lorenz([1.0, 1.0, 50.0])\ntr = trajectory(lo, 100; dt = 0.01, Ttr = 10)\n\ns = vec(tr[:, 1]) # input timeseries = x component of Lorenz\ntheiler = estimate_delay(s, \"mi_min\") # estimate a Theiler window\nTmax = 100 # maximum possible delay\n\nY, τ_vals, ts_vals, Ls , εs = pecuzal_embedding(s; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(τ_vals)\nprintln(ts_vals)\nprintln(Ls)\nprintln(\"L_total_uni: $(sum(Ls))\")","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"The output reveals that PECUZAL suggests a 3-dimensional embedding out of the un-lagged time series as the 1st component of the reconstruction, the time series lagged by 18 samples as the 2nd component and the time series lagged by 9 samples as the 3rd component. In the third embedding cycle there is no ΔL<0 and the algorithm breaks. The result after two successful embedding cycles is the 3-dimensional embedding shown above. The total obtained decrease of ΔL throughout all encountered embedding cycles has been ~-1.24.","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"using PyPlot\n\nfigure(figsize=(14., 8.))\nsubplot(1,2,1, projection=\"3d\")\nplot3D(Y[:,1], Y[:,2], Y[:,3],\"gray\")\ntitle(\"PECUZAL reconstructed\")\nxlabel(\"x(t+$(τ_vals[1]))\")\nylabel(\"x(t+$(τ_vals[2]))\")\nzlabel(\"x(t+$(τ_vals[3]))\")\ngrid()\n\nsubplot(1,2,2, projection=\"3d\")\nplot3D(tr[:,1], tr[:,2], tr[:,3],\"gray\")\ntitle(\"Original\")\nxlabel(\"x(t)\")\nylabel(\"y(t)\")\nzlabel(\"z(t)\")\ngrid()\n\ntight_layout()\nsavefig(\"pecuzal_uni.png\"); nothing # hide","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"(Image: )","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"We can also look at the output of the low-level function leading to the results, here the continuity statistic.","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"using PyPlot\n\nfigure()\nplot(εs[:,1], label=\"1st embedding cycle\")\nscatter([τ_vals[2]], [εs[τ_vals[2],1]])\nplot(εs[:,2], label=\"2nd embedding cycle\")\nscatter([τ_vals[3]], [εs[τ_vals[3],2]])\nplot(εs[:,3], label=\"3rd embedding cycle\")\ntitle(\"Continuity statistics PECUZAL Lorenz\")\nxlabel(\"delay τ\")\nylabel(\"⟨ε⋆⟩\")\nlegend(loc=\"center right\"; fontsize = 20)\ngrid()\nsavefig(\"continuity_uni.png\"); nothing # hide","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"(Image: )","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"The picked delay values are marked with filled circles. As already mentioned, the third embedding cycle did not contribute to the embedding, i.e. there has been no delay value chosen. Similar to the approach in the preceding example, we now highlight the capability of the PECUZAL embedding method for a multivariate input. The idea is now to feed in all three time series to the algorithm, even though this is a very far-from-reality example. We already have an adequate representation of the system we want to reconstruct, namely the three time series from the numerical integration. But let us see what PECUZAL suggests for a reconstruction.","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"# compute Theiler window\nw1 = estimate_delay(tr[:,1], \"mi_min\")\nw2 = estimate_delay(tr[:,2], \"mi_min\")\nw3 = estimate_delay(tr[:,3], \"mi_min\")\nw = maximum(hcat(w1,w2,w3))\nY_m, τ_vals_m, ts_vals_m, Ls_m , εs_m = pecuzal_embedding(tr; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(τ_vals_m)\nprintln(ts_vals_m)\nprintln(Ls_m)\nprintln(\"L_total_multi: $(sum(Ls_m))\")","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"PECUZAL returns a 6-dimensional embedding using the un-lagged z- and x-component as 1st and 3rd component of the reconstruction vectors, as well as the x-component lagged by 12, 79, 64, and 53 samples. The total decrease of ΔL is ~-1.64, and thus, way smaller compared to the univariate case, as we would expect it. Nevertheless, the main contribution to this increase is made by the first two embedding cycles. For surpressing embedding cycles, which yield negligible - but negative - ΔL-values one can use the keyword argument L_threshold   ","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"\nY_mt, τ_vals_mt, ts_vals_mt, Ls_mt , εs_mt = pecuzal_embedding(tr; τs = 0:Tmax , L_threshold = 0.05, w = theiler, econ = true)\n\nprintln(τ_vals_mt)\nprintln(ts_vals_mt)\nprintln(Ls_mt)\nprintln(\"L_total_multi_t: $(sum(Ls_mt))\")","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"Let's plot these three components:","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"\nts_str = [\"x\", \"y\", \"z\"]\n\nfigure(figsize=(14., 8.))\nsubplot(1,2,1, projection=\"3d\")\nplot3D(Y_m[:,1], Y_m[:,2], Y_m[:,3],\"gray\")\ntitle(\"PECUZAL reconstructed\")\nxlabel(\"$(ts_str[ts_vals_m[1]])(t+$(τ_vals_m[1]))\")\nylabel(\"$(ts_str[ts_vals_m[2]])(t+$(τ_vals_m[2]))\")\nzlabel(\"$(ts_str[ts_vals_m[3]])(t+$(τ_vals_m[3]))\")\ngrid()\n\nsubplot(1,2,2, projection=\"3d\")\nplot3D(tr[:,1], tr[:,2], tr[:,3],\"gray\")\ntitle(\"Original\")\nxlabel(\"x(t)\")\nylabel(\"y(t)\")\nzlabel(\"z(t)\")\ngrid()\n\ntight_layout()\nsavefig(\"pecuzal_multi.png\"); nothing # hide","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"(Image: )","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"Finally we show what PECUZAL does with a non-deterministic source:","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"using Random\n\n# Dummy input\nRandom.seed!(1234)\nd1 = randn(1000)\nd2 = rand(1000)\nTmax = 100\ndummy_set = Dataset(d1,d2)\n\nw1 = estimate_delay(d1, \"mi_min\")\nw2 = estimate_delay(d2, \"mi_min\")\ntheiler = min(w1, w2)\n\nY_d, τ_vals_d, ts_vals_d, Ls_d , ε★_d = pecuzal_embedding(dummy_set; τs = 0:Tmax , w = theiler, econ = true)\n\nsize(Y_d)","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"So, no (proper) embedding is done.","category":"page"},{"location":"embedding/unified/#All-unified-algorithms","page":"Unified Optimal Embedding","title":"All unified algorithms","text":"","category":"section"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"Several algorithms have been created to implement a unified approach to delay coordinates embedding. You can find some implementations below:","category":"page"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"pecora\nuzal_cost\ngarcia_almeida_embedding\nmdop_embedding\npecuzal_embedding","category":"page"},{"location":"embedding/unified/#DelayEmbeddings.pecora","page":"Unified Optimal Embedding","title":"DelayEmbeddings.pecora","text":"pecora(s, τs, js; kwargs...) → ⟨ε★⟩, ⟨Γ⟩\n\nCompute the (average) continuity statistic ⟨ε★⟩ and undersampling statistic ⟨Γ⟩ according to Pecora et al.[Pecoral2007] (A unified approach to attractor reconstruction), for a given input s (timeseries or Dataset) and input generalized embedding defined by (τs, js), according to genembed. The continuity statistic represents functional independence between the components of the existing embedding and one additional timeseries. The returned results are matrices with size TxJ.\n\nKeyword arguments\n\ndelays = 0:50: Possible time delay values delays (in sampling time units). For each of the τ's in delays the continuity-statistic ⟨ε★⟩ gets computed. If undersampling = true (see further down), also the undersampling statistic ⟨Γ⟩ gets returned for all considered delay values.\nJ = 1:dimension(s): calculate for all timeseries indices in J. If input s is a timeseries, this is always just 1.\nsamplesize::Real = 0.1: determine the fraction of all phase space points (=length(s)) to be considered (fiducial points v) to average ε★ to produce ⟨ε★⟩, ⟨Γ⟩\nK::Int = 13: the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic). ⟨ε★⟩ is computed taking the minimum result over all k ∈ K.\nmetric = Chebyshev(): metrix with which to find nearest neigbhors in the input embedding (ℝᵈ space, d = length(τs)).\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nundersampling = false : whether to calculate the undersampling statistic or not (if not, zeros are returned for ⟨Γ⟩). Calculating ⟨Γ⟩ is thousands of times slower than ⟨ε★⟩.\ndb::Int = 100: Amount of bins used into calculating the histograms of each timeseries (for the undersampling statistic).\nα::Real = 0.05: The significance level for obtaining the continuity statistic\np::Real = 0.5: The p-parameter for the binomial distribution used for the computation of the continuity statistic.\n\nDescription\n\nNotice that the full algorithm is too large to discuss here, and is written in detail (several pages!) in the source code of pecora.\n\n[Pecora2007]: Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007). A unified approach to attractor reconstruction. Chaos 17(1).\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.uzal_cost","page":"Unified Optimal Embedding","title":"DelayEmbeddings.uzal_cost","text":"uzal_cost(Y::Dataset; kwargs...) → L\n\nCompute the L-statistic L for input dataset Y according to Uzal et al.[Uzal2011], based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. It serves as a cost function of a state space trajectory/embedding and therefore allows to estimate a \"goodness of a embedding\" and also to choose proper embedding parameters, while minimizing L over the parameter space. For receiving the local cost function L_local (for each point in state space - not averaged), use uzal_cost_local(...).\n\nKeyword arguments\n\nsamplesize = 0.5: Number of considered fiducial points v as a fraction of input state space trajectory Y's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce L.\nK = 3: the amount of nearest neighbors considered, in order to compute σ_k^2 (read algorithm description). If given a vector, minimum result over all k ∈ K is returned.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input state space trajectory `Y.\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nTw = 40: The time horizon (in sampling units) up to which E_k^2 gets computed and averaged over (read algorithm description).\n\nDescription\n\nThe L-statistic is based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. Technically, it is the logarithm of the product of σ-statistic and a normalization statistic α:\n\nL = log10(σ*α)\n\nThe σ-statistic is computed as follows. σ = √σ² = √(E²/ϵ²). E² approximates the conditional variance at each point in state space and for a time horizon T ∈ Tw, using K nearest neighbors. For each reference point of the state space trajectory, the neighborhood consists of the reference point itself and its K+1 nearest neighbors. E² measures how strong a neighborhood expands during T time steps. E² is averaged over many time horizons T = 1:Tw. Consequently, ϵ² is the size of the neighborhood at the reference point itself and is defined as the mean pairwise distance of the neighborhood. Finally, σ² gets averaged over a range of reference points on the attractor, which is controlled by samplesize. This is just for performance reasons and the most accurate result will obviously be gained when setting samplesize=1.0\n\nThe α-statistic is a normalization factor, such that σ's from different embeddings can be compared. α² is defined as the inverse of the sum of the inverse of all ϵ²'s for all considered reference points.\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.garcia_almeida_embedding","page":"Unified Optimal Embedding","title":"DelayEmbeddings.garcia_almeida_embedding","text":"garcia_almeida_embedding(s; kwargs...) → Y, τ_vals, ts_vals, FNNs ,NS\n\nA unified approach to properly embed a time series (Vector type) or a set of time series (Dataset type) based on the papers of Garcia & Almeida [Garcia2005a],[Garcia2005b].\n\nKeyword arguments\n\nτs= 0:50: Possible delay values τs (in sampling time units). For each of the τs's the N-statistic gets computed.\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nr1 = 10: The threshold, which defines the factor of tolerable stretching for the d_E1-statistic.\nr2 = 2: The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension.\nfnn_thres= 0.05: A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1).\nT::Int = 1: The forward time step (in sampling units) in order to compute the d_E2-statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to T=1.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input phase space trajectory Y.\nmax_num_of_cycles = 50: The algorithm will stop after that many cycles no matter what.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding vectors Y. Based on the N-statistic the algorithm picks an optimal delay value τ for each embedding cycle as the first local minimum of N. In case of multivariate embedding, i.e. when embedding a set of time series (s::Dataset), the optimal delay value τ is chosen as the first minimum from all minimum's of all considered N-statistics for each embedding cycle. The range of considered delay values is determined in τs and for the nearest neighbor search we respect the Theiler window w. After each embedding cycle the FNN-statistic FNNs [Hegger1999][Kennel1992] is being checked and as soon as this statistic drops below the threshold fnn_thres, the algorithm breaks. In order to increase the  practability of the method the algorithm also breaks, when the FNN-statistic FNNs increases . The final embedding vector is stored in Y (Dataset). The chosen delay values for each embedding cycle are stored in the τ_vals and the according time series number chosen for the according delay value in τ_vals is stored in ts_vals. For univariate embedding (s::Vector) ts_vals is a vector of ones of length τ_vals, because there is simply just one time series to choose from. The function also returns the N-statistic NS for each embedding cycle as an Array of Vectors.\n\nNotice that we were not able to reproduce the figures from the papers with our implementation (which nevertheless we believe is the correct one).\n\n[Garcia2005a]: Garcia, S. P., Almeida, J. S. (2005). Nearest neighbor embedding with different time delays. Physical Review E 71, 037204.\n\n[Garcia2005b]: Garcia, S. P., Almeida, J. S. (2005). Multivariate phase space reconstruction by nearest neighbor embedding with different time delays. Physical Review E 72, 027205.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.mdop_embedding","page":"Unified Optimal Embedding","title":"DelayEmbeddings.mdop_embedding","text":"mdop_embedding(s::Vector; kwargs...) → Y, τ_vals, ts_vals, FNNs, βS\n\nMDOP (for \"maximizing derivatives on projection\") is a unified approach to properly embed a timeseries or a set of timeseries (Dataset) based on the paper of Chetan Nichkawde [Nichkawde2013].\n\nKeyword arguments\n\nτs= 0:50: Possible delay values τs. For each of the τs's the β-statistic gets computed.\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nfnn_thres::Real= 0.05: A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1).\nr::Real = 2: The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension.\nmax_num_of_cycles = 50: The algorithm will stop after that many cycles no matter what.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding Y. Based on the beta_statistic the algorithm picks an optimal delay value τ for each embedding cycle as the global maximum of β. In case of multivariate embedding, i.e. when embedding a set of time series (s::Dataset), the optimal delay value τ is chosen as the maximum from all maxima's of all considered β-statistics for each possible timeseries. The range of considered delay values is determined in τs and for the nearest neighbor search we respect the Theiler window w.\n\nAfter each embedding cycle the FNN-statistic FNNs [Hegger1999][Kennel1992] is being checked and as soon as this statistic drops below the threshold fnn_thres, the algorithm terminates. In order to increase the practability of the method the algorithm also terminates when the FNN-statistic FNNs increases.\n\nThe final embedding is returned as Y. The chosen delay values for each embedding cycle are stored in the τ_vals and the according timeseries index chosen for the the respective according delay value in τ_vals is stored in ts_vals. βS, FNNs are returned for clarity and double-checking, since they are computed anyway. In case of multivariate embedding, βS will store all β-statistics for all available time series in each embedding cycle. To double-check the actual used β-statistics in an embedding cycle 'k', simply βS[k][:,ts_vals[k+1]].\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Hegger1999]: Hegger, Rainer and Kantz, Holger (1999). Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970.\n\n[Kennel1992]: Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992). Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.pecuzal_embedding","page":"Unified Optimal Embedding","title":"DelayEmbeddings.pecuzal_embedding","text":"pecuzal_embedding(s; kwargs...) → 𝒟, τ_vals, ts_vals, ΔLs, ⟨ε★⟩\n\nA unified approach to properly embed a time series or a set of time series (Dataset) based on the ideas of Pecora et al. [Pecoral2007] and Uzal et al. [Uzal2011]. For a detailled description of the algorithm see Kraemer et al. [Kraemer2020].\n\nKeyword arguments\n\nτs = 0:50: Possible delay values τs (in sampling time units). For each of the τs's the continuity statistic ⟨ε★⟩ gets computed and further processed in order to find optimal delays τᵢ for each embedding cycle i (read algorithm description).\nw::Int = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nsamplesize::Real = 1: determine the fraction of all phase space points (=length(s)) to be considered (fiducial points v) to average ε★, in order to produce ⟨ε★⟩.\nK::Int = 13: the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic). ⟨ε★⟩ is computed taking the minimum result over all k ∈ K.\nKNN::Int = 3: the amount of nearest neighbors considered, in order to compute σk^2 (read algorithm description [`uzalcost]@ref). If given a vector, the minimum result over allknn ∈ KNN` is returned.\nL_threshold::Real = 0: The algorithm breaks, when this threshold is exceeded by ΔL in an embedding cycle (set as a positive number, i.e. an absolute value of ΔL).\nα::Real = 0.05: The significance level for obtaining the continuity statistic\np::Real = 0.5: The p-parameter for the binomial distribution used for the computation of the continuity statistic ⟨ε★⟩.\nmax_cycles = 50: The algorithm will stop after that many cycles no matter what.\necon::Bool = false: Economy-mode for L-statistic computation. Instead of computing L-statistics for time horizons 2:Tw, here we only compute them for 2:2:Tw, see description for further details.\n\nDescription\n\nThe method works iteratively and gradually builds the final embedding vectors Y. Based on the ⟨ε★⟩-statistic pecora the algorithm picks an optimal delay value τᵢ for each embedding cycle i. For achieving that, we take the inpute time series s, denoted as the actual phase space trajectory Y_actual and compute the continuity statistic ⟨ε★⟩.\n\nEach local maxima in ⟨ε★⟩ is used for constructing a\n\ncandidate embedding trajectory Y_trial with a delay corresponding to that specific peak in ⟨ε★⟩. 2. We then compute the L-statistic uzal_cost for Y_trial (L-trial) and Y_actual (L_actual) for increasing prediction time horizons (free parameter in the L-statistic) and save the maximum difference max(L-trial - L_actual) as ΔL (Note that this is a negative number, since the L-statistic decreases with better reconstructions).\n\nWe pick the peak/τ-value, for which ΔL is minimal (=maximum decrease of\n\nthe overall L-value) and construct the actual embedding trajectory Y_actual (steps 1.-3. correspond to an embedding cycle). 4. We repeat steps 1.-3. with Y_actual as input and stop the algorithm when ΔL is > 0, i.e. when and additional embedding component would not lead to a lower overall L-value. Y_actual -> Y.\n\nIn case of multivariate embedding, i.e. when embedding a set of M time series (s::Dataset), in each embedding cycle the continuity statistic ⟨ε★⟩ gets computed for all M time series available. The optimal delay value τ in each embedding cycle is chosen as the peak/τ-value for which ΔL is minimal under all available peaks and under all M ⟨ε★⟩'s. In the first embedding cycle there will be M² different ⟨ε★⟩'s to consider, since it is not clear a priori which time series of the input should consitute the first component of the embedding vector and form Y_actual.\n\nThe range of considered delay values is determined in τs and for the nearest neighbor search we respect the Theiler window w. The final embedding vector is stored in Y (Dataset). The chosen delay values for each embedding cycle are stored in τ_vals and the according time series numbers chosen for each delay value in τ_vals are stored in ts_vals. For univariate embedding (s::Vector) ts_vals is a vector of ones of length τ_vals, because there is simply just one time series to choose from. The function also returns the ΔLs-values for each embedding cycle and the continuity statistic ⟨ε★⟩ as an Array of Vectors.\n\nFor distance computations the Euclidean norm is used.\n\n[Pecora2007]: Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007). A unified approach to attractor reconstruction. Chaos 17(1).\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n[Kraemer2020]: Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2020). A unified and automated approach to attractor reconstruction. arXiv:2011.07040.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#Low-level-functions-of-unified-approach","page":"Unified Optimal Embedding","title":"Low-level functions of unified approach","text":"","category":"section"},{"location":"embedding/unified/","page":"Unified Optimal Embedding","title":"Unified Optimal Embedding","text":"DelayEmbeddings.n_statistic\nDelayEmbeddings.beta_statistic\nDelayEmbeddings.mdop_maximum_delay","category":"page"},{"location":"embedding/unified/#DelayEmbeddings.n_statistic","page":"Unified Optimal Embedding","title":"DelayEmbeddings.n_statistic","text":"n_statistic(Y, s; kwargs...) → N, d_E1\n\nPerform one embedding cycle according to the method proposed in [Garcia2005a] for a given phase space trajectory Y (of type Dataset) and a time series s (of typeVector). Return the proposed N-StatisticNand all nearest neighbor distancesd_E1for each point of the input phase space trajectoryY. Note thatY` is a single time series in case of the first embedding cycle.\n\nKeyword arguments\n\nτs= 0:50: Considered delay values τs (in sampling time units). For each of the τs's the N-statistic gets computed.\nr = 10: The threshold, which defines the factor of tolerable stretching for the d_E1-statistic (see algorithm description).\nT::Int = 1: The forward time step (in sampling units) in order to compute the d_E2-statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to T=1.\nw::Int = 0: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors. Note that in the paper this is not a free parameter and always w=0.\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input phase space trajectory Y.\n\nDescription\n\nFor a range of possible delay values τs one constructs a temporary embedding matrix. That is, one concatenates the input phase space trajectory Y with the τ-lagged input time series s. For each point on the temporary trajectory one computes its nearest neighbor, which is denoted as the d_E1-statistic for a specific τ. Now one considers the distance between the reference point and its nearest neighbor T sampling units ahead and calls this statistic d_E2. [Garcia2005a] strictly use T=1, so they forward each reference point and its corresponding nearest neighbor just by one (!) sampling unit. Here it is a free parameter.\n\nThe N-statistic is then the fraction of d_E2/d_E1-pairs which exceed a threshold r.\n\nPlotted vs. the considered τs-values it is proposed to pick the τ-value for this embedding cycle as the value, where N has its first local minimum.\n\n[Garcia2005a]: Garcia, S. P., Almeida, J. S. (2005). Nearest neighbor embedding with different time delays. Physical Review E 71, 037204.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.beta_statistic","page":"Unified Optimal Embedding","title":"DelayEmbeddings.beta_statistic","text":"beta_statistic(Y::Dataset, s::Vector) [, τs, w]) → β\n\nCompute the β-statistic β for input state space trajectory Y and a timeseries s according to Nichkawde [Nichkawde2013], based on estimating derivatives on a projected manifold. For a range of delay values τs, β gets computed and its maximum over all considered τs serves as the optimal delay considered in this embedding cycle.\n\nArguments τs, w as in mdop_embedding.\n\nDescription\n\nThe β-statistic is based on the geometrical idea of maximal unfolding of the reconstructed attractor and is tightly related to the False Nearest Neighbor method ([Kennel1992]). In fact the method eliminates the maximum amount of false nearest neighbors in each embedding cycle. The idea is to estimate the absolute value of the directional derivative with respect to a possible new dimension in the reconstruction process, and with respect to the nearest neighbor, for all points of the state space trajectory:\n\nϕ'(τ) = Δϕd(τ) / Δxd\n\nΔxd is simply the Euclidean nearest neighbor distance for a reference point with respect to the given Theiler window w. Δϕd(τ) is the distance of the reference point to its nearest neighbor in the one dimensional time series s, for the specific τ. Δϕ_d(τ) = |s(i+τ)-s(j+τ)|, with i being the index of the considered reference point and j the index of its nearest neighbor.\n\nFinally,\n\nβ = log β(τ) = ⟨log₁₀ ϕ'(τ)⟩ ,\n\nwith ⟨.⟩ being the mean over all reference points. When one chooses the maximum of β over all considered τ's, one obtains the optimal delay value for this embedding cycle. Note that in the first embedding cycle, the input state space trajectory Y can also be just a univariate time series.\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Kennel1992]: Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992). Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403.\n\n\n\n\n\n","category":"function"},{"location":"embedding/unified/#DelayEmbeddings.mdop_maximum_delay","page":"Unified Optimal Embedding","title":"DelayEmbeddings.mdop_maximum_delay","text":"mdop_maximum_delay(s, tw = 1:50, samplesize = 1.0)) -> τ_max, L\n\nCompute an upper bound for the search of optimal delays, when using mdop_embedding mdop_embedding or beta_statistic beta_statistic.\n\nDescription\n\nThe input time series s gets embedded with unit lag and increasing dimension, for dimensions (or time windows) tw (RangeObject). For each of such a time window the L-statistic from Uzal et al. [Uzal2011] will be computed. samplesize determines the fraction of points to be considered in the computation of L (see uzal_cost). When this statistic reaches its global minimum the maximum delay value τ_max gets returned. When s is a multivariate Dataset, τ_max will becomputed for all timeseries of that Dataset and the maximum value will be returned. The returned L-statistic has size (length(tw), size(s,2)).\n\n[Nichkawde2013]: Nichkawde, Chetan (2013). Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905.\n\n[Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223.\n\n\n\n\n\n","category":"function"}]
}
